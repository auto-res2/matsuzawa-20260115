{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data-efficient image classification",
    "regularization augmentation CNN",
    "few-shot image classification"
  ],
  "research_study_list": [
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification",
      "full_text": "Contextual Squeeze-and-Excitation for Efﬁcient Few-Shot Image Classiﬁcation Massimiliano Patacchiola University of Cambridge mp2008@cam.ac.uk John Bronskill University of Cambridge jfb54@cam.ac.uk Aliaksandra Shysheya University of Cambridge as2975@cam.ac.uk Katja Hofmann Microsoft Research kahofman@microsoft.com Sebastian Nowozin∗ nowozin@gmail.com Richard E. Turner University of Cambridge ret26@cam.ac.uk Abstract Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personaliza- tion, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a speciﬁc user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classiﬁcation setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to signiﬁcantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a ﬁne-tuning routine to adapt a linear head, deﬁning a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading ﬁne-tuning methods with the beneﬁt of orders of magnitude lower adaptation cost. 1 Introduction In recent years, the growth of industrial applications based on recommendation systems (Bennett et al., 2007), speech recognition (Xiong et al., 2018), and personalization (Massiceti et al., 2021) has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a speciﬁc user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classiﬁcation task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user’s device (context). In these applications the goal is twofold: maximize the classiﬁcation accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources. Methods developed to face these challenges in the few-shot classiﬁcation setting can be grouped in two categories: meta-learning and ﬁne-tuning. Meta-learning is based on the idea of learning-how-to- learn by improving the algorithm itself (Schmidhuber, 1987; Hospedales et al., 2020). Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates (Chen et al., 2019; Triantaﬁllou et al., 2019; Tian et al., 2020; Kolesnikov et al., 2020; Dumoulin et al., 2021). ∗Work done while the author was at Microsoft Research – Cambridge (UK) 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.09843v3  [cs.CV]  11 Jan 2023We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. Figure 1 illustrates this comparison by showing on the vertical axis the average classiﬁcation accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, Dumoulin et al. 2021), and on the horizontal axis the adaptation cost measured as the number of multiply–accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, ﬁne-tuners achieve a higher classiﬁcation accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, Kolesnikov et al. 2020) and LITE (Bronskill et al., 2021), shows a substantial performance gap of 14% in favor of the ﬁne-tuner but at a much higher adaptation cost, with BiT requiring 526 ×1012 MACs and LITE only 0.2 ×1012 MACs. Figure 1: Accuracy and adaptation cost on VTAB for meta-learners (blue), ﬁne- tuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE nar- rows the gap with the leading ﬁne-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost. It is crucial to ﬁnd solutions that retain the best of both worlds: the accuracy of ﬁne-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of ﬁne-tuners is the need for multiple gradi- ent adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) signiﬁcantly speeds up ﬁne-tuning, but it harms perfor- mance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is there- fore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a ﬁne-tuning routine for optimizing the head. At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by Hu et al. (2018) to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive ﬁne-tuning routines for the linear head, similarly to methods like MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). Figure 1 shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, ﬁne-tuners such as MD-Transfer (Triantaﬁllou et al., 2019), and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classiﬁcation performance. Our contributions can be summarized as follows: 1. We introduce a new adapter calledContextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by Hu et al. (2018), that outperforms other adaptation mechanisms (e.g. the FiLM generators used in Bronskill et al. 2021) in terms of parameter efﬁciency (a 75% reduction in the number of adaptation parameters) and classiﬁcation accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1. 2. We use CaSE adaptive blocks in conjuction with a ﬁne-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classiﬁcation accuracy compared to the SOTA meta-learner (Bronskill et al., 2021) on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT (Kolesnikov et al., 2020) with the beneﬁt of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset (Massiceti et al., 2021), where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT). 1https://github.com/mpatacchiola/contextual-squeeze-and-excitation 22 Contextual Squeeze-and-Excitation (CaSE) Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us deﬁne a collection of meta-training tasks as D= {τ1,...,τ D}where τi = (Ci,Ti) represents a generic task composed of a context set Ci = {(x,y)1,..., (x,y)M}and a target set Ti = {(x,y)1,..., (x,y)D}of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task τ∗ = {C∗,x∗}the goal is to predict the true label y∗ of the unlabeled target point x∗ conditioned on the context set C∗. In ﬁne-tuning methods, we are given a neural network fθ(·), with parameters θ estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task τ∗ adaptation consists of minimizing the lossL(·) via gradient updates to ﬁnd the task-speciﬁc parameters θτ∗ ←G(ϵ,L,τ∗,fθ), where ϵis a learning rate, and G(·) is a functional representing an iterative routine that returns the adapted parameters θτ∗ (used for prediction). This procedure is particularly effective because it can exploit efﬁcient mini-batching, parallelization, and large pretrained models. In meta-learning methods training and evaluation are performed episodically (Vinyals et al., 2016), with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to deﬁne a hierarchy. The parameters are divided in two groups: φtask-common parameters shared across all tasks (top of the hierarchy), and ψτ task-speciﬁc parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way φand ψτ come into play is method dependent; they can be estimated via gradient updates (e.g. MAML, Finn et al. 2017), learned metrics (e.g. ProtoNets, Snell et al. 2017), or Bayesian methods (Gordon et al., 2018; Patacchiola et al., 2020; Sendera et al., 2021). Standard Squeeze-Excite (SE) We brieﬂy introduce standard SE (Hu et al., 2018), as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of Llayers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function gφ(·). The number of hidden units in the MLP is deﬁned by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B×C×H×W where Cis the number of channels, Hthe height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H1,..., HB}with Hi ∈RC×H×W. To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B×C×1 ×1; this can be interpreted as a set of vectors {h1,..., hB}with hi ∈RC. For each layer l, the set is passed to the associated MLP that will generate an individual scale vector γi ∈RC, where γ(l) 1 = g(l) φ ( h(l) 1 ) ··· γ(l) B = g(l) φ ( h(l) B ) . (1) An elementwise product is then performed between the scale vector and the original tensor ˆH(l) 1 = H(l) 1 ∗γ(l) 1 ··· ˆH(l) B = H(l) B ∗γ(l) B , (2) with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in Figure 2 (left). Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to deﬁne a new version of SE for task-based channel-wise feature adaptation. For a task τ = (C,T), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H1,..., HN}with Hi ∈RC×H×W. As in standard SE, we ﬁrst apply a spatial pooling to each tensor Hi which produces N vectors {h1,..., hN}of shape hi ∈RC. Then a context pooling is performed; this corresponds to an empirical mean over {h1,..., hN}(see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer γ(l) = g(l) φ ( ¯h(l) ) with ¯h(l) = 1 N ( h(l) 1 + ··· + h(l) N ) , (3) 3Figure 2: Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, ∗elementwise multiplication. which is then multiplied elementwise by the original tensors ˆH(l) 1 = H(l) 1 ∗γ(l) ··· ˆH(l) N = H(l) N ∗γ(l). (4) The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of Figure 2. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in Figure 2 (right), additional technical details are provided in Appendix A. Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, Perez et al. 2018). Those mechanisms perform adaptation using a separate convolutional set-encoder to produce an embedding of the context set. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM (Oreshkin et al., 2018), CNAPs (Requeima et al., 2019), SimpleCNAPs (Bateni et al., 2020), CA VIA (Zintgraf et al., 2019), and LITE (Bronskill et al., 2021). We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efﬁcient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method (LITE, Bronskill et al. 2021), showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters. 3 UpperCaSE: system description and optimization protocol We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate- Descent (CD). We call this protocolhybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a ﬁne-tuning routine to estimate the task-speciﬁc parameters (head). Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), deﬁned as bθ(·) where θare the pretrained parameters. CaSE blocks, 4parameterized by φ, are added to the model at speciﬁc locations to give bθ,φ(·) (see Appendix A for details about this step). We are interested in learning the CaSE parameters φkeeping constant the pretrained parameters θ(omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks τ = {C,T}∼D , where Dis the training set. The number of classes (way) is calculated from the context set and used to deﬁne a linear classiﬁcation head hψτ (·) parameterized by ψτ. The complete model is obtained by nesting the two functions as hψτ (bφ(·)). We indicate a forward pass through the body over the context inputs with the shorthand bφ(Cx) →{z1,..., zN}, where zn is the context embedding for the input xn. All the context embeddings and the associated labels are stored in M= {(zn,yn)}N n=1. Optimization challenges We have two sets of learnable parameters,φthe CaSE parameters, and ψτ the parameters of the linear head for the taskτ. While φis shared across all tasks (task-common), ψτ must be inferred on the task at hand (task-speciﬁc). In both cases, the objective is the minimization of a classiﬁcation loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient dL dφ = ∑ τ (∂Lτ ∂ψτ dψτ dφ + ∂Lτ ∂φ ) . (5) The ﬁrst term ∂Lτ/∂ψτ (sensitivity of the loss w.r.t. the head) and the direct gradient ∂L/∂φ (sensitivity of the loss w.r.t. the adaptation parameters with a ﬁxed head) can be obtained with auto-differentiation as usual. The second term dψτ/dφ(sensitivity of the head w.r.t. the adaptation parameters) is problematic because ψτ is obtained iteratively after a sequence of gradient updates. Backpropagating the gradients to φincludes a backpropagation through all the gradient steps per- formed to obtain the task-speciﬁc ψτ. Previous work has showed that this produces instability, vanishing gradients, and high memory consumption (Antoniou et al., 2018; Rajeswaran et al., 2019). Meta-training via Coordinate-Descent A potential solution to these issues is the use of implicit gradients (Chen et al., 2020; Rajeswaran et al., 2019; Chen et al., 2022). The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy’s implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). These methods share the idea of inner-loop-head/outer- loop-body meta-training, and they ﬁnd the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) (Wright, 2015). The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. φand ψτ as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ψτ ﬁrst, we reach a local minimum where ∂Lτ/∂ψτ ≈0. Therefore CD induces a direct optimization objective w.r.t.φ, with Equation (5) reducing to ∂Lτ/∂φ(no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details. In practice, at each training iteration we sample a task τ = (C,T) ∼D, perform a forward pass on the body (with CaSE in adaptive mode) to get bφ(Cx) →{z1,..., zN}. (6) The context embeddings are temporarily stored in a buffer with their associated labels M = {(zn,yn)}N n=1 to avoid expensive calls to bφ(·). We then set the head parameters to zero, and solve the ﬁrst minimization problem (inner-loop), obtaining the task-speciﬁc parameters ψτ via ψτ ←G ( α,M,L,hψτ ) (7) where αis a learning rate, and G(·) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation(7) only relies on the headhψτ (·) and not on the bodybφ(·), which is the primary source of memory savings and the crucial difference with common ﬁne-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efﬁcient mini-batching. 5We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t.φ(outer-loop) given support/target points and the task-speciﬁc parameters ψτ identiﬁed previously. The ﬁnal form of the equation depends on the optimizer, for a generic SGD the update is given by φ←φ−β∇φL ( Cy ∪Qy,hψτ ,bφ ) , (8) where βis a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B. Inference on unseen tasks After the training phase, we are given an unseen task τ∗ = (C∗,x∗) where x∗ is a single target input and y∗ the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation (6) and embeddings/labels stored in M, (ii) estimation of the task-speciﬁc parameters ψ∗ via iterative updates as in Equation (7), and (iii) inference of the target-point membership via a forward pass over body and head ˆy∗ = hψ∗ (bφ(x∗)) with CaSE in inference mode. 4 Related work Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details (Hospedales et al., 2020). LITE (Bronskill et al., 2021) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs (Bateni et al., 2020) that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classiﬁcation accuracy and parameter efﬁciency. Fine-tuning Chen et al. (2019) were the ﬁrst to expose the potential of simple ﬁne-tuning baselines for transfer learning. MD-Transfer has been proposed in Triantaﬁllou et al. (2019) as an effective ﬁne-tuning baseline for the MetaDataset benchmark. More recently Kolesnikov et al. (2020) have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classiﬁcation head, learning schedule, normalization layers, and batching. Fine-tuning only the last linear layer can be effective (Bauer et al., 2017; Tian et al., 2020). We compare against this baseline in Section 5.1, showing that adapting the body via CaSE signiﬁcantly boosts the performance. Overall, ﬁne-tuners have consistently outperformed meta-learners in terms of classiﬁcation accuracy, only under particular conditions (e.g. strong class-imbalance) the trend is reversed (Ochal et al., 2021a,b). Hybrids Hybrid methods are trained episodically like meta-learners but rely on ﬁne-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, Finn et al. 2017) ﬁnds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ (Antoniou et al., 2018), ProtoMAML (Triantaﬁllou et al., 2019), and Reptile (Nichol et al., 2018). Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs (Han et al., 2021). Adaptive components have been used in a variety of applications, such as neural compression (Veit and Belongie, 2018; Wu et al., 2018), generation of artistic styles (Dumoulin et al., 2016; Huang and Belongie, 2017), or routing (Guo et al., 2019). Residual adapters (Rebufﬁ et al., 2017, 2018) have been used in transfer learning (non few-shot) but they rely on ﬁne-tuning routines which are signiﬁcantly slow during adaptation. More recently, Li et al. (2022) have used serial and residual adapters in the few-shot setting, with the task-speciﬁc weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-speciﬁc weights in the body of the network which is costly. In Sun et al. (2019) the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via ﬁne-tuning. This method suffers of similar limitations, as the ﬁne-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. 6Figure 3: Left: CaSE vs Squeeze-and-Excitation (SE) (both methods use EfﬁcientNetB0, 84 × 84 inputs, Mahalanobis-distance head). CaSE outperforms SE in all conditions. Center: CaSE vs. FiLM generators (Bronskill et al., 2021) and a baseline with no body adaptation (all methods use EfﬁcientNetB0, 84 ×84 inputs, Mahalanobis-distance head). CaSE outperforms FiLM generators in all conditions. Right: boxplot of CaSE activations at different depth of an EfﬁcientNetB0 for 800 tasks sampled from the MDv2 test set (224 ×224 inputs, UpperCaSE). The modulation of CaSE is minimal at early stages for general-purpose ﬁlters and increases at deeper stages. 5 Experiments In this section we report on experiments on VTAB+MD (Dumoulin et al., 2021) and ORBIT (Massiceti et al., 2021). VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfﬁcientNetB0 from the ofﬁcial Torchvision repository; ResNet50x1-S released with BiT (Kolesnikov et al., 2020). We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of Bronskill et al. (2021) ( 10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10−3,10−5] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (conﬁdence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . 5.1 Analysis of CaSE blocks In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE (Hu et al., 2018) and CaSE on MDv2 and VTAB, conﬁrming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator (Bronskill et al., 2021), showing that CaSE is signiﬁcantly more efﬁcient using 75% fewer parameters while boosting the classiﬁcation accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis. Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD- v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in Bronskill et al. (2021), reporting results with a linear head in the appendix. We summarize the results in Figure 3 (left) and add a tabular breakdown in Appendix C.2. CaSE outperforms SE in all conditions, conﬁrming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks. Comparing adaptation mechanisms We perform a comparison on VTAB+MD of CaSE against FiLM generators (Bronskill et al., 2021), and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfﬁcientNetB0 backbone, and same training schedule. We show a summary of the results in 7Figure 3 (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneﬁcial. This is likely due to the strong domain shift introduced by some of the specialized datasets. Role of CaSE blocks To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfﬁcientNetB0 trained with UpperCaSE on224×224 images. In Figure 3 (right) we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose ﬁlters that are useful across all tasks. In deeper layers the variance increases, showing a task-speciﬁc modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4. 5.2 Performance evaluation of UpperCaSE In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA ﬁne-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB;2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset (Massiceti et al., 2021) for the cross-domain condition MDv2→ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars. Table 1: UpperCaSE outperforms ﬁne-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold. Cost↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. MACs all all natur. spec. struc. MD-Transfer ﬁne-tuning RN18 126 11.2 118.6 63.4 55.6 52.4 72.9 49.3 SUR ﬁne-tuning RN50 224 164.6 28.8 71.3 43.7 50.9 66.2 27.2 Big Transfer ﬁne-tuning RN50 224 23.5 526.3 73.3 65.4 69.4 81.0 54.5 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.2 76.1 58.4 69.1 80.3 39.4 Table 2: UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest num- ber of parameters per adaptive blocks . Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold. Adapt.↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. count all all natur. spec. struc. ProtoMAML hybrid RN18 126 11.2 n/a 64.2 45.0 45.7 70.7 31.5 CTX meta-learning RN34 224 21.3 n/a 71.6 50.5 61.1 67.3 34.0 ProtoNet meta-learning ENB0 224 4.0 n/a 72.7 46.1 60.9 64.2 25.9 LITE meta-learning ENB0 224 4.0 1.7 73.8 51.4 65.2 71.9 30.8 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.4 76.1 58.4 69.1 80.3 39.4 Comparison on VTAB+MDWe compare UpperCaSE against ﬁne-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results 8Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% conﬁdence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al. (2021): meta-train on MetaDataset and test on ORBIT, image-size 84 ×84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold. Cost Clean Video Evaluation (CLE-VE) Clutter Video Evaluation (CLU-VE) Method MACs ↓ frame acc.↑ FTR↓ video acc.↑ frame acc.↑ FTR↓ video acc.↑ ProtoNet 3.2 59.0 ±2.2 11.5 ±1.8 69.2 ±3.0 47.0 ±1.8 20.4 ±1.7 52.8 ±2.5 CNAPs 3.5 51.9 ±2.5 20.8 ±2.3 60.8 ±3.2 41.6 ±1.9 30.7 ±2.1 43.0 ±2.5 MAML 95.3 42.5 ±2.7 37.3 ±3.0 47.0 ±3.2 24.3 ±1.8 62.3 ±2.3 25.7 ±2.2 FineTuner 317.7 61.0±2.2 11.5 ±1.8 72.6 ±2.9 48.4 ±1.9 19.1 ±1.7 54.1 ±2.5 UpperCaSE 3.5 63.0±2.2 8.8 ±1.6 74.4 ±2.8 48.1 ±1.8 18.2 ±1.7 54.5 ±2.5 in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfﬁcientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see Table 1) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%). The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE (Bronskill et al., 2021) which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difﬁcult to compensate without ﬁne-tuning the body. It is not clear whether transfer learning is beneﬁcial on these datasets in the ﬁrst place. The results in terms of adaptation cost (see Table 1) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efﬁcient (0.2 ×1012 MACs) than all ﬁne-tuners, with BiT being the most expensive method overall (526.3 × 1012 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see Table 2) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfﬁcientNetB0) compared to LITE (Bronskill et al., 2021) which is based on FiLM generators. Comparison on ORBIT We compare UpperCaSE to other methods on ORBIT (Massiceti et al., 2021), a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classiﬁers are meta-trained on MetaDataset and tested on ORBIT. Results are reported in Table 3. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR −2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efﬁcient than FineTuner and close to the leading method (ProtoNet). 6 Conclusions We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and- Excitation (SE) block proposed by Hu et al. (2018). CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of ﬁne-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark. 9Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that ﬁne-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large. Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classiﬁ- cation settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efﬁciency of our method can reduce energy consumption and beneﬁt the environment. Certain applications require careful consideration to avoid biases that can harm speciﬁc groups of people (e.g. surveillance, legal decision-making). Acknowledgments and Disclosure of Funding Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos, Daniela Massiceti, and Shoaib Ahmed Siddiqui for providing suggestions and feedback on the preliminary version of the manuscript. References Antoniou, A., Edwards, H., and Storkey, A. (2018). How to train your maml. arXiv preprint arXiv:1810.09502. Bateni, P., Goyal, R., Masrani, V ., Wood, F., and Sigal, L. (2020). Improved few-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Bauer, M., Rojas-Carulla, M., ´Swi ˛ atkowski, J. B., Schölkopf, B., and Turner, R. E. (2017). Discrimi- native k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326. Bennett, J., Lanning, S., et al. (2007). The netﬂix prize. In Proceedings of KDD cup and workshop. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136. Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efﬁcient meta-learning with large images. Advances in Neural Information Processing Systems. Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. (2019). A closer look at few-shot classiﬁcation. arXiv preprint arXiv:1904.04232. Chen, Y ., Friesen, A. L., Behbahani, F., Doucet, A., Budden, D., Hoffman, M., and de Freitas, N. (2020). Modular meta-learning with shrinkage. Advances in Neural Information Processing Systems. Dumoulin, V ., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and Larochelle, H. (2021). Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark. arXiv preprint arXiv:2104.02638. Dumoulin, V ., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style.arXiv preprint arXiv:1610.07629. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W. (2018). Neural processes. arXiv preprint arXiv:1807.01622. 10Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. (2018). Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921. Guo, Y ., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. (2019). Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. (2020). Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439. Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Huang, X. and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In European conference on computer vision. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021a). Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021b). How sensitive are meta-learners to dataset imbalance? arXiv preprint arXiv:2104.05344. Oreshkin, B., Rodríguez López, P., and Lacoste, A. (2018). Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31. Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and Storkey, A. J. (2020). Bayesian meta- learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. (2019). Meta-learning with implicit gradients. Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2018). Efﬁcient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. 11Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. (2019). Fast and ﬂexible multi- task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems. Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München. Sendera, M., Tabor, J., Nowak, A., Bedychaj, A., Patacchiola, M., Trzcinski, T., Spurek, P., and Zieba, M. (2021). Non-gaussian gaussian processes for few-shot regression. Advances in Neural Information Processing Systems. Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems. Sun, Q., Liu, Y ., Chua, T.-S., and Schiele, B. (2019). Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. (2020). Rethinking few-shot image classiﬁcation: a good embedding is all you need? In European Conference on Computer Vision. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. Veit, A. and Belongie, S. (2018). Convolutional networks with adaptive inference graphs. In Proceedings of the European Conference on Computer Vision (ECCV). Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learning. Advances in Neural Information Processing Systems. Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Programming, 151. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L. S., Grauman, K., and Feris, R. (2018). Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. In IEEE international conference on acoustics, speech and signal processing (ICASSP). Zintgraf, L., Shiarli, K., Kurin, V ., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In International Conference on Machine Learning. 12A CaSE: additional details A.1 CaSE implementation Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer las showed in Equation (3), that is ¯h(l) ∈RC, subtracting the mean and dividing by the standard deviation. Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c= 2which extends the range to [0,2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 (Table 6). CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in Bronskill et al. (2021) for FiLM generators. In EfﬁcientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the ﬁrst layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfﬁcientNet): Conv2d→BatchNorm2d→SiLU→CaSE→SqueezeExcitation→Conv2d→BatchNorm2d This results in a total of 18 CaSE blocks for EfﬁcientNetB0. Increasing the number of blocks did not provide a signiﬁcant beneﬁt. In ResNet18 we place two CaSE blocks per each basic block as: Conv2d→BatchNorm2d→ReLU→CaSE→Conv2d→BatchNorm2d→ReLU→CaSE Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details. Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor. CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input sizeC(corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor rmin that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels. A.2 Context pooling In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2). Similarities with other methods Context pooling is a way to summarize a task with a permutation- invariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets (Snell et al., 2017) a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class. The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs (Bateni et al., 2020) a prototype is estimated as in ProtoNets but it is used to deﬁne a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family (Garnelo et al., 2018) also rely on similar permutation-invariant aggregations to deﬁne distributions over functions. 13Global vs. local context-pooling Comparing CaSE with the FiLM generators of Bronskill et al. (2021) it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of Bronskill et al. (2021) rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More speciﬁcally, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding. CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4). 14A.3 Pytorch code for CaSE Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . import torch from torch import nn class CaSE (nn. Module ): def __init__ (self , cin , reduction =32 , min_units =32 , standardize =True , out_mul =2.0, device =None , dtype = None ): \"\"\" Initialize a CaSE adaptive block . Parameters : cin ( int ): number of input channels . reduction ( int ): divider for computing number of hidden units . min_units ( int ): clip hidden units to this value (if lower ). standardize ( bool ): standardize the input for the MLP . out_mul ( float ): multiply the MLP output by this value . \"\"\" factory_kwargs = {’ device ’: device , ’dtype ’: dtype } super (CaSE , self ). __init__ () self . cin = cin self . standardize = standardize self . out_mul = out_mul hidden = max ( min_units , cin // reduction ) self . gamma_generator = nn. Sequential ( nn. Linear (cin , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , cin , bias =True , ** factory_kwargs ), nn. Sigmoid () ) self . reset_parameters () def reset_parameters ( self ): nn. init . zeros_ ( self . gamma_generator [4]. weight ) nn. init . zeros_ ( self . gamma_generator [4]. bias ) self . gamma = torch . tensor ([1.0]) def forward (self , x): if( self . training ): # adaptive mode self . gamma = torch . mean (x, dim =[2,3]) # spatial pooling self . gamma = torch . mean ( self . gamma , dim =[0])# context pooling if( self . standardize ): self . gamma = ( self . gamma - torch . mean ( self . gamma )) / \\ torch . sqrt ( torch . var ( self . gamma , unbiased = False ) + 1e-5) self . gamma = self . gamma . unsqueeze (0) self . gamma = self . gamma_generator ( self . gamma ) * self . out_mul self . gamma = self . gamma . reshape ([1,-1,1,1]) return self . gamma * x else : # inference mode self . gamma = self . gamma .to(x. device ) return self . gamma * x def extra_repr ( self ): return ’cin ={}’. format ( self . cin ) 15B UppereCaSE: additional details B.1 Algorithm of UpperCaSE Algorithm 1 UpperCaSE: training function for the few-shot classiﬁcation setting. Require: D= {τ1,...,τ D}training dataset Require: bφ() pretrained feature extractor (body) with CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α, β: step-size hyperparameters for the optimizer. 1: Set φto random values ⊿optional: set φto enforce identity in CaSE output 2: while not done do 3: Sample task τ = (C,T) ∼D 4: Forward pass over context set bφ(Cx) → z1,..., zN ⊿CaSE in adaptive mode 5: Store context embeddings and associated labels M= {(zn,yn)}N n=1 ⊿temporary memory buffer 6: Deﬁne a linear model for the head hψτ () and set ψτ to zero 7: for total inner-steps do ⊿loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B∼M 9: Update the head parameters ψτ ←step(α,L,B,hψτ ) 10: end for 11: Update the CaSE parameters φ←step(β,L,C,T,bφ,hψτ ) ⊿CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classiﬁcation setting. Require: τ∗ = (C∗,x∗) unseen test task with target input x∗ an context C∗. Require: bφ() pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α: step-size hyperparameter for the optimizer. 1: Forward pass over context set bφ(Cx ∗ ) → z1,..., zN ⊿CaSE in adaptive mode 2: Store context embeddings and associated labels M∗ = {(zn,yn)}N n=1 ⊿temporary memory buffer 3: Deﬁne a linear model for the head hψτ∗ () and set ψτ∗ to zero 4: for total inner-steps do ⊿loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B∗ ∼M∗ 6: Update the head parameters ψτ∗ ←step(α,L,B∗,hψτ∗ ) 7: end for 8: Return Prediction ˆy∗ = hψτ∗ (bφ(x∗)) ⊿CaSE in inference mode C Additional experimental details and results C.1 Additional details MACs counting MACs are proportional to the size of the task, size of the images, and number of classes. We can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ×224 ×3 generated via Gaussian noise (µ= 0,σ = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfﬁcientNetB0 backbone for the ﬁrst and a ResNet50-S for the second. For MD-Transfer we used the same parameters reported in Dumoulin et al. (2021) with images of size 126 ×126 ×3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone. VTAB+MD trainingWe follow the protocol reported in the original papers (Triantaﬁllou et al., 2019; Dumoulin et al., 2021) training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and conﬁdence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfﬁcientNetB0 from the ofﬁcial Torchvision repository 3, and a pretrained ResNet50-S 2https://github.com/microsoft/ORBIT-Dataset 3https://pytorch.org/vision 16from the BiT repository 4. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use the BiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the ofﬁcial Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). For the evaluation phase we followed the instructions reported in Massiceti et al. (2021). C.2 CaSE vs SE Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al., 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold. Model SE CaSE SE CaSE Contextual pooling No Yes No Yes Adaptation head MD MD Linear Linear Image size 84 84 224 224 MetaDataset (all) 67.8 69.6 74.6 76.2 VTAB (all) 43.6 45.3 56.6 58.2 VTAB (natural) 47.5 50.2 65.3 68.1 VTAB (specialized) 63.6 64.9 79.8 79.6 VTAB (structured) 30.6 31.8 38.6 40.1 C.3 CaSE vs other adapters Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al. (2021), and a baseline with no body adaptation. CaSE blocks are more efﬁcient in terms of adaptive and amortization parameters while providing higher classiﬁcation accuracy. All models have been trained and tested on 84 ×84 images, using a Mahalanobis distance head. Best results in bold. Adaptation type None FiLM CaSE64 CaSE32 CaSE16 Adaptive Params (M) n/a 0.02 0.01 0.01 0.01 Amortiz. Params (M) n/a 1.7 0.4 0.8 1.6 MetaDataset (all) 53.4 68.4 69.8 69.6 70.4 VTAB (all) 43.5 44.7 46.2 45.3 46.4 VTAB (natural) 45.4 49.5 52.1 50.2 52.6 VTAB (specialized) 69.4 63.8 66.3 64.9 65.5 VTAB (structured) 29.1 31.7 31.8 31.8 32.1 C.4 Ablation studies In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows: • Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in Table 6 show that the sigmoid with multiplier and the linear layer provide the best results. 4https://github.com/google-research/big_transfer 17• Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in Table 8 show that blocks with more units provide marginal gains or no gains at all. This is probably due to overﬁtting issues affecting the models with more units. • Ablation of the number of hidden layers of CaSE. The results reported in Table 7 show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overﬁtting issues affecting the models with more parameters. • Ablation of the activation function for the hidden layers. Results reported in Table 9 show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfﬁcientNet backbones). Table 6: Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfﬁcientNetB0 backbone with a linear head on images of size 224 ×224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the ﬁrst iterations). Best results in bold. Adaptive block SE SE CaSE CaSE CaSE Activation (output) Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2 MetaDataset (all) 74.2 74.6 75.8 74.9 76.2 VTAB (all) 56.8 56.6 58.4 56.8 58.2 VTAB (natural) 67.0 65.3 68.3 67.1 68.1 VTAB (specialized) 81.1 79.8 79.5 80.8 79.6 VTAB (structured) 36.9 38.6 40.3 37.1 40.1 Table 7: Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ×224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. # Hidden layers 1 2 3 4 Amortiz. Params (M) 0.420 0.426 0.432 0.438 MetaDataset (all) 76.0 76.1 75.5 75.2 VTAB (all) 58.2 58.4 58.2 58.0 VTAB (natural) 68.3 69.1 68.0 67.4 VTAB (specialized) 79.7 80.3 80.5 80.3 VTAB (structured) 40.0 39.4 39.7 39.7 18Table 8: Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is deﬁned by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Reduction factor 64 32 16 8 Clip factor 16 32 48 64 Amortiz. Params (M) 0.4 0.8 1.6 3.0 MetaDataset (all) 76.1 76.2 75.8 76.2 VTAB (all) 58.4 58.2 57.9 58.5 VTAB (natural) 69.1 68.1 67.9 68.3 VTAB (specialized) 80.3 79.6 79.4 79.0 VTAB (structured) 39.4 40.1 39.7 40.9 Table 9: Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortiza- tion parameters) and they have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Activation (hidden) SiLU ReLU Tanh MetaDataset (all) 76.1 75.8 74.8 VTAB (all) 58.4 57.8 48.2 VTAB (natural) 69.1 69.8 67.0 VTAB (specialized) 80.3 79.7 80.8 VTAB (structured) 39.4 39.4 36.4 19C.5 Role of CaSE blocks Figure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfﬁcientNetB0 (horizontal axis, with early stages on the left). The box encloses ﬁrst to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles. 20Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfﬁcientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the ﬁrst stages but diverge in the latest. The magnitude tends to increase with depth. 21C.6 UpperCaSE: results on VTAB+MD In this section we provide a full breakdown of the results for UpperCaSE vs. other methods on the VTAB+MD benchmark. Results for other methods are taken from Bronskill et al. (2021) and Dumoulin et al. (2021). UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfﬁcientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks. In Table 10 we report the results for UpperCaSE against ﬁne-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE). Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneﬁcial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and ﬁne-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set. In this case, ﬁne-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against ﬁne-tuning methods. Best result in bold. Model BiT MD-Transfer SUR UpperCaSE UpperCaSE Image Size 224 126 224 224 224 Network RN50-S RN18 RN50 ×7 ENB0 RN50-S Params (M) 23.5 11.2 164.5 4.0 23.5 Omniglot 68.0 ±4.5 82.0 ±1.3 92.8±0.5 90.7±0.4 89.1 ±0.5 Aircraft 77.4 ±3.5 76.8 ±1.2 84.4 ±0.6 89.4±0.4 87.5±0.4 Birds 90.8±1.5 61.2±1.3 75.8 ±1.0 90.4±0.4 89.6 ±0.4 DTD 85.0±2.5 66.0±1.1 74.3 ±0.7 83.4±0.4 84.8 ±0.5 QuickDraw 66.6 ±3.7 61.3 ±1.1 70.3 ±0.7 76.8±0.5 73.7±0.6 Fungi 59.4 ±4.2 35.5 ±1.1 81.7±0.6 59.3±0.8 56.8 ±0.8 Trafﬁc Sign 73.5 ±4.7 84.7±0.9 50.0±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 65.7±2.7 39.6±1.0 49.4 ±1.1 50.8 ±0.7 46.7 ±0.8 Caltech101 87.2 70.6 82.3 88.3 86.2 CIFAR100 54.4 31.3 33.7 52.7 47.0 Flowers102 83.3 66.1 55.7 85.3 83.0 Pets 87.9 49.1 76.3 89.9 89.3 Sun397 33.3 13.9 27.5 35.8 32.5 SVHN 70.4 83.2 18.7 62.7 59.8 EuroSAT 94.4 88.7 78.9 92.2 91.6 Resics45 76.1 63.7 62.4 75.5 74.4 Patch Camelyon 83.1 81.5 75.6 79.3 80.9 Retinopathy 70.2 57.6 27.9 74.3 73.7 CLEVR-count 74.0 40.3 30.0 40.3 42.0 CLEVR-dist 51.5 52.9 37.1 38.9 37.3 dSprites-loc 82.7 85.9 30.0 45.3 38.1 dSprites-ori 55.1 46.4 19.8 42.5 41.4 SmallNORB-azi 17.8 36.5 12.9 15.7 15.1 SmallNORB-elev 32.1 31.2 18.1 22.7 21.0 DMLab 43.2 37.9 33.3 38.7 36.1 KITTI-dist 79.9 58.7 52.3 71.0 69.6 MetaDataset (all) 73.3 63.4 71.0 76.1 74.9 VTAB (all) 65.4 55.6 42.9 58.4 56.6 VTAB (natural) 69.4 52.4 49.0 69.1 66.3 VTAB (specialized) 81.0 72.9 61.2 80.3 80.1 VTAB (structured) 54.5 49.4 29.2 39.4 37.6 22Table 11: Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold. Model ProtoNet ProtoMAML CTX LITE UpperCaSE UpperCaSE Image Size 224 126 224 224 224 224 Network ENB0 RN18 RN34 ENB0 ENB0 RN50-S Params (M) 4.0 11.2 21.3 4.0 4.0 23.5 Omniglot 88.3 ±0.8 90.2±0.7 84.6±0.9 86.5 ±0.8 90.7±0.4 89.1±0.5 Aircraft 85.0 ±0.7 82.1 ±0.6 85.3 ±0.8 83.6 ±0.7 89.4±0.4 87.5±0.4 Birds 90.2±0.5 73.4±0.9 72.9 ±1.1 88.6 ±0.7 90.4±0.4 89.6±0.4 DTD 81.4 ±0.6 66.3 ±0.8 77.3 ±0.7 84.1±0.7 83.4±0.4 84.8±0.5 QuickDraw 76.0±0.7 66.4±1.0 73.3 ±0.8 75.7±0.8 59.3±0.8 56.8 ±0.8 Fungi 57.4 ±1.1 46.3 ±1.1 48.0 ±1.2 56.9 ±1.2 59.3±0.8 56.8±0.8 Trafﬁc Sign 53.5 ±1.1 50.3 ±1.1 80.1±1.0 65.8±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 49.8±1.1 39.0±1.0 51.4±1.1 50.0 ±1.0 50.8 ±0.7 46.7±0.8 Caltech101 87.4 73.1 84.2 87.7 88.3 86.2 CIFAR100 43.1 29.7 37.5 48.8 52.7 47.0 Flowers102 78.2 60.2 81.8 83.5 85.3 83.0 Pets 88.6 56.6 70.9 89.3 89.9 89.3 Sun397 32.9 8.1 24.8 30.9 35.8 32.5 SVHN 35.2 46.8 67.2 51.0 62.7 59.8 EuroSAT 83.3 80.1 86.4 89.3 92.2 91.6 Resics45 68.8 53.5 67.7 76.4 75.5 74.4 Patch Camelyon 73.3 75.9 79.8 81.4 79.3 80.9 Retinopathy 31.3 73.2 35.5 40.3 74.3 73.7 CLEVR-count 27.2 32.7 27.9 31.4 40.3 42.0 CLEVR-dist 28.5 35.4 29.6 32.8 38.9 37.3 dSprites-loc 13.4 42.0 23.2 12.3 45.3 38.1 dSprites-ori 19.6 23.0 46.9 31.1 42.5 41.4 SmallNORB-azi 9.4 13.4 37.0 14.5 15.7 15.1 SmallNORB-elev 17.0 18.8 21.6 21.0 22.7 21.0 DMLab 35.8 32.5 31.9 39.4 38.7 36.1 KITTI-dist 56.5 54.4 54.3 63.9 71.0 69.6 MetaDataset (all) 72.7 64.2 71.6 73.9 76.1 74.9 VTAB (all) 46.1 45.0 50.5 51.4 58.4 56.6 VTAB (natural) 60.9 45.7 61.1 65.2 69.1 66.3 VTAB (specialized) 64.2 70.7 67.3 71.9 80.3 80.1 VTAB (structured) 25.9 31.5 34.1 30.8 39.4 37.6 23",
      "references": [
        "How to train your maml",
        "Improved few-shot visual classiﬁcation",
        "Discrimi- native k-shot learning using probabilistic models",
        "The netﬂix prize",
        "Meta-learning with differentiable closed-form solvers",
        "Memory efﬁcient meta-learning with large images",
        "Meta-learning feature representations for adaptive gaussian processes via implicit differentiation",
        "A closer look at few-shot classiﬁcation",
        "Modular meta-learning with shrinkage",
        "Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark",
        "A learned representation for artistic style",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Neural processes",
        "Meta-learning probabilistic inference for prediction",
        "Spottune: transfer learning through adaptive ﬁne-tuning",
        "Dynamic neural networks: A survey",
        "Meta-learning in neural networks: A survey",
        "Squeeze-and-excitation networks",
        "Arbitrary style transfer in real-time with adaptive instance normalization",
        "Big transfer (bit): General visual representation learning",
        "Meta-learning with differentiable convex optimization",
        "Cross-domain few-shot learning with task-speciﬁc adapters",
        "Orbit: A real-world few-shot dataset for teachable object recognition",
        "On ﬁrst-order meta-learning algorithms",
        "Few-shot learning with class imbalance",
        "How sensitive are meta-learners to dataset imbalance?",
        "Tadam: Task dependent adaptive metric for improved few-shot learning",
        "Bayesian meta-learning for the few-shot setting via deep kernels",
        "Film: Visual reasoning with a general conditioning layer",
        "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
        "Meta-learning with implicit gradients",
        "Learning multiple visual domains with residual adapters",
        "Efﬁcient parametrization of multi-domain deep neural networks",
        "Fast and ﬂexible multi-task classiﬁcation using conditional neural adaptive processes",
        "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook",
        "Non-gaussian gaussian processes for few-shot regression",
        "Prototypical networks for few-shot learning",
        "Meta-transfer learning for few-shot learning",
        "Rethinking few-shot image classiﬁcation: a good embedding is all you need?",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples",
        "Convolutional networks with adaptive inference graphs",
        "Matching networks for one shot learning",
        "Coordinate descent algorithms",
        "Blockdrop: Dynamic inference paths in residual networks",
        "The microsoft 2017 conversational speech recognition system",
        "Fast context adaptation via meta-learning"
      ],
      "meta_data": {
        "arxiv_id": "2206.09843v3",
        "authors": [
          "Massimiliano Patacchiola",
          "John Bronskill",
          "Aliaksandra Shysheya",
          "Katja Hofmann",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2022-06-20T15:25:08Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Contextual Squeeze-and-Excitation (CaSE), a lightweight task-adaptive block for few-shot image classification that conditions a pretrained CNN using contextual pooling, and UpperCaSE, a hybrid meta-learning/fine-tuning protocol combining meta-trained CaSE modules for single-pass body adaptation with gradient-based head fine-tuning. Achieves state-of-the-art accuracy–cost trade-off, outperforming prior meta-learners on VTAB+MetaDataset and real-world ORBIT personalization while using far fewer adaptation parameters and orders-of-magnitude lower adaptation MACs than fine-tuning methods.",
        "methodology": "1) CaSE: Extends squeeze-and-excitation by (a) pooling activations over the context set to obtain task-level statistics, (b) generating per-layer channel-wise scaling vectors via small MLPs, and (c) re-using these scales for target examples (adaptive vs inference modes). Only scale (no shift) vectors are produced, reducing parameters by 50% vs FiLM adapters.\n2) UpperCaSE: Adds CaSE blocks to a pretrained backbone; during episodic meta-training it alternates coordinate-descent steps—inner-loop: optimize task-specific linear head via SGD; outer-loop: update shared CaSE parameters via gradient from target loss while freezing backbone weights. At test time a single forward pass on context computes CaSE scales, then the head is optimized, yielding fast adaptation without back-prop through body.",
        "experimental_setup": "Backbones: EfficientNet-B0 and ResNet50-S/18 pretrained on ImageNet.\nBenchmarks: MetaDataset-v2 (8 datasets), VTAB (18 datasets across natural, specialized, structured), ORBIT personalization benchmark.\nImage sizes: 84×84 or 224×224.\nTraining: 10k episodic tasks, Adam with learning rate 1e-3→1e-5, 500 inner-loop head updates, batch 128. Evaluation: 1200 test tasks per MD dataset; VTAB uses provided test split. Adaptation cost measured as MACs on synthetic 100-way 10-shot tasks.\nComparisons: Fine-tuning baselines (BiT, MD-Transfer, SUR), meta-learners (ProtoNet, ProtoMAML, CTX, LITE) and hybrids (MAML). Metrics: average accuracy per benchmark, frames/video accuracy & FTR on ORBIT, adaptation MACs and parameter counts.",
        "limitations": "1) Requires iterative gradient updates for head optimization; although cheaper than full fine-tuning, may still be heavy for low-power devices.\n2) Performance gap remains on structured VTAB tasks needing localization/counting; CaSE body adaptation alone may be insufficient when domain shift is large.\n3) Relies on pretrained backbones and assumes availability of such models and comparable data distributions.\n4) Meta-training on large task sets is resource-intensive.\n5) Only channel-wise scaling (no spatial or shift modulation) may limit expressiveness.",
        "future_research_directions": "1) Explore more expressive adapters (e.g., adding spatial attention or shift parameters) while preserving efficiency.\n2) Develop hardware-aware variants minimizing or eliminating inner-loop gradients for on-device adaptation.\n3) Investigate automatic placement or pruning of CaSE blocks to further reduce parameters.\n4) Address weaknesses on structured and localization-centric tasks, possibly integrating detection or transformer components.\n5) Extend CaSE/UpperCaSE to modalities beyond images (e.g., audio, NLP) and to continual or federated learning scenarios.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Scaling Spherical CNNs",
      "full_text": "Scaling Spherical CNNs Carlos Esteves 1 Jean-Jacques Slotine 2 Ameesh Makadia 1 Abstract Spherical CNNs generalize CNNs to functions on the sphere, by using spherical convolutions as the main linear operation. The most accurate and effi- cient way to compute spherical convolutions is in the spectral domain (via the convolution theorem), which is still costlier than the usual planar convo- lutions. For this reason, applications of spherical CNNs have so far been limited to small problems that can be approached with low model capacity. In this work, we show how spherical CNNs can be scaled for much larger problems. To achieve this, we make critical improvements including novel variants of common model components, an implementation of core operations to exploit hard- ware accelerator characteristics, and application- specific input representations that exploit the prop- erties of our model. Experiments show our larger spherical CNNs reach state-of-the-art on several targets of the QM9 molecular benchmark, which was previously dominated by equivariant graph neural networks, and achieve competitive perfor- mance on multiple weather forecasting tasks. Our code is available https://github.com/ google-research/spherical-cnn. 1. Introduction Spherical convolutional neural networks (Cohen et al., 2018) were introduced as a response to the convolutional neural networks (CNNs) that were central to a series of break- throughs in computer vision (Krizhevsky et al., 2012; He et al., 2016; Simonyan & Zisserman, 2015; Ronneberger et al., 2015). Given the prevalence of spherical data across many applications, it seemed sensible to design neural net- works that possess attributes analogous to those that con- tribute to the success of planar CNNs, such as translation equivariance, spatial weight sharing, and localized filters. 1Google Research, New York, NY , USA2Nonlinear Systems Laboratoty, MIT, Cambridge, MA, USA. Correspondence to: Car- los Esteves <machc@google.com>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Figure 1.Previous spherical CNNs were limited to low resolutions and relatively shallow models. In this work, we scale spherical CNNs by one order of magnitude and show that they can be com- petitive and even outperform state-of-the-art graph neural networks and transformers on scientific applications. In the figure, the num- ber of convolutions in a layer mapping between Cin and Cout channels is counted as CinCout, and the feature map size for a H×W feature with C channels is the number of entries HWC . Much of the ensuing research into designing spherical CNNs (Cohen et al., 2018; Kondor et al., 2018; Esteves et al., 2020) fulfilled these objectives, providing theoretical guarantees on rotation equivariance, the ability to learn local and expressive filters, and faithful models of both scalar and vector fields on the sphere. Nonetheless, these models have not impacted many real- world applications. One reason is that learning from large datasets requires models with adequate representational ca- pacity, and it has not yet been shown that spherical convolu- tion layers can be composed to construct such large models effectively. See Figure 1 – there is no spherical CNN archi- tecture used in practice analogous to common CNN models such as VGG19 (Simonyan & Zisserman, 2015). We are inspired by scientific applications in two areas, drug discovery and climate analysis, that have the potential for broad societal impact. Naturally, both have drawn great in- terest from the machine learning community – for example AlphaFold for predicting 3D structure of proteins (Jumper et al., 2021), and a litany of deep learning approaches for molecular property prediction (Wiedera et al., 2020). Prop- 1 arXiv:2306.05420v1  [cs.LG]  8 Jun 2023Scaling Spherical CNNs erty prediction of small molecules may also be relevant in the design of drugs targeting the interaction between two proteins. For instance, current cancer drugs based on disrupt- ing the binding of tumor suppressor p53 and ubiquitin ligase MDM2 (which targets p53 for degradation) have very low efficiency (Sun, 2006). The second area of interest is short and medium-range weather forecasting (Ravuri et al., 2021; Lam et al., 2022; Rasp et al., 2020). Climate interventions are being considered to mitigate the effects of increased greenhouse gas concentrations in the atmosphere1. As such interventions represent uncharted and potentially dangerous territory, climate prediction models may prove important to improve their safety and effectiveness. Intuitively, both molecular property prediction and climate forecasting problems should benefit from spherical CNNs. The intrinsic properties of molecules are invariant to rota- tions of the 3D structure (atom positions), so representa- tions that are rotation equivariant by design would provide a natural way to encode this symmetry. However, QM9 (Ra- makrishnan et al., 2014), a current standard benchmark for this problem, contains 134K molecules, over 18 times larger than the dataset existing spherical CNNs can accommo- date (Rupp et al., 2012). The scale of this problem neces- sitates models with much greater representation power and computational efficiency. Similarly, climate forecasting datasets (Rasp et al., 2020) represent samples of the Earth’s atmospheric state and thus are ideally represented as spherical signals. Furthermore, in meaningful forecasting applications, models will rely on numerous input variables and the objective is to predict at a high spatial resolution (e.g. 1◦ angular resolution or 64k samples). Such input and output sizes demand large models. In this work we present a systematic and principled approach to scale spherical CNNs. Our contributions include • a design of large scale spherical CNN models, which includes an efficient implementation of spin-weighted spherical harmonic transforms tailored to TPUs, • general purpose layers and activations that improve expressivity and efficiency, • application-specific modeling for molecules and weather forecasting. As the contributions listed above hint at, we observe a naive scaling of existing spherical CNN architectures (simply in- creasing depth and/or width) is insufficient. Rather, our larger models required a measured design that altered mul- tiple standard components such as the nonlinearity, batch normalization, and residual blocks – all of these improved both efficiency and test performance (see Table 1). 1https://www.ametsoc.org/index. cfm/ams/about-ams/ams-statements/ statements-of-the-ams-in-force/ climate-intervention These advancements, along with novel domain-specific in- put feature representations, lead to state of the art perfor- mance on the QM9 benchmark, which has been mostly dom- inated by variations of graph neural networks and transform- ers. Our models are also competitive in multiple weather forecasting settings, showing, for the first time, that spheri- cal CNNs are viable neural weather models. This work shows the feasibility of, and introduces best prac- tices for, scaling spherical CNNs. Based on our findings, we expect our JAX (Bradbury et al., 2018) implementation will provide a platform for further research with spherical CNNs targeting real world applications. 2. Related work 2.1. Spherical CNNs Spherical CNNs have been introduced as the natural exten- sion of standard CNNs to the sphere (Cohen et al., 2018; Esteves et al., 2018), with spherical convolutions computed via generalized Fourier transforms, where the translation equivariance is generalized to 3D rotation equivariance. Later work introduced spectral nonlinearities (Kondor et al., 2018), and extended the equivariance to conformal transfor- mations (Mitchel et al., 2022). One set of approaches applies filters directly to discrete samples of the spherical input. Perraudin et al. (2019) used a rotation equivariant graph CNN based on isotropic filters. Cohen et al. (2019) considered charts of an icosahedral grid, where filters are rotated and shared, yielding approximate rotation equivariance. Shakerinava & Ravanbakhsh (2021) generalized Cohen et al. (2019) to other grids, using less constrained filters that maintain equivariance. Tangentially related are methods that operate on the sphere but are not rotation-equivariant (Coors et al., 2018; Jiang et al., 2019; Su & Grauman, 2019). There have been attempts at improving spherical CNN’s efficiency. Esteves et al. (2020) introduced spin-weighted spherical CNNs, which brought anisotropic filters at smaller cost than the full rotation group Fourier transforms. Cobb et al. (2021) counteracted the feature size expansion caused by the tensor products in Kondor et al. (2018) with heuris- tics to select the representation type at each layer. McEwen et al. (2022) handled high resolution spherical signals by first applying a scattering network with fixed filters (not trainable), followed by downsampling and a spherical CNN. Ocampo et al. (2022) approximated the group convolution integral using a quadrature rule, avoiding expensive general- ized Fourier transforms. These previous attempts were still limited to small and sometimes contrived applications. In this paper, we scale spherical CNNs to a number of oper- ations and feature resolutions one order of magnitude larger 2Scaling Spherical CNNs than prior work (see Figure 1), and apply them successfully to large benchmarks on molecule and weather modeling, showing that they can be comparable and sometimes surpass the state-of-the-art graph and transformer-based models. 2.2. Deep learning for molecules There have been many flavors of message-passing graph neural networks designed specifically for molecules. See Wiedera et al. (2020) and Han et al. (2022) for relevant sur- veys, and a broader discussion not limited to deep learning techniques can be found in von Lilienfeld et al. (2020). Re- garding the deep learning approaches, much of the recent work has focused on 3D equivariant or invariant models. Examples of invariant models include SchNet (Sch¨utt et al., 2017) and DimeNet++ (Klicpera et al., 2020), where the update function only uses invariant information such as bond angles or atomic distances. E(n)-equivariant net- works (Satorras et al., 2021) propose an equivariant up- date function for node coordinates that operates on direc- tional information (displacement between atoms), although the model instantiation for molecules skips this update leading to strict invariance. Related, Tensor Field Net- works (Thomas et al., 2018) also construct an equivariant update function, this one is based on spherical harmon- ics. Cormorant (Anderson et al., 2019) and Steerable E(3)- equivariant GNNs (Brandstetter et al., 2022) can be seen as extensions of TFN, the former noted for using the Clebsch- Gordan non-linearity, and the latter generalizing to E(n) equivariance. PaiNN (Sch¨utt et al., 2021) is a related model whose gated equivariant update block does not rely on spher- ical harmonics. This work also closely examines the loss of directional information in invariant models and finds equivariance allows for models with reduced model size. Related to the equivariant graph models are the equivari- ant transformer approaches such as TorchMD-Net (Th¨olke & Fabritiis, 2022) which updates scalar and vector node features with self-attention. 2.3. Deep learning for weather modeling Mudigonda et al. (2017) and Weyn et al. (2019) utilize vanilla CNNs for extreme weather segmentation and short- term forecasting (“NowCasting”), respectively, while Rasp & Thuerey (2021) uses a ResNet model. Other methods for NowCasting use UNets (Agrawal et al., 2019) and condi- tional generative modeling (Ravuri et al., 2021). In Weyn et al. (2020) and Lopez-Gomez et al. (2022), a cubed sphere representation (projecting the sphere onto six planar segments) is proposed. This approach enjoys some of the computational benefits of traditional CNNs while more closely observing the underlying spherical topology. Jiang et al. (2019) introduces a model for unstructured grids with experiments on extreme weather segmentation on icosa- hedral grids. This orientable CNN model does not offer equivariance to 3D rotations and thus expects inputs to be consistently oriented which is true of climate data. Keisler (2022) recently introduced a graph neural network model inspired by Battaglia et al. (2018). The central com- ponent of this model is a message passing network operating on an icosahedral grid. A similar approach is taken in Lam et al. (2022), with a multi-scale mesh graph representation. The datasets used in most of the recent deep learning re- search for climate modeling, such as WeatherBench (Rasp et al., 2020), consists of equiangular grids derived from reanalysis of the ERA5 data (Hersbach et al., 2020). 3. Background Spherical CNNs. The ubiquitous convolutional neural net- works (CNNs) for image analysis have convolutions on the plane as their main operation, (f ∗ k)(x) = Z t∈R2 f(t)k(x − t) dt, for an input function f and learnable filter k. This opera- tion brings filter sharing between different regions of the image via translation equivariance, which means that given a shifted input f′(x) = f(x + h), the convolution output also shifts: (f′ ∗ k)(x) = (f ∗ k)(x + h). This is one of the main reasons for CNNs high performance. Spherical CNNs generalize this notion to functions on the sphere (f : S2 7→ R) by using spherical convolutions, (f ∗ k)(x) = Z g∈SO(3) f(gν)k(g−1x) dg, (1) where SO(3) is the group of 3D rotations (which can be rep- resented by special orthogonal 3 × 3 matrices), and ν ∈ S2 is a fixed point. Any two points on the sphere S2 are related by a rotation in 3D (in technical terms, the sphere is a homo- geneous space of the group of rotations), and the spherical convolution is equivariant to 3D rotations. Equation (1) was adopted by Esteves et al. (2018), while Cohen et al. (2018) lifts from S2 to SO(3) and performs convolutions on the group, which have an almost identical expression to Equation (1) but with x ∈ SO(3). Esteves et al. (2020) introduced spin-weighted spherical CNNs to overcome the limited expressivity of Esteves et al. (2018) and the computational overhead of Cohen et al. (2018). Spin-weighted spherical functions are complex- valued and their phase changes under rotation. They can also be interpreted as functions on SO(3) (Boyle, 2016) with sparse spectrum. Convolution is computed through products of spin-weighted spherical harmonics coefficients (Esteves et al., 2020). 3Scaling Spherical CNNs Computing generalized convolutions. Approximating spherical and rotation group convolutions with a discrete sum is problematic because there is no arbitrarily dense self- similar grid on the sphere and on the rotation group. Hence, these convolutions are most efficiently and accurately com- puted in the spectral domain, via products of (generalized) Fourier coefficients (Driscoll & Healy, 1994; Kostelec & Rockmore, 2008; Huffenberger & Wandelt, 2010), which correspond to the spherical harmonics decomposition coef- ficients ˆfℓ m for degree ℓ and order m in the case of Equa- tion (1). Even the fastest algorithms for spherical and rota- tion group convolutions are still much slower than a planar convolution with a 3 × 3 kernel on modern devices, which has so far limited the applications of spherical CNNs. In this paper, we adapt the algorithm of Huffenberger & Wandelt (2010) for computing spin-weighted transforms, s ˆfℓ m = Z S2 f(x)sY ℓm(x) dx, (2) f(x) = X ℓ X |m|≤ℓ s ˆfℓ msY ℓ m(x), (3) where sY ℓ m is the spin-weighted spherical harmonic of spin s, degree ℓ, and order m, and 0Y ℓ m corresponds to the stan- dard spherical harmonic Y ℓ m. The forward transform imple- mentation rewrites Equation (2) as s ˆfℓ m = (−1)sim+s r 2ℓ + 1 4π ℓX m′=−ℓ ∆ℓ m′m∆ℓ m′(−s)Im′m, (4) where ∆ℓ m,m′ = dℓ m,m′(π/2) is the Wigner ∆ function, dℓ is a Wigner (small) d matrix, and Im′m is an inner product with f over the sphere, computed by extending f to the torus and evaluating standard fast Fourier transforms (FFTs) on it. Symmetries of the Wigner ∆ enable rewriting the sum in Equation (4) with half the number of terms as ℓX m′=−ℓ ∆ℓ m′m∆ℓ m′(−s)Im′m = ℓX m′=0 ∆ℓ m′m∆ℓ m′(−s)Jm′m, (5) where Jm′m = Im′m + (−1)m+sI−m′m for m′ > 0 and J0m = I0m. The inverse transform rewrites Equation (3) as f(θ, ϕ) = ℓX m′=−ℓ ℓX m=−ℓ eim′θeimϕGm′m, (6) Gm′m = (−1)sim+s X ℓ αℓ∆ℓ (−m′)(−s)∆ℓ (−m′)ms ˆfℓ m, (7) Table 1.Effects of our modeling and implementation contributions. Differences are shown with respect to the results of the previous row. A model similar to the one described in Section 5.1.3 for enthalpy of atomization on QM9 was used for this analysis. ∆ Steps/s [%] ↑ ∆ RMSE [%] ↓ JAX implementation 33.7 0 .0 Phase collapse −4.6 −8.0 No ∆ symmetries 16.3 0 .0 Use DFT 21.4 0 .0 Spectral batch norm 7.8 −1.4 Efficient residual 19.3 −2.4 where αℓ = q 2ℓ+1 4π . Again, the Wigner ∆ symmetries imply Gm′m = (−1)m+sG(−m′)m so the full G is recon- structed by computing only half of its values. The algorithm just described was adopted and implemented in TensorFlow (Abadi et al., 2016) with no changes by Esteves et al. (2020). In this work, we offer a complete rewrite in JAX, tuned for TPUs. This is by itself faster, but we also propose modifications to the algorithm to further improve its speed (see Section 4.2). 4. Method We contribute a fast implementation of spin-weighted spher- ical CNNs in JAX, optimized for TPUs, that can run dis- tributed in dozens of devices. The implementation is about 3×faster than the original, and the ability to run distributed can speed it up 100×or more (we use up to 32 TPUs). Moreover, we introduce a new nonlinearity, normalization layer, and residual block architecture that are more accurate and efficient than the alternatives. Table 1 summarizes the effects on efficiency and accuracy. 4.1. Modeling Phase collapse nonlinearity. Designing equivariant nonlin- earities for equivariant neural networks containing vector or tensor features is challenging. A number of equivari- ant activations appear in the literature (Weiler et al., 2018; Kondor et al., 2018; de Haan et al., 2021; Xu et al., 2022) and typically the best performing one is problem-dependent. Spin-weighted spherical CNNs require specialized activa- tions for nonzero spin features, and Esteves et al. (2020) chose a simple magnitude thresholding. Guth et al. (2021) introduced phase collapse nonlineari- ties for complex-valued planar CNNs with wavelet filters, motivated by 1) translation invariance is usually desirable 4Scaling Spherical CNNs for image classification, 2) in the spectral domain, trans- lations correspond to phase shifts, 3) when applying com- plex wavelet filters to images, which yields complex feature maps, input translations approximately correspond to fea- ture phase shifts, 4) using the modulus as part of the acti- vation collapses the phase, achieving translation invariance and increasing class separability. We adapt these ideas to spin-weighted spherical functions; in our case, we want rotation invariance (or equivariance) for functions on the sphere. The features are complex-valued and an input rotation results in phase shifts when the spin number is nonzero. Thus, using the modulus as part of the activation eliminates these shifts and brings some degree of invariance. The activation mixes all spins but only updates the zero-spin features. Since the nonzero spin features are unaffected, no information is lost by collapsing the phase. Formally, let x0 ∈ CC be the stack of C channels of zero spin at some position on the sphere, andx ∈ CSC be a stack of all S spins in the feature map (including zero). We apply x0 ← W1x0 + W2|x| + b, where W1, W2 and b are learnable parameters. This oper- ation updates only the spin zero; subsequent convolutions propagate information to the nonzero ones. Table 6 shows this nonlinearity brings sizeable performance improvements. Spectral batch normalization. Previous spherical CNNs computed spherical convolutions on the spectral domain and batch normalization on the spatial domain. Batch normalization requires approximating the statistics with a quadrature rule in the spatial domain. Moreover, in the spin-weighted case, zero and nonzero spins need different treatments, which is inefficient. We propose to compute the batch normalization in the spec- tral domain instead. Consider that 1) the coefficient 0fℓ for ℓ = 0 corresponds to the function average, and 2) the variance of the rest of the coefficients is the variance of the function. The normalization is then computed by 1) setting 0f0 to zero and 2) dividing all coefficients by the variance. Similarly, a learnable bias is applied by directly setting 0f0, and a learnable scale is applied to all coefficients. The spectral batch norm is shown to be faster and more accurate than the spatial one in Table 1. It also enables a faster residual block as described next. Spectral pooling and efficient residual block. In contrast with Esteves et al. (2020), and similarly to Cohen et al. (2018), we perform pooling in the spectral domain, which proves to be faster and more accurate. This is because the spatial pooling is sensitive to the sampling grid so it is only approximately rotation-equivariant; it also requires approximation with quadrature weights which adds to the errors. Spectral pooling is implemented by simply skipping the computation of the higher frequency coefficients within a spin-spherical Fourier transform. Spectral pooling is also conceptually different than spatial because it is a global operation while spatial pooling is localized. One potential issue with spectral pooling is in residual lay- ers, where the downsampling happens in the first Fourier transform, so the downsampled spatial input is never com- puted and hence cannot be used in the skip-connection. Our solution is to add the residual connection between Fourier coefficients, which is enabled by the spectral batch nor- malization described earlier. Figure 2 shows our residual block. Table 1 shows it is faster and performs better than the alternative with spatial pooling and batch norms. Figure 2.Our efficient residual block contains spin-weighted spher- ical Fourier transforms (FT) and inverses (IFT), multiplication with filter coefficients (∗K), activation (σ) and spectral batch normal- ization (BN). The residual connection happens in Fourier space. Optionally, spectral pooling is performed at the first FT block. 4.2. Efficient TPU implementation We implement the spin-weighted spherical harmonics trans- forms aiming for fast execution on TPUs (Jouppi et al., 2017)2. This drives our design decisions and sometimes departs from the optimal implementation for CPUs as in- troduced by Huffenberger & Wandelt (2010). The main difference is that TPUs perform matrix multiplications ex- tremely fast, but memory manipulations like slicing and concatenating tensors may quickly become a bottleneck. In particular, the use of Wigner ∆ symmetries to reduce the number of elements computed in Equations (4) and (7) requires slicing, modifying and reconstructing the original tensors in order to cut the number of operations in half. It turns out this is slower on TPU than just computing twice as many operations without the intermediate steps for the architecture we consider, so we skip the computation of Jnm (Equation (5)) completely, and compute all entries of Gnm (Equation (7)) in a single step. Furthermore, Huffenberger & Wandelt (2010) leverage the Fast Fourier transform (FFT) algorithm to reduce asymp- totic complexity of the standard Fourier transforms (from O(n2) to O(n log n)). While there are on-device imple- mentations of the FFT, it turns out that in our cases it is significantly faster to compute Fourier transforms as ma- trix multiplications via the discrete Fourier transform (DFT) matrix. This is because, in a typical neural network pass, we will compute thousands of Fourier transforms (one for each channel for each convolution), but the resolution of 2The implementation is compatible with GPUs as well. 5Scaling Spherical CNNs each transform is relatively small (up to n = 256 in our experiments), so the constant terms dominate and there is no benefit in reducing the asymptotic complexity. Table 1 quantifies the efficiency increase of these changes. 5. Experiments 5.1. Molecular property regression We first demonstrate scaling spherical CNNs for molecu- lar property regression from atoms and their positions in space, a task that was so far dominated by rotation equiv- ariant graph neural networks and transformer-based models (Wiedera et al., 2020; Klicpera et al., 2020; Liao & Smidt, 2022; Th¨olke & Fabritiis, 2022). Previous applications of spherical CNNs (Cohen et al., 2018; Kondor et al., 2018; Cobb et al., 2021) considered only the QM7 (Rupp et al., 2012) dataset, which has 7165 molecules with up to 23 atoms, and a single regression target. However, the much larger QM9 (Ramakrishnan et al., 2014) dataset, which contains 134 000 molecules with up to 29 atoms and 12 different regression targets, has supplanted QM7 as the stan- dard benchmark for this task. The molecules are described by their atom types and 3D positions, and labeled with geo- metric, energetic, electronic, and thermodynamic properties such as enthalpies and free energies of atomization. In this section, we report the first results of spherical CNNs on QM9. The main reason to employ spherical CNNs for this task is their equivariance to 3D rotations, since the molecule properties do not change under rotations. A sec- ondary reason is that we can design rich physically-based features when mapping from molecule to sphere. 5.1.1. S PHERICAL REPRESENTATION OF MOLECULES The first step for applying spherical CNNs is to represent the molecule as spherical functions. Cohen et al. (2018) proposed a map where spheres are placed around each atom, and points on each sphere are assigned a Coulomb-like quan- tity using the charge of the central atom and the distances between points on the sphere and other atoms. We propose an alternative formulation which performs better in practice (see Table 6). Our spherical functions have no assigned radius, so they only contain directional information. The values of these functions are constructed from an inverse power law computed from pairs of atoms, spread out with a Gaussian decay. The input consists of one set of features per atom, with one channel per atom type in the dataset. We sum the contributions of all atoms of the same type. Formally, let zi be the atomic number of atom i and rij the displacement between atoms i and j, we define the one Figure 3. We represent a molecule with a set ofZ functions on the sphere for each atom, where Z is the number of atom types in the dataset. Consider the H2O molecule in the figure and let Z = 2; the rectangles show the two channels for each atom. The values on the sphere come from physically-based interactions between pairs of atoms, smoothed with a Gaussian kernel, and aggregated over atom types. For example, the sphere marked with an H on the top right sums up the Coulomb forces between the oxygen the two hydrogen atoms. input channel of atom i corresponding to atom type z as fiz(x) = X zj=z zizj |rij|p e − (x·rij)2 β|rij| , where β and p are hyperparameters. We set β such that the value is reduced by 95% at 45◦ away from rij. We stack the features for p = 2 and p = 6, which correspond to the power laws of Coulomb and van der Waals forces, respectively. These powers have been shown to perform well by Huang & von Lilienfeld (2016) and we confirm their findings in our setting. Thus, a molecule with N atoms in a dataset containing Z different atom types is represented by 2NZ feature maps. The input representation contains global information since it aggregates interactions between all atoms, however the power law makes it biased towards nearby atoms. Figure 3 depicts the spherical representation of an H2O molecule. This representation is computed on-device using JAX primi- tives and thus is differentiable, enabling future applications such as predicting molecule deformations or interactions. 5.1.2. A RCHITECTURE AND TRAINING We first apply a spherical CNN separately to the input fea- tures, at 32 × 32 resolution, for each atom (up to 29 on QM9). The model contains one standard spin-spherical con- volutional block followed by 5 residual blocks as depicted in Figure 2 (for a total of 11 convolutional layers) with 64 to 256 channels per layer. Our method’s computational cost roughly scales linearly with the number of atoms. This first step results in one feature map per atom. We then apply global average pooling which results in a set of feature vectors, one per atom. Two different methods are used for aggregating this set to obtain per-molecule predictions. The first method, used for most of the QM9 targets, applies 6Scaling Spherical CNNs Table 2. QM9 mean average errors (MAE). We scale spherical CNNs for QM9 for the first time, and show they are competitive with the previously dominant equivariant graph neural networks and transformers. We compare on two splits found in the literature, where “Split 1” has a larger training set. Our model outperforms the baselines on 8 out of 12 targets in “Split 1” and 9 out of 12 targets in “Split 2”. µ α ϵ HOMO ϵLUMO ϵgap < R2 > zpve U0 U H G Cv [D] [ a30] [meV] [meV] [meV] [ a20] [meV] [meV] [meV] [meV] [meV] [ cal mol K] Split 1 DimeNet++ (2020) 0.030 0.044 24 .6 19 .5 32.6 0 .331 1.21 6 .32 6 .28 6 .53 7 .56 0.023 PaiNN (2021) 0.012 0.045 27 .6 20 .4 45 .7 0 .066 1 .28 5.85 5.83 5.98 7.35 0 .024 TorchMD-Net (2022) 0.011 0 .059 20.3 17.5 36 .1 0.033 1 .84 6 .15 6 .38 6 .16 7 .62 0 .026 Ours 0.016 0 .049 21.6 18.0 28.8 0.027 1.15 5.65 5.72 5.69 6.54 0.022 Split 2 EGNN (2021) 0.029 0 .071 29 .0 25 .0 48 .0 0.106 1 .55 11 .00 12 .00 12 .00 12 .00 0 .031 SEGNN (2022) 0.023 0 .060 24 .0 21 .0 42 .0 0 .660 1 .62 15 .00 13 .00 16 .00 15 .00 0 .031 Equiformer (2022) 0.014 0.056 17.0 16.0 33.0 0 .227 1.32 10.00 11.00 10.00 10.00 0.025 Ours 0.017 0.049 22.3 19.1 29.8 0.028 1.19 5.96 5.98 5.97 6.97 0.023 a DeepSets (Zaheer et al., 2017) or PointNet (Qi et al., 2017) aggregation, similarly to Cohen et al. (2018). The second method applies a self-attention transformer (Vaswani et al., 2017) with four layers and four heads, and is applied only to the polarizability α and electronic spatial extent < R2 >, which require more refined reasoning between the atom features for accurate prediction. It is common in the literature to use different aggregation for these and other targets (Th¨olke & Fabritiis, 2022; Sch¨utt et al., 2021). We train for 2000 epochs on 16 TPUv4 with batch size 16; training runs at around 37 steps/s. 5.1.3. R ESULTS Table 2 shows our results on the QM9 dataset. There are two different splits used in the literature, the major difference being that “Split 1” uses a training set of 110 000elements while “Split 2” uses 100 000. We evaluate our model on both splits and compare against the relevant models. Our model outperforms the baselines on 8 out of 12 targets in “Split 1” and 9 out of 12 targets in “Split 2”. 5.2. Weather forecasting We now analyze large spherical CNNs for weather forecast- ing. A unique challenge here is that accurate recordings of weather data are limited to the last few decades, and thus the limited training data motivates a search for the right inductive biases for best generalization. One potential issue to consider is that the Earth has specific topography and orientation in space which influence the weather, and input atmospheric data is always aligned, so one could argue that global rotation equivariance is unnec- essary or even harmful. We claim, however, that equivari- ance is not harmful because we can simply include constant feature channels such as the latitude, longitude, land-sea mask and orography at each point. In fact, current neural weather models (NWMs) do include these constants (Rasp & Thuerey, 2021; Lopez-Gomez et al., 2022; Keisler, 2022). Furthermore, we speculate that rotation equivariance can be beneficial, not in the global sense since inputs are aligned, but in the local sense where local patterns can appear at different orientations at different locations. We evaluate large spherical CNNs on different settings us- ing ERA5 reanalysis data (Hersbach et al., 2020), which combines meteorological observations with simulation mod- els to provide atmospheric data such as wind speed and temperatures uniformly sampled in time and space. 5.2.1. W EATHER BENCH The WeatherBench (Rasp et al., 2020) benchmark is based on ERA5 data, where the data is provided in hourly timesteps for 40 years, for a total of around 350 000exam- ples. The dataset is accompanied by simple baseline models for predicting geopotential height at 500 hPa (Z500) and temperature at 850 hPa (T850) at t + 3 days and t + 5 days given the values at t. In follow-up work, Rasp & Thuerey (2021) applied deep residual networks to similar targets, but now taking a much larger number of predictors including geopotential, wind speed, specific humidity at 7 vertical levels, temperature at 2 m (T2M), total precipitation, and solar radiation. Each predictor is sampled at t, t − 6h and t − 12h, and the constants land-sea mask, orography, and latitude are included as features, for a total of 117 channels. In both settings, the inputs are sampled at 32×64 resolution. Architecture and training. We follow Rasp & Thuerey (2021) and train a spherical CNN with an initial block fol- lowed by 19 residual blocks (as in Figure 2) and no pooling, for a total of 39 spherical convolutional layers, all with 128 channels. We train one model to directly predict Z500, T850 and T2M at 3 days ahead, and another to predict 5 days ahead. We train for 4 epochs on 16 TPUv4 with batch size 32; training runs at around 8.9 steps/s. 7Scaling Spherical CNNs Results. Table 3 shows the results on the test set which comprises years 2017 and 2018. We outperform the baseline on all metrics in the simpler setting that takes two predictors, and show lower temperature errors on the second setting with 117 predictors. The spherical CNN even outperforms models that are pre-trained on large amounts of simulated data on some metrics. We notice that models tend to overfit, and Rasp & Thuerey (2021) employ dropout and learning rate decay based on validation loss to mitigate the issue. We did not use these methods, which might explain our underperforming the geopotential target. Table 3.WeatherBench results. We report the RMSE on geopo- tential height (Z500) and temperature at two verticals (T850 and T2M). The top block follows the protocol from Rasp et al. (2020), the middle follows Rasp & Thuerey (2021). A “cont” superscript indicates a continuous model that takes the lead time as input. Spherical CNNs generally outperform conventional CNNs on this task, and even outperform models pre-trained (superscript “pre”) on large amounts of simulated data on most temperature metrics. 3 days 5 days Z500 T850 T2M Z500 T850 T2M [m2/s2] [K] [K] [ m2/s2] [K] [K] 2 predictors Rasp et al. (2020) 626 2.87 - 757 3.37 - Ours 531 2.38 - 717 3.03 - 117 predictors Rasp & Thuereycont 331 1 .87 1 .60 545 2.57 2.06 Rasp & Thuerey 314 1.79 1.53 561 2.82 2 .32 Ours 329 1.62 1.29 601 2.57 1.89 Pretrained Rasp & Thuereypre 268 1 .65 1 .42 523 2 .52 2 .03 Rasp & Thuereypre,cont 284 1 .72 1 .48 499 2 .41 1 .92 5.2.2. G LOBAL TEMPERATURE FORECASTING Lopez-Gomez et al. (2022) proposed the task of extreme heat forecasting from short to subseasonal (up to 28 days head) timescales. Current physics-based weather models cannot forecast such long lead times, which motivates the use of machine learning. In contrast with Rasp et al. (2020) and Rasp & Thuerey (2021), Lopez-Gomez et al. (2022) does consider the spherical topology and employ an approx- imately uniform cubical sampling on the sphere. Data used for this task is averaged over 24 h, and sampled daily, resulting in around 15 000 examples. Furthermore, data that is not present in WeatherBench is used, such as soil moisture, longwave radiation and vorticity. For the task we consider, Lopez-Gomez et al. (2022) used 20 pre- dictors while we use only the 5 that are present in Weath- erBench, namely temperature at 2 m (T2M), geopotential height at 300 hPa, 500 hPaand 700 hPaand incoming ra- diation. Lopez-Gomez et al. (2022) applied a UNet-like (Ronneberger et al., 2015) model on a 6×48×48 cubemap to forecast 28 channels corresponding to 1 to 28 days T2M. Architecture and training. We used WeatherBench data at 128×128 resolution, which has similar number of samples to the 6×48×48 cubemap. We implement a spherical UNet with 9 spherical convolutional layers with 128 channels each. We train for 5 epochs on 16 TPUv4 with batch size 32; training runs at around 13 steps/s. Results. Table 4 shows a comparison against 3 models introduced by Lopez-Gomez et al. (2022) over the test set (2017 to 2021). HeatNet has a loss biased towards high temperatures, ExtNet is biased towards both hot and cold extremes, while GenNet uses a standard L2 loss like our model. Our model nearly matches GenNet’s performance, even when using a small subset of the predictors. Table 4.Temperature at 2 m(T2M) prediction, following the pro- tocol and comparing against baselines from Lopez-Gomez et al. (2022). Our model nearly matches the best baseline performance, even when using only a small subset of predictors. T2M RMSE [K] Predictors 1 day 2 days 4 days 7 days 14 days 28 days ExtNet 20 1.15 1 .64 2 .11 2 .31 2 .40 2.42 HeatNet 20 1 .26 1 .77 2 .23 2 .42 2 .50 2 .53 GenNet 20 1.13 1.60 2.03 2.22 2.31 2.34 Ours 5 1 .24 1.63 2.04 2.27 2.39 2 .46 5.2.3. I TERATIVE HIGH RESOLUTION FORECASTING Keisler (2022) proposed an iterative graph neural network for weather forecasting. In this setting, predictors and tar- gets have the same cardinality such that the model can be iterated repeatedly to forecast longer ranges, where a sin- gle iteration produces the forecast 6 hahead. Temperature, geopotential height, specific humidity and the three com- ponents of the wind speed, are all sampled at 13 vertical levels, for a total of 78 predictors and targets, at 180×360 resolution. The dataset comprises the years 1979 to 2020 with one example every 3 h, for a total of 120 000examples. Architecture and training. We use the same data as Keisler (2022), but at256×256 resolution, which has approximately the same number of samples as the baseline. We implement a spherical UNet with 7 convolutional layers and a single round of subsampling, because a too large receptive field should not be necessary for predicting 6 h ahead iteratively. The model is repeated up to 12 steps during training, for a total of 84 convolutional layers. We train in 3 stages. The first uses 4 rollout steps (24 h) for 50 epochs on 32 TPUv4 with batch size 32, and is followed by 10 epochs with 8 rollout steps (48 h) and 10 epochs with 12 rollout steps (72 h). Training runs at around 0.92 steps/s, 0.35 steps/s and 0.24 steps/s for each stage. 8Scaling Spherical CNNs Results. Table 5 and Figure 4 show the results. Our model tends to perform better at geopotential but worse at temper- ature forecasts. Keisler (2022) employs a different training procedure where the resolution changes in each stage, and the results are smoothed during evaluation. Table 5.Iterative weather forecasting, following the protocol from Keisler (2022). We compare results for 24h, 72h and 120h lead times, and report the RMSE. Our model tends to perform better at geopotential but worse at the temperature forecasts. 1 day 3 days 5 days Z500 T850 Z500 T850 Z500 T850 Keisler (2022) 64.9 0.730 175 .5 1.17 344 .7 1.78 Ours 58.3 0 .827 167.2 1 .26 340.0 1 .91 5.3. Ablations Activation, pooling, molecule representation. We train a small model with five spherical convolutional layers to regress the enthalpy of atomization H on QM9; it is trained for 250 epochs (in contrast with 2000 epochs in Section 5.1). We compare the effect of replacing each of our main contri- butions and show that each of them increases performance. Specifically, we compare against the magnitude activation used in Esteves et al. (2020), the gated activation introduced by Weiler et al. (2018), which we implement by learning a spin 0 feature map that is squashed and pointwise-multiplies each channel. We also compare against the spherical molec- ular representation introduced by Cohen et al. (2018). Ta- ble 6 shows the results. Table 6. Effects of activation, pooling, molecule representation. We employ a phase collapse activation, compared against the gated nonlinearity of Weiler et al. (2018) and the magnitude thresholding of Esteves et al. (2020). We employ spectral pooling, compared against the spatial pooling from Esteves et al. (2020). We introduce a novel spherical representation of molecules, compared against the one by Cohen et al. (2018). Activation Pooling Molecule QM9/H representation MAE (meV) Ours Ours Ours 15.25 Ours Esteves et al. Ours 16.13 Weiler et al. Ours Ours 16.70 Esteves et al. Ours Ours 17.01 Ours Ours Cohen et al. 20.90 Effects of scaling. In this experiment, we investigate how the resolutions and model capacity affect accuracy in weather forecasting. As in Section 5.2.3, we follow the protocol of Keisler (2022), but only supervising and evaluat- ing the forecasts 6 hin the future. Table 7 shows the results; the most important factors for high performance in this task are the input and feature maps resolutions. Table 7. Effects of scaling. We report the RMSE for geopotential at 500 hPa(Z500) and temperature at 850 hPa(T850), predicting 6 hahead following Keisler (2022). Top row shows our base model. The next block reduces the input resolution. The following row uses separable convolutions in every other layer, which reduces the number of convolutions but keeps the feature size constant. The final block reduces the number of channels per layer, which reduces both the number of operations and feature size. channels convolutions feature size Z500 T850 [m2/s2] [K] 256×256 100% 3.0×105 8.4×107 34.93 0.62 192×192 100% 3.0×105 4.7×107 39.68 0.74 128×128 100% 3.0×105 2.1×107 46.39 0.87 64×64 100% 3.0×105 5.2×106 69.60 1.08 256×256 100% 1.6×105 8.4×107 36.24 0.65 256×256 75% 1.7×105 6.3×106 36.65 0.65 256×256 50% 7.4×104 4.2×106 41.34 0.71 6. Discussion Limitations. The major limitation of our models is still the computational cost – our best results require training up to 4 days on 32 TPUv4, which can be expensive. As a comparison, the baseline for weather (Keisler, 2022) trains in 5.5 days on a single GPU, and the baseline for molecules (Liao & Smidt, 2022) trains in 3 days on a single GPU. From the point of view of the applications, there are con- cerns of how much a model trained on reanalysis is useful for forecasting the real weather, and whether models super- vised by chemical properties at some level of theory like QM9 are useful to estimate the true properties. Conclusion. Spherical CNNs possess numerous qualities that makes them appealing for modeling spherical data or rotational symmetries. We have introduced an approach to scaling these models so they can be utilized on larger problems, and our initial results already reach state of the art or comparable performance on molecule and weather prediction tasks. We hope this work and supporting imple- mentation will allow the research community to revisit this powerful class of models for important large scale problems. Acknowledgments We thank Stephan Hoyer, Stephan Rasp, and Ignacio Lopez- Gomez for helping with data processing and evaluation, and Fei Sha, Vivian Yang, Anudhyan Boral, Leonardo Zepeda- N´u˜nez, and Avram Hershko for suggestions and discussions. 9Scaling Spherical CNNs References Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V ., Warden, P., Wicke, M., Yu, Y ., and Zheng, X. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) , pp. 265–283, 2016. URL https://www.usenix.org/system/files/ conference/osdi16/osdi16-abadi.pdf. Agrawal, S., Barrington, L., Bromberg, C., Burge, J., Gazen, C., and Hickey, J. Machine learning for pre- cipitation nowcasting from radar images. arXiv preprint arXiv:1912.12132, 2019. Anderson, B., Hy, T. S., and Kondor, R. Cormorant: Covari- ant molecular neural networks. In Advances in Neural Information Processing Systems, pp. 14510–14519, 2019. Battaglia, P., Hamrick, J. B. C., Bapst, V ., Sanchez, A., Zambaldi, V ., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G. E., Vaswani, A., Allen, K., Nash, C., Langston, V . J., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, Y ., and Pascanu, R. Relational inductive biases, deep learning, and graph networks. arXiv, 2018. URL https://arxiv.org/ pdf/1806.01261.pdf. Boyle, M. How should spin-weighted spherical functions be defined? Journal of Mathematical Physics , 57(9): 092504, Sep 2016. ISSN 1089-7658. doi: 10.1063/ 1.4962723. URL http://dx.doi.org/10.1063/ 1.4962723. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Brandstetter, J., Hesselink, R., van der Pol, E., Bekkers, E. J., and Welling, M. Geometric and physical quantities improve E(3) equivariant message passing. In Interna- tional Conference on Learning Representations, ICLR , 2022. Cobb, O., Wallis, C. G. R., Mavor-Parker, A. N., Marig- nier, A., Price, M. A., d’Avezac, M., and McEwen, J. Efficient generalized spherical {cnn}s. In International Conference on Learning Representations, 2021. Cohen, T., Weiler, M., Kicanaoglu, B., and Welling, M. Gauge equivariant convolutional networks and the icosa- hedral CNN. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 2019. Cohen, T. S., Geiger, M., K¨ohler, J., and Welling, M. Spher- ical CNNs. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=Hkbd5xZRb. Coors, B., Condurache, A. P., and Geiger, A. Spherenet: Learning spherical representations for detection and clas- sification in omnidirectional images. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. de Haan, P., Weiler, M., Cohen, T., and Welling, M. Gauge equivariant mesh cnns: Anisotropic convolutions on geo- metric graphs. In 9th International Conference on Learn- ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview. net/forum?id=Jnspzp-oIZE. Driscoll, J. R. and Healy, D. M. Computing fourier trans- forms and convolutions on the 2-sphere. Advances in applied mathematics, 15(2):202–250, 1994. Esteves, C., Allen-Blanchette, C., Makadia, A., and Dani- ilidis, K. Learning SO(3) equivariant representations with spherical cnns. In The European Conference on Computer Vision (ECCV), September 2018. Esteves, C., Makadia, A., and Daniilidis, K. Spin-weighted spherical CNNs. In Advances in Neural Information Processing Systems, 2020. Gastegger, M., Behler, J., and Marquetand, P. Machine learning molecular dynamics for the simulation of in- frared spectra. Chem. Sci., 8:6924–6935, 2017. doi: 10.1039/C7SC02267K. URL http://dx.doi.org/ 10.1039/C7SC02267K. Gasteiger, J., Groß, J., and G ¨unnemann, S. Directional message passing for molecular graphs. CoRR, 2020. URL http://arxiv.org/abs/2003.03123v2. Guth, F., Zarka, J., and Mallat, S. Phase collapse in neural networks. CoRR, 2021. URL http://arxiv.org/ abs/2110.05283v1. Han, J., Rong, Y ., Xu, T., and Huang, W. Geometrically equivariant graph neural networks: A survey. arXiv preprint arXiv:2202.07230, 2022. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 10Scaling Spherical CNNs Hersbach, H., Bell, B., Berrisford, P., Hirahara, S., Hor´anyi, A., Mu ˜noz-Sabater, J., Nicolas, J., Peubey, C., Radu, R., Schepers, D., Simmons, A., Soci, C., Abdalla, S., Abellan, X., Balsamo, G., Bechtold, P., Biavati, G., Bidlot, J., Bonavita, M., De Chiara, G., Dahlgren, P., Dee, D., Diamantakis, M., Dragani, R., Flemming, J., Forbes, R., Fuentes, M., Geer, A., Haimberger, L., Healy, S., Hogan, R. J., H ´olm, E., Janiskov ´a, M., Keeley, S., Laloyaux, P., Lopez, P., Lupu, C., Radnoti, G., de Ros- nay, P., Rozum, I., Vamborg, F., Villaume, S., and Th´epaut, J.-N. The era5 global reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730): 1999–2049, 2020. doi: https://doi.org/10.1002/qj.3803. URL https://rmets.onlinelibrary.wiley. com/doi/abs/10.1002/qj.3803. Huang, B. and von Lilienfeld, O. A. Communication: Un- derstanding molecular representations in machine learn- ing: The role of uniqueness and target similarity. The Journal of Chemical Physics, 145(16):161102, 2016. doi: 10.1063/1.4964627. URL https://doi.org/10. 1063/1.4964627. Huffenberger, K. M. and Wandelt, B. D. Fast and exact spin-s spherical harmonic transforms. The Astrophysical Journal Supplement Series , 189(2):255–260, jul 2010. doi: 10.1088/0067-0049/189/2/255. Jiang, C. M., Huang, J., Kashinath, K., Prabhat, Marcus, P., and Nießner, M. Spherical cnns on unstructured grids. In International Conference on Learning Representations, (ICLR), 2019. Jouppi, N. P., Young, C., Patil, N., Patterson, D. A., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., Boyle, R., luc Cantin, P., Chao, C., Clark, C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Ghaem- maghami, T. V ., Gottipati, R., Gulland, W., Hagmann, R., Ho, C. R., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan, A., Khaitan, H., Killebrew, D., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Na- garajan, R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A., Ross, J., Ross, M., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Va- sudevan, V ., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of a tensor processing unit. In ISCA. ACM, 2017. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., ˇZ´ıdek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021. Keisler, R. Forecasting global weather with graph neural networks. CoRR, 2022. URL http://arxiv.org/ abs/2202.07575v1. Kingma, D. P. and Ba, J. Adam: a method for stochastic op- timization. CoRR, 2014. URL http://arxiv.org/ abs/1412.6980v9. Klicpera, J., Giri, S., Margraf, J. T., and G ¨unnemann, S. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. CoRR, 2020. URL http: //arxiv.org/abs/2011.14115v2. Kondor, R., Lin, Z., and Trivedi, S. Clebsch–gordan nets: a fully fourier space spherical convolutional neural network. In Advances in Neural Information Processing Systems, pp. 10138–10147, 2018. Kostelec, P. J. and Rockmore, D. N. Ffts on the rotation group. Journal of Fourier Analysis and Applications, 14 (2):145–179, 2008. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 25, 2012. Lam, R., Sanchez-Gonzalez, A., Willson, M., Wirnsberger, P., Fortunato, M., Pritzel, A., Ravuri, S., Ewalds, T., Alet, F., Eaton-Rosen, Z., Hu, W., Merose, A., Hoyer, S., Holland, G., Stott, J., Vinyals, O., Mohamed, S., and Battaglia, P. Graphcast: Learning skillful medium- range global weather forecasting, 2022. URL https: //arxiv.org/abs/2212.12794. Liao, Y .-L. and Smidt, T. Equiformer: Equivariant graph at- tention transformer for 3d atomistic graphs. CoRR, 2022. URL http://arxiv.org/abs/2206.11990. Lopez-Gomez, I., McGovern, A., Agrawal, S., and Hickey, J. Global extreme heat forecasting using neural weather models. Artificial Intelligence for the Earth Systems , pp. 1 – 41, 2022. doi: 10.1175/AIES-D-22-0035.1. URL https://journals.ametsoc.org/view/ journals/aies/aop/AIES-D-22-0035.1/ AIES-D-22-0035.1.xml . 11Scaling Spherical CNNs McEwen, J., Wallis, C., and Mavor-Parker, A. N. Scatter- ing networks on the sphere for scalable and rotationally equivariant spherical CNNs. In International Conference on Learning Representations, 2022. Mitchel, T. W., Aigerman, N., Kim, V . G., and Kazh- dan, M. M ¨obius convolutions for spherical cnns. In SIGGRAPH ’22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Van- couver, BC, Canada, August 7 - 11, 2022, pp. 30:1–30:9, 2022. doi: 10.1145/3528233.3530724. URL https: //doi.org/10.1145/3528233.3530724. Mudigonda, M., Kim, S., Mahesh, A., Kahou, S., Kashinath, K., Williams, D., Michalski, V ., O’Brien, T., and Prabhat, M. Segmenting and tracking extreme climate events using neural networks. In Deep Learning for Physical Sciences (DLPS) Workshop, held with NIPS Conference, 2017. Ocampo, J., Price, M. A., and McEwen, J. D. Scalable and equivariant spherical cnns by discrete-continuous (disco) convolutions. CoRR, 2022. URL http://arxiv. org/abs/2209.13603v1. Perraudin, N., Defferrard, M., Kacprzak, T., and Sgier, R. Deepsphere: Efficient spherical convolutional neural net- work with healpix sampling for cosmological applica- tions. Astronomy and Computing, 27:130–146, 2019. Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmen- tation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 1(2):4, 2017. Ramakrishnan, R., Dral, P. O., Rupp, M., and von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014. Rasp, S. and Thuerey, N. Data-driven medium-range weather prediction with a resnet pretrained on climate simulations: a new model for weatherbench. Journal of Advances in Modeling Earth Systems, 13(2):nil, 2021. doi: 10.1029/2020ms002405. URL http://dx.doi. org/10.1029/2020MS002405. Rasp, S., Dueben, P. D., Scher, S., Weyn, J. A., Mouatadid, S., and Thuerey, N. Weatherbench: a benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems , 12(11):nil, 2020. doi: 10. 1029/2020ms002203. URL http://dx.doi.org/ 10.1029/2020MS002203. Ravuri, S., Lenc, K., Willson, M., Kangin, D., Lam, R., Mirowski, P., Fitzsimons, M., Athanassiadou, M., Kashem, S., Madge, S., Prudden, R., Mandhane, A., Clark, A., Brock, A., Simonyan, K., Hadsell, R., Robin- son, N., Clancy, E., Arribas Herranz, A., and Mohamed, S. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597:672–677, September 2021. Ronneberger, O., Fischer, P., and Brox, T. U-Net: Convolu- tional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer Assisted Intervention, 2015. Rupp, M., Tkatchenko, A., M¨uller, K.-R., and V on Lilien- feld, O. A. Fast and accurate modeling of molecular atomization energies with machine learning. Physical review letters, 108(5):058301, 2012. Satorras, V . G., Hoogeboom, E., and Welling, M. E(n) equiv- ariant graph neural networks. CoRR, abs/2102.09844, 2021. URL https://arxiv.org/abs/2102. 09844. Sch¨utt, K., Kindermans, P.-J., Felix, H. E. S., Chmiela, S., Tkatchenko, A., and M¨uller, K.-R. Schnet: A continuous- filter convolutional neural network for modeling quantum interactions. In Advances in Neural Information Process- ing Systems, pp. 992–1002, 2017. Sch¨utt, K., Unke, O., and Gastegger, M. Equivariant mes- sage passing for the prediction of tensorial properties and molecular spectra. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Ma- chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9377–9388. PMLR, 18–24 Jul 2021. Shakerinava, M. and Ravanbakhsh, S. Equivariant networks for pixelized spheres. In Proceedings of the 38th Inter- national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , pp. 9477–9488, 2021. URL http://proceedings.mlr.press/v139/ shakerinava21a.html. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Interna- tional Conference on Learning Representations, 2015. Su, Y . and Grauman, K. Kernel transformer networks for compact spherical convolution. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp. 9442–9451, 2019. doi: 10.1109/CVPR.2019.00967. URL http://openaccess.thecvf.com/content_ CVPR_2019/html/Su_Kernel_Transformer_ Networks_for_Compact_Spherical_ Convolution_CVPR_2019_paper.html. Sun, Y . E3 ubiquitin ligases as cancer targets and biomarkers. Neoplasia, 8(8):645–654, 2006. ISSN 1476-5586. doi: https://doi.org/10.1593/neo.06376. URL https://www.sciencedirect.com/ science/article/pii/S1476558606800035. 12Scaling Spherical CNNs Th¨olke, P. and Fabritiis, G. D. Torchmd-net: Equivariant transformers for neural network based molecular poten- tials. In International Conference on Learning Represen- tations, 2022. URL https://openreview.net/ forum?id=zNHzqZ9wrRB. Thomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L., Kohlhoff, K., and Riley, P. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. CoRR, 2017. URL http://arxiv. org/abs/1706.03762v5. von Lilienfeld, O. A., M ¨uller, K.-R., and Tkatchenko, A. Exploring chemical compound space with quantum-based machine learning. Nature Reviews Chemistry, 4(7):347– 358, Jul 2020. Weiler, M., Geiger, M., Welling, M., Boomsma, W., and Cohen, T. S. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. In Advances in Neural Information Processing Systems , pp. 10381– 10392, 2018. Weyn, J. A., Durran, D. R., and Caruana, R. Can machines learn to predict weather? using deep learning to pre- dict gridded 500-hpa geopotential height from historical weather data. Journal of Advances in Modeling Earth Systems, 11(8):2680–2693, 2019. Weyn, J. A., Durran, D. R., and Caruana, R. Improving data- driven global weather prediction using deep convolutional neural networks on a cubed sphere. Journal of Advances in Modeling Earth Systems, 12(9), sep 2020. Wiedera, O., Kohlbachera, S., Kuenemann, M., Garona, A., Ducrota, P., Seidela, T., and ThierryLanger. A compact review of molecular property prediction with graph neural networks. Drug Discovery Today: Technologies, 37:1–12, 2020. Xu, Y ., Lei, J., Dobriban, E., and Daniilidis, K. Unified fourier-based kernel and nonlinearity design for equivari- ant networks on homogeneous spaces. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, pp. 24596–24614, 2022. URL https://proceedings.mlr.press/ v162/xu22e.html. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/ f22e4747da1aa27e363d86d40ff442fe-Paper. pdf. 13Scaling Spherical CNNs A. Appendix A.1. Experimental details We use the Adam (Kingma & Ba, 2014) optimizer and a cosine decay on the learning rate with one epoch linear warmup in all experiments. The inputs of all models are conventional spherical functions (zero spin). The first layer maps it to features of spins zero and one, which are mapped back to spin zero at the last spherical convolutional layer. This last feature is complex- valued, which we convert to real by taking the magnitude. A.1.1. M OLECULAR PROPERTY REGRESSION For the experiments in Section 5.1, we use five spherical residual blocks with resolutions [322, 162, 162, 82, 82] and [64, 128, 128, 256, 256] channels per layer. We minimize the L1 loss with a maximum learning rate of 10−4. Our model applies the spherical CNN independently to each atom’s features, followed by global average pooling, re- sulting in one feature vector per atom. These are further processed by a DeepSets or transformer, as explained in Section 5.1. Finally, we map the set of atom feature vectors to the regression target in three different ways, depending on the target. The dipole moment µ relates to the displacement between atoms and the center of mass, so we use a weighted average by the displacements to aggregate the atom features (as Gastegger et al. (2017)), followed by a small MLP. We compute the electronic spatial extent < R2 > similarly, but using the distance to the center of mass squared as the weights, following Sch ¨utt et al. (2021). For the other tar- gets, which are energy-related, we use the atom types as the weights. Following Gasteiger et al. (2020), we estimate ϵgap as ϵHOMO − ϵLUMO, using the predictions from models the trained for ϵHOMO and ϵLUMO, without training a model specifically for the gap. A.1.2. I TERATIVE HIGH RESOLUTION WEATHER FORECASTING We implement a spherical UNet similar to the one in Appendix A.1.4, with feature maps of resolu- tions [2562, 2562, 1282, 1282, 1282, 1282, 2562, 2562] and [128, 128, 256, 256, 256, 256, 128, 128] channels per layer, which are followed by batch normalization and phase col- lapse Similarly to Keisler (2022), we concatenate a few constant fields to the 78 predictors; namely, the orography, land-sea mask, latitude (sine), longitude (sine and cosine), hour of the year, and hour of the day. The maximum learning rate for the first stage is 2 × 10−4, and we reduced it by a factor of 10 at each subsequent state. A.1.3. W EATHER BENCH For the experiments in Section 5.2.1, we use 64×64 inputs and feature maps, while the baseline is at 32×64. Since the spherical harmonic transform algorithm we use requires the same number of samples along both axes, we upsample the inputs from 32×64 to 64×64. We minimize the L2 loss for this and all weather experiments. A.1.4. G LOBAL TEMPERATURE FORECASTING For the experiments in Section 5.2.2, we implement a spherical UNet with feature maps of resolutions [1282, 642, 642, 322, 322, 322, 322, 642, 642, 1282, 1282], and 128 channels on all convolutional layers, which are followed by batch normalization and phase collapse activation. Features in the downsampling layers are concatenated to the same resolutions in the upsampling layers. A.2. Extra experiments FFT vs DFT. One of our perhaps surprising findings is that computing Fourier transforms via DFT matrix multiplica- tion is faster than using the fast Fourier transform (FFT) algorithm. Here, we investigate whether this remains true for larger input resolutions. We train shallow models with only two spin-spherical convolutional layers on the protocol of Keisler (2022), with upsampled inputs to 512 × 512 and 768 × 768. Table 8 shows the results when running on 32 TPUv4 with batch size of one per device. The direct DFT method performs better than FFT on TPU even at higher resolutions, due the TPU greatly favoring computing a large matrix multiplication instead of running multiple steps on smaller inputs. Table 8. Training time comparison of a shallow model using DFT and FFT for Fourier transform computation, varying the input resolution. FT method resolution steps/s DFT 256 × 256 28.5 FFT 256 × 256 18.7 DFT 512 × 512 12.6 FFT 512 × 512 5.8 DFT 768 × 768 5.1 FFT 768 × 768 1.8 TPUs vs GPUs While we made design decisions with TPUs in mind, the model can also run on GPUs. We evaluated our model for molecules (Section 5.1) on 8 V100 GPUs, with batch size of 1 per device, and it trains at 13.1 steps/s. 14Scaling Spherical CNNs In comparison, the same model trains at 35.6 steps/s on 16 TPUv4. Visualization Figure 4 shows a sequence of predictions of our model for a few variables, compared to the ground truth. 15Scaling Spherical CNNs Figure 4.One day rollout of a few predictions of our model. Top two rows: specific humidity at 850 hPa(Q850). Middle two rows: geopotential height at 500 hPa(Z500). Bottom two rows: temperature at 850 hPa(T500). The first column shows the input values at t = 0, and subsequent columns shows 6 hsteps. On each group of two rows, the top shows the ground truth and the bottom one shows our predictions. Our predictions show that large spherical CNNs are capable of producing high resolution outputs with high frequency details. 16",
      "references": [
        "Tensorflow: A system for large-scale machine learning.",
        "Machine learning for precipitation nowcasting from radar images.",
        "Cormorant: Covariant molecular neural networks.",
        "Relational inductive biases, deep learning, and graph networks.",
        "How should spin-weighted spherical functions be defined?",
        "JAX: composable transformations of Python+NumPy programs",
        "Geometric and physical quantities improve E(3) equivariant message passing.",
        "Efficient generalized spherical {cnn}s.",
        "Gauge equivariant convolutional networks and the icosahedral CNN.",
        "Spherical CNNs.",
        "Spherenet: Learning spherical representations for detection and classification in omnidirectional images.",
        "Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs.",
        "Computing fourier transforms and convolutions on the 2-sphere.",
        "Learning SO(3) equivariant representations with spherical cnns.",
        "Spin-weighted spherical CNNs.",
        "Machine learning molecular dynamics for the simulation of infrared spectra.",
        "Directional message passing for molecular graphs.",
        "Phase collapse in neural networks.",
        "Geometrically equivariant graph neural networks: A survey.",
        "Deep residual learning for image recognition.",
        "The era5 global reanalysis.",
        "Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity.",
        "Fast and exact spin-s spherical harmonic transforms.",
        "Spherical cnns on unstructured grids.",
        "In-datacenter performance analysis of a tensor processing unit.",
        "Highly accurate protein structure prediction with AlphaFold.",
        "Forecasting global weather with graph neural networks.",
        "Adam: a method for stochastic optimization.",
        "Fast and uncertainty-aware directional message passing for non-equilibrium molecules.",
        "Clebsch–gordan nets: a fully fourier space spherical convolutional neural network.",
        "Ffts on the rotation group.",
        "Imagenet classification with deep convolutional neural networks.",
        "Graphcast: Learning skillful medium- range global weather forecasting",
        "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs.",
        "Global extreme heat forecasting using neural weather models.",
        "Scattering networks on the sphere for scalable and rotationally equivariant spherical CNNs.",
        "M¨obius convolutions for spherical cnns.",
        "Segmenting and tracking extreme climate events using neural networks.",
        "Scalable and equivariant spherical cnns by discrete-continuous (disco) convolutions.",
        "Deepsphere: Efficient spherical convolutional neural network with healpix sampling for cosmological applications.",
        "Pointnet: Deep learning on point sets for 3d classification and segmentation.",
        "Quantum chemistry structures and properties of 134 kilo molecules.",
        "Data-driven medium-range weather prediction with a resnet pretrained on climate simulations: a new model for weatherbench.",
        "Weatherbench: a benchmark data set for data-driven weather forecasting.",
        "Skilful precipitation nowcasting using deep generative models of radar.",
        "U-Net: Convolutional networks for biomedical image segmentation.",
        "Fast and accurate modeling of molecular atomization energies with machine learning.",
        "E(n) equivariant graph neural networks.",
        "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions.",
        "Equivariant message passing for the prediction of tensorial properties and molecular spectra.",
        "Equivariant networks for pixelized spheres.",
        "Very deep convolutional networks for large-scale image recognition.",
        "Kernel transformer networks for compact spherical convolution.",
        "E3 ubiquitin ligases as cancer targets and biomarkers.",
        "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.",
        "Torchmd-net: Equivariant transformers for neural network based molecular potentials.",
        "Attention is all you need.",
        "Exploring chemical compound space with quantum-based machine learning.",
        "3d steerable cnns: Learning rotationally equivariant features in volumetric data.",
        "Can machines learn to predict weather? using deep learning to predict gridded 500-hpa geopotential height from historical weather data.",
        "Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere.",
        "A compact review of molecular property prediction with graph neural networks.",
        "Unified fourier-based kernel and nonlinearity design for equivariant networks on homogeneous spaces.",
        "Deep sets."
      ],
      "meta_data": {
        "arxiv_id": "2306.05420v1",
        "authors": [
          "Carlos Esteves",
          "Jean-Jacques Slotine",
          "Ameesh Makadia"
        ],
        "published_date": "2023-06-08T17:59:08Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a principled framework for scaling spherical convolutional neural networks (CNNs) an order of magnitude beyond prior work. Key contributions include (1) an optimized JAX implementation of spin-weighted spherical harmonic transforms that exploits TPU hardware, (2) novel architectural components—phase-collapse nonlinearity, spectral batch normalization, spectral pooling with Fourier-space residual connections—improving both speed and accuracy, (3) task-specific spherical input representations for molecules and weather data, and (4) empirical demonstration that large spherical CNNs achieve state-of-the-art results on the QM9 molecular benchmark and competitive performance on multiple weather-forecasting tasks.",
        "methodology": "Builds deep spin-weighted spherical CNNs whose convolutions are executed in the spectral domain via products of spin-weighted spherical harmonic coefficients. Efficiency is gained by: replacing FFTs with dense DFT matrix multiplies, omitting Wigner-Δ symmetry reductions that cause slicing overhead on TPUs, and distributing computation across up to 32 TPUv4 devices. Architectural innovations include a phase-collapse activation (magnitude-based update of spin-0 channels), spectral batch normalization using coefficient statistics, and residual blocks that perform pooling and skips entirely in Fourier space. Domain-specific adaptations map molecules to sphere functions using inverse-power-law kernels per atom type and encode weather fields together with constant geographic channels.",
        "experimental_setup": "1) Molecular property regression: QM9 dataset (134k molecules, 12 targets). Models with 11 convolutional layers (64–256 channels) applied per-atom at 32×32 resolution; atom features aggregated with DeepSets or a 4-layer transformer. Evaluated on two common train/val/test splits; metric: mean absolute error. Baselines include DimeNet++, PaiNN, TorchMD-Net, EGNN, SEGNN, Equiformer. 2) Weather forecasting: (a) WeatherBench—ERA5 reanalysis at 32×64 grid, 40-year hourly data. Single model with 39 layers (128 channels) predicts Z500, T850, T2M at 3- and 5-day lead times; RMSE metric. (b) Extreme-heat task—daily ERA5, 5 predictors on 128×128 grid, spherical UNet (9 layers, 128 channels) forecasts 1–28-day T2M; RMSE metric. (c) Iterative high-resolution forecasting—ERA5 3-hourly data, 78 variables on 256×256 grid; spherical UNet with 7 layers iterated up to 72 h; RMSE on Z500, T850. Training uses Adam with cosine decay, batch sizes 16–32, 4–50 epochs, and up to 32 TPUv4s.",
        "limitations": "• High computational cost: best models require several days on 32 TPUv4s; still slower than planar CNN or GNN baselines on single GPUs. • Spectral transforms demand square equi-angular grids, limiting flexibility in grid choice and complicating very high resolutions. • Global rotation equivariance may be unnecessary for Earth-fixed climate data and could hinder learning, despite added constant channels. • Experiments confined to QM9 and ERA5; generalization to larger molecular datasets, higher-resolution weather, or other spherical domains untested. • Reliance on reanalysis and DFT-level molecule labels introduces label noise and potential mismatch with real-world targets.",
        "future_research_directions": "1) Develop lower-cost approximations (e.g., learned sparse spectra, mixed-precision, kernel approximations) to further reduce compute. 2) Extend to higher-resolution and multi-scale spherical data for sub-kilometre climate or astronomy applications. 3) Combine spherical CNNs with graph or transformer modules for hybrid geometric learning. 4) Explore adaptive or learned spherical sampling grids to relax equi-angular constraint. 5) Investigate pretraining and transfer learning across spherical domains, and study robustness when rotational symmetry is only approximate. 6) Apply the framework to additional scientific problems such as protein–protein interactions, cosmology, and global remote sensing.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation",
      "full_text": "OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation Bohao Peng1 Xiaoyang Wu2 Li Jiang3 Yukang Chen1 Hengshuang Zhao2 Zhuotao Tian4 Jiaya Jia1 1CUHK 2HKU 3CUHK, Shenzhen 4HIT, Shenzhen Abstract The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of- the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adap- tivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of net- works that integrates a lightweight module to greatly en- hance the adaptivity of sparse CNNs at minimal computa- tional cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5× better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks. Our code is built upon Pointcept [9], which is available at here1. 1. Introduction 3D scene understanding is critical in various practical appli- cations, including robotics, autonomous driving, and aug- mented reality [13, 16, 19, 20, 30, 56, 71, 72, 75]. In con- trast to images, which typically exhibit densely and uni- formly arranged pixels [10, 23, 36, 50, 52, 53], 3D point clouds often manifest irregular and scattered distributions. It leads to various feature extractors in 3D scene under- standing. There are two mainstream 3D networks. The first is 1https://github.com/Pointcept/Pointcept Small LargeRaw PointsReceptive Fields TableChairWallJunction Specific Objects in Red Boxes ①② ③ ④ ① ② ③ ④ Figure 1. Visualization of 3D scene receptive fields controlled by our proposed adaptive aggregator. Objects’ edges and junctions re- quire smaller receptive fields due to their sophisticated structures, while flat planes and unitary structures require broader fields. point-based networks [42, 43], which advocate directly ma- nipulating the unstructured points. Thanks to the flexibility of point-wise operations, point-based methods, particularly those with transformer architectures [12, 32, 37, 40, 51, 54, 55, 74], have gradually become dominant. The second is sparse CNNs [7, 13], where irregular point clouds are con- verted into voxels during data preprocessing. This allows us to leverage thelocally structured benefits and facilitate high efficiency. Due to this practical value, sparse CNNs have been widely exploited in existing literature [38, 46, 66, 76]. However, its accuracy is usually inferior to its transformer counterparts [21, 37, 61, 74], especially in 3D scene seman- tic segmentation. Given the high potential of sparse CNNs, we carefully examine the inner reasons for the performance gap in this paper. We find that the key distinction between sparse CNNs and point transformers behind is adaptivity – the 1 arXiv:2403.14418v1  [cs.CV]  21 Mar 2024Stratified Transformer PTv2 MinkUNet PTv2 Point Transformer Sparse UNet (b) Memory and mIoU(a) Speed and mIoU 76 75 74 73 72 71 70 100 1730 ms200 300 400 500 mIoU 76 75 74 73 72 71 70 mIoU 1 G3 5 7 9 Ours SparseUNet Ours OctFormer Figure 2. Comparison between various transformer-based [21, 61, 74] and CNN-based [7, 13] within RTX 3090. For OctFormer, we reproduce the official repository and include the cost of building the octree. If a method has multiple versions, they are indicated by different dots. latter can flexibly adapt to individual contexts while it may not be feasible for the former with static perception. With- out degrading efficiency, we bridge this gap via two key components: (1) spatially adaptive receptive fields, and (2) adaptive relations. Adaptively adjusting receptive fields via attention mechanisms is one of key designs in transformer-based frameworks [55, 74] to achieve top performance. Intu- itively, different parts of the 3D scene with various geomet- ric structures and appearances should be catered with differ- ent receptive sizes, as visualized in Fig. 1. Flat and sparse regions like the wall and floor need large receptive fields to yield consistent predictions with broader cues, while so- phisticated parts like the plane junctions and small objects need smaller ones to screen unnecessary context that may overwhelm the local details. To enable our CNN-based framework to adaptively perceive the contextual informa- tion, we partition the 3D scene into non-overlapping pyra- mid grids. We then utilize the proposed Adaptive Rela- tion Convolution (ARConv) in multiple scales and design a selective aggregator to adaptively aggregate the multi- scale outputs based on the local characteristics . Instead of pursuing consistent large receptive fields (like LargeK- ernel3D [6]), we find that this adaptive manner is sufficient and more efficient. Adaptive relationships , achieved via self-attention maps, is another key strength over CNNs. To facilitate the establishment of relationships among local contexts, we in- troduce a multi-one-multi paradigm in ARConv, as depicted in Fig. 6. Specifically, we dynamically generate kernel weights for non-empty voxels based on their correlations with the grid centroid. By adopting this approach, we can maintain a lightweight design[59] with a linear complexity proportional to the voxel quantity, whicheffectively expands the receptive fields and achieves optimal efficiency. Extensive experiments validate our approach’s effective- x yz Raw Points MLP VoxelizationSparse Conv Reference PointsReceptive Field Comparison ConvNetPointNet Figure 3. Comparisons between the 3D point-based [42, 74] and convolutional networks [7, 13]. PointNets directly process the raw points and provide more flexible and broader receptive fields. Con- vNets handle structural data after additional voxelization pretreat- ment with higher efficiency and lower consumption. ness, and our designs enable sparse CNNs to outperform state-of-the-art point-based methods with transformer ar- chitectures, with little efficiency compromise, as shown in Fig. 2. We conduct the comparisons under the same ex- perimental settings, without any additional pretraining or auxiliary methods. Remarkably, it achieves mIoU scores of 76.1%, 78.9%, and 70.6% on the ScanNet v2 [11], nuScenes [4], and SemanticKITTI [2] validation bench- marks, respectively. It highlights the potential of sparse CNNs over transformer-related models in both performance and efficiency, regardless of indoor or outdoor scenes. In conclusion, our contributions are listed as follows: • We analyze and find that adaptivity is the key to bridging the gap between sparse CNNs and point transformers. • We propose OA-CNNs as solutions, consisting of dy- namic receptive fields and adaptive relation mapping. • Our method outperforms state-of-the-art methods with promising efficiency on popular benchmarks including ScanNet v2, ScanNet200, nuScenes and SemanticKITTI semantic segmentation. 2. Related Work Point-based learning. Point-based methods advocate di- rectly processing the unstructured raw points without any additional regulation pretreatment [14, 18, 31, 64, 68]. PointNet [42] is pioneering work in this trend which lever- ages point-wise MLP and permutation invariance operation to obtain the global feature of input points. More details and comparisons are shown in Fig. 3. Several follow-up works [15, 18, 43] continue to strengthen their capabilities through hieratical multi-scale perception and local-global feature aggregation. Especially with the development of the attention mechanism [55, 69, 70], point-wise perception with the transformer architecture [21, 61, 63, 74] provides long-range dependences and bridges global contexts rela- tionships. These frameworks have shown outperforming su- 2periority and gradually become dominant. However, atten- tion calculation and point-wise operation suffer from more expensive computation and memory consumption, and the complex architecture also makes them more challenging to deploy. CNN-based learning. Compared with dense images ar- ranging pixels into a rasterized grid, point cloud directly records the points’ spatial coordinates, which are typically irregular and lack unified metrics. Projection-based [5, 24– 26, 29, 47] methods intuitively project the raw 3D points into flat images from various views, and the subsequence operations are logically the same as the 2D pipeline. How- ever, the projection seriously destroyed the point cloud’s geometrical information, especially for the in-door scenes with more stereoscopic structures. An alternative technique is to quantize the 3D scene and transform irregular point clouds into regular voxel representation [3, 38, 39, 46]. 3D convolutions are commonly applied to handle these voxel collections while consuming high computation and mem- ory. Sparse and submanifold convolutions [13] are in- troduced to alleviate these issues and improve efficiency. Sparse convolution introduces the hash table for the voxels’ indices retrieval, which is convenient and efficient. More- over, 3D submanifold convolution has made a further re- striction only processing the non-empty elements sacrific- ing some flexibility in change for more efficiency and less consumption. However since the complexity of the kernel size is O(K3), the receptive fields of sparse convolutions are still limited by the parameter quantity, which seriously restricts the global perception ability. In this work, we ex- plore a lightweight design [59] to expand 3D convolution with an adaptive receptive range [27]. Dynamic convolutions. Regular convolutions optimize the learnable kernel weights during training and fix ker- nel weights in the inference process. Dynamic convolu- tion [17, 67] proposes to generate the convolution kernel adaptively depending on the specific conditions. Previous works [49, 60, 65] have widely explored introducing dy- namic convolution into sparse data processing. However, these works are also based on point-wise methods and typi- cally generate kernel weights depending on the relative po- sition information, which requires expensive computation and memory consumption. In this work, we inherit condi- tional convolution to propose a lightweight grid convolution with a regular structure. Moreover, we introduce the adap- tive aggregator for the multi-scale pyramid aggregation to bridge extended-range contexts efficiently. 3. Omni-Adaptive 3D Sparse CNNs In this section, we provide a detailed introduction to our designed lightweight modules and their application in con- structing a series of omni-adaptive 3D sparse CNNs (OA- ARConvARConvARConvLinearSoftMax∑ ×××Linear𝑤ଵ𝑤ଶ𝑤ଷSumPyramid Grid Partition𝑔ଵ𝑔ଶ𝑔ଷ𝒐௜,ଵ𝒐௜,ଶ𝒐௜,ଷ𝒇௜ ConcatLinear Figure 4. Illustration for the adaptive aggregator, which learns to aggregate various grid contexts under multi-pyramid scales from the voxel’s instinct characteristics. CNNs). It surpasses point transformers in 3D recognition with limited latency/memory overhead. OA-CNNs consist of three design contents, i.e., spatially adaptive receptive fields in Sec. 3.1, Adaptive Relation Convolution (ARConv) in Sec. 3.2, and the overall architecture in Sec. 3.3. 3.1. Spatially adaptive receptive fields Motivation. Various receptive field sizes are required in distinct positions and objects in one 3D scene. For exam- ple, as shown in Fig. 1, regions belonging to the wall and floor are relatively flat and elementary, which require larger receptive fields to yield consistent predictions. However the geometric structures of the plane junction or sophisti- cated objects are more complex and need smaller recep- tive fields to retain the local characteristics. Transformer frameworks [21, 55, 74] adjust the perception range by the attention mechanism retrieving the relevance with the sur- rounding contexts but significantly increasing memory and computing consumption. However, sparse CNNs lack the ability to handle this issue. In OA-CNNs, we overcome this by directly determining the perception size with the aid of the intrinsic voxel features, as illustrated in Fig. 4. Voxel grid. Expanding the receptive field is necessary for pursuing adaptive perception since the typical 3D convo- lution kernel size is generally set as 3 × 3 × 3 limited by the parameter quantity. To achieve this, we utilize the voxel grid in our approach. Formally, define V = (P, F) as a sparse voxelized 3D scene representation containing a set of voxels vi = ( pi, fi), where pi ∈ R3 represents the positional integer indice and fi ∈ Rd is the corre- sponding feature with d channels. The global voxel set V is then partitioned into N non-overlapping voxel grids [V1, V2, . . . ,VN ], Vi = {vj | pj ∈ Ω(i)}, where Vi indi- cates i-th voxel grid and Ω(i) obtains i-th voxel grid’s in- dices range. The voxel grid size can be considerably larger than that of the typical 3D convolution kernel, such that the receptive field is effectively expanded. 3Pyramid grid partition. Although a sufficiently large grid size can provide a global view, it may not be able to capture intricate details for sophisticated objects. In an ef- fort to prepare the alternative grid sizes for adaptively ac- commodating different areas, we rasterize the entire 3D scene into pyramid voxel grids. Specifically, let’s define G = {gk}K as the set of K grid sizes partitioning the 3D space, where K is set as 3 in our experiments. The output oi ∈ Rk×d of the i-th voxel grid underk-th scale is obtained as: oi,k,: = Conv({fj | pj ∈ Ω(i, gk)}), (1) where Ω(i, gk) represents the range of voxel indices in the i-th voxel grid in the sizegk, and Conv(·) indicates the con- volution for aggregating voxel features in the voxel grids to get the voxel grid feature. Observing the intolerably heavy parameters associated with the standard sparse 3D convolu- tion Conv(·) using a large kernel, we introduce the ARConv in Sec. 3.2 as a solution to this issue. The ARConv im- proves results without sacrificing efficiency and establishes relationships among the voxel grid. Adaptive aggregator. To achieve a customizable recep- tive field, we propose an adaptive aggregator that au- tonomously adjusts the receptive fields based on the intrin- sic characteristics and spatial structure of individual voxels, which is illustrated in Fig. 4. Given K multi-scale grid par- titions with sizes G = {gk}K, our proposed adaptive aggre- gator weights and fuses the multi-scale outputs. We use a learnable function δadp to predict the preference weightswi of K grid sizes as: wi = SoftMax(δadp(fi)), (2) where w ∈ RNi×K, Ni denotes the number of voxels in- side the i-th voxel grid, and δadp : Rd 7→ RK is a learnable linear layer and SoftMax (·) denotes the softmax operation over K grid sizes. We subsequently employ the predicted weights to aggregate the convolution outputs, which contain global information, with the original features to enhance them, f′ i = δout(δproj(fi) ⊕ KX k=1 wi,k · oϕ′(i,k),k), (3) where δout : R2d 7→ Rd and δproj : Rd 7→ Rd are two linear layers with normalization and activation, ⊕ denotes vector concatenation and ϕ′(i, k) reversely returns the voxel grid index containing the i-th voxel under gk grid size partition. So far, we have presented a method for constructing the spatially adaptive receptive fields based on individual con- text, but it is not yet capable of establishing adaptive rela- tionships as the point-based transformer counterparts. Voxel Grid Linear𝑓!(#,%)𝑓!(#,') 𝑓!(#,(!)… CentroidVoxel Features Weights 𝑊!,:,$𝑊!,:,% 𝑊!,:,&! Weights Generation SoftMax …………… ……… ℝ)×+ ℝ)×+ HadamardProductSum Outputℝ+ Figure 5. Illustration of the Adaptive Relation Convolution (AR- Conv). It dynamically generates grid convolution’s kernel weights only for the non-empty voxels with their relationships to the cen- troid voxel. 3.2. Adaptive relation convolution Observations. Transformer frameworks [40, 61] have achieved remarkable success and become one of the domi- nant architectures in 3D semantic segmentation. Their per- formance superiority is largely related to the ability of re- lation learning among various local point features. It is achieved by self-attention mechanisms and essentially in- creases the representation capacity. However, plain sparse CNNs miss this design. On the other hand, CNNs have verified, via extensive re- search [6, 33, 58], the importance of large receptive fields to enable a global perception. Unfortunately, 3D convolution struggles to improve perception range by directly expand- ing the convolution kernel since its complexity is O(K3), where K is the kernel size, indicating that the consump- tion of the large kernel may be unacceptable in practice, especially for the edge devices. To this end, we explore the large-kernel design to be lightweight and propose the Adap- tive Relation Convolution (ARConv), which incorporates the aforementioned adaptive relation reasoning into sparse CNNs. More details are illustrated in Fig. 5. Depthwise convolution. To assemble the framework in a lightweight manner, we could start by considering depth- wise convolution for parsing the voxel grid features. In practical applications, it is also found that the depthwise convolution generalizes better [59] and converges faster as shown in our experiments. Compared with regular convo- lutions performed over multiple input channels, depthwise convolutions independently apply a single convolutional fil- ter for each input channel and keep each channel separate. The output for i-th voxel grid feature oi ∈ Rd and c-th di- 4Reference Set Query Set Centroid Voxel Block K-Nearest Neighbors Multi-to-MultiMulti-One-Multi GridPartition Figure 6. Comparison of the multi-to-multi paradigm in point transformers with the multi-one-multi paradigm in OA-CNNs. mension can be precisely described as, oi,c = XNi j=1 Wi,c,j · fϕ(i,j),c, (4) where Ni is the number of non-empty voxels in the i-th voxel grid Vi, Wi ∈ Rd×Ni indicates the learnable kernel weight and ϕ(i, j) returns the j-th non-empty voxel index in i-th voxel grid. Adaptive relation kernel. For accomplishing the adap- tive relation reasoning, the attention mechanisms [55, 74] adopt the multi-to-multi paradigm, which incorporates a “reference set” [43, 61] for capturing long-range depen- dencies through multiple queries and keys. However, this approach results in significant inference time and memory demands on GPUs. In contrast, we propose a more effi- cient multi-one-multi pipeline, generating a single centroid voxel of the grid, which serves as the agent for capturing long-range relationships. This strategy facilitates efficient computation and lowers memory consumption, while still enabling the extraction of complex relationships among the non-empty voxels in the grid. The idea is illustrated in Fig. 6. Specifically, for the sub voxel grid Vi, its corresponding centroid voxel feature fctr i ∈ Rd, where d indicates the number of channels, is formed as: fctr i = AvgPool({δproj(fj) | pj ∈ Ω(i)}), (5) where AvgPool(·) applies 3D average pooling over the in- put, Ω(·) indicates the subset’s indices range, and δproj : Rd 7→ Rd is a linear projection layer with normalization and activation. Then the dynamic kernel weight Wi ∈ Rd×Ni of the depthwise convolution for the i-th voxel grid is generated by considering voxels’ feature correlations with the centroid voxel: Wi,:,j = δweight(fϕ(i,j) − fctr i ), (6) EmbeddingDownSampleConvBlockDownSampleConvBlockDownSampleConvBlockDownSampleConvBlock UpSample UpSample UpSample UpSample ×(% InputOutput ×(& ×() ×(* Linear UpSample$#$ $#%&$ ARConv&Aggregator Submanifold ConvSubmanifold ConvKernel(3, 3, 3)  Stride1 Sparse Conv PointsVoxelizationStem LayerKernel(2, 2, 2)  Stride2 ×(% Embeddings Embeddings Figure 7. Illustration for the whole architecture and more imple- mentation details. where δweight : Rd 7→ Rd is a linear projection layer, and ϕ(i, j) returns the j-th non-empty voxel index in i-th voxel grid. We normalize the dynamically generated weights Wi,:,j using softmax operation along each channel separately across the whole voxel grid. The normalization enhances the stability of the neural network outputs during training and assigns feature weights based on internal relevance be- tween the specific voxel and the centroid voxel. Mathemat- ically, for the c-th channel, W ′ i,c,j = exp(Wi,c,j − Max(Wi,:,:))PNi k=1 exp(Wi,c,k − Max(Wi,:,:)) , (7) where Max(·) returns the maximum value. We empirically find that the dynamically generated weights were volatile at the early training phase, yielding large values that may cause the exponential function explosion and lead to “inf” outputs. Thus, we adopt an additional operation in Eq. (7) that subtracts the maximum values from the numerator and denominator respectively to prevent the explosion without affecting the output – it is numerically equal to the case without this operation. In essence, we have introduced an efficient approach named Adaptive Relation Convolution (ARConv) that gen- erates kernel weights only for the non-empty voxels dynam- ically by considering their correlations to the geometric cen- troid representatives, thus achieving effectiveness without sacrificing efficiency. 3.3. Architecture In this section, we provide the architectural details of the OA-CNNs. Fig. 7 depicts the overall structure. Concretely, the sparse and submanifold voxel mod- ules [13, 37] both process spatially sparse data effectively. The primary difference between them is that submanifold convolution only handles the non-empty voxels in the 3D 5scene and strictly preserves the original geometric struc- ture. Differently, sparse convolution can extract features at empty locations and is more flexible. We construct our basic blocks with an ARConv module followed by two subman- ifold convolutions with necessary normalization and acti- vation layers. Following [41, 43], we adopt the hieratical structure to the encoder and use a sparse convolution with kernel size and stride that are both set to (2, 2, 2), down- sampling the spacial size to 1/8 at each time. As for the upsampling process, the up-block only consists of a skip connection and a single linear layer that aligns the feature channel numbers without other components. 4. Experiments 4.1. Implementation details. Datasets. We conducted experiments using our proposed OA-CNNs on the standard benchmark, ScanNet v2 [11], as well as its recent extension, ScanNet200 [45], and the S3DIS dataset [1] for indoor scenes. ScanNet v2 con- tains 1,201 training scenes and 312 validation scans recon- structed from RGB-D frames. The model utilizes recon- structed meshes to sample point clouds as input, where each point cloud is attributed a semantic label from a set of 20 categories. ScanNet200 benchmark extends the class cate- gories to 200, an order of magnitude more than the previ- ous. The S3DIS dataset consists of 271 rooms in six areas from three different buildings with 13 categories. Following a standard protocol, area 5 is withheld during training and used for S3DIS testing. As for the outdoor semantic seg- mentation, we select two popular benchmarks, nuScenes [4] and SemanticKITTI [2]. The nuScenes dataset contains approximately 1000 scenes, with each scene consisting of multiple sensor sweeps captured from a moving vehicle. In contrast, the SemanticKITTI dataset consists of sequences from the raw KITTI dataset, which contains 22 sequences in total. Each sequence includes around 1,000 lidar scans, corresponding to approximately 20,000 individual frames. Training details. We train our models on 4 RTX 3090 GPUs with the batch size and the number of epochs set to 16 and 100, respectively. With the considerations re- garding computational efficiency and memory constraints, the training process leverages a subset of up to 100,000 randomly sampled points from the point cloud. In con- trast, the full point cloud is used during validation to en- sure an unbiased and rigorous evaluation of the model’s per- formance. Moreover, we attribute parts of the point-based frameworks’ performance superiority to the modern train- ing strategy with advanced data enhancement [44, 61]. We refer to these strategies to train our models. Specifically, we use the AdamW optimizer [34] for parameter optimization, which is widely used in transformer architectures. The ini- Method Input Val mIoU Test mIoU PointNet++ [43] point 53.5 55.7 PointNeXt-XL [44] point 71.5 71.2 PointCNN [28] point - 45.8 KPConv [49] point 69.2 68.6 PointConv [60] point 61.0 66.6 PointTransformer [74] point 70.6 - FastPointTransformer [40] point 72.1 - Stratified Transformer [21] point 74.3 73.7 OctFormer [57] point 75.7 76.6 PTv2 [61] point 75.4 75.2 SparseUNet [13] voxel 69.3 72.5 MinkowskiNet [7] voxel 72.2 73.6 LargeKernel3D [6] voxel 73.2 73.9 OA-CNNs(ours) voxel 76.1 75.6 Table 1. We compared semantic segmentation results on ScanNet v2. All the selected methods are under the same experimental set- tings without the use of additional pretraining or auxiliary meth- ods. Outdoor Sem. Seg. Benchmarks Method nuScenes [4] SemanticKITTI [2] SparseUNet [13] 73.3 63.8 SPVNAS [48] 77.4 64.7 Cylender3D [77] 76.1 64.3 SphereFormer [22] 78.4 67.8 OA-CNNs(ours) 78.9 70.6 Table 2. Results on outdoor semantic segmentation benchmarks. Method Val Test Head Comm. Tail All All MinkowskiNet [7] 48.3 19.1 7.9 25.1 25.3 LGround [45] 51.5 22.7 12.5 28.9 27.2 SparseUNet [62] - - - 28.8 - OctFormer [57] - - - - 32.6 PTv2 [62] - - - 29.3 - OA-CNNs(Ours) 51.3 28.0 17.7 32.3 33.3 Table 3. Results on ScanNet200 for semantic segmentation. tial learning rate lr is 0.001, and the weight decay is set to 0.02 with the cosine annealing strategy. Following [61] for data preprocessing, we estimate normal vectors for points and add coordinates as additional feature input. As for the data augmentation, we apply random drop, random defor- mation, and color jitter following [61, 74]. 4.2. Comparisons Performance. We conduct a comprehensive comparison of our proposed OA-CNNs with alternative backbone mod- els on multiple benchmarks, including ScanNet v2, Scan- Net200, S3DIS, nuScenes, and SemanticKITTI [1, 2, 4, 11, 45]. All the methods compared in our experiments are eval- uated under the same experimental settings, without any ad- 6Method Input OA mIoU PointNet [42] point - 41.1 PointTransformer [74] point 90.8 70.4 PTv2 [61] point 91.1 71.6 MinkowskiNet [7] voxel - 65.4 OA-CNNs(ours) voxel 90.7 71.1 Table 4. Results on S3DIS area 5 for semantic segmen- tation. ID Aggregation Stage Nums mIoU ∆(%) I w/o 1 75.0 + 0.0 II Concatenation 3 75.2 + 0.2 III Adaptive (ours) 2 75.2 + 0.2 IV Adaptive (ours) 3 76.1 + 1.1 Table 5. Effectiveness of adaptive aggregator and naive concatenation through ablation studies with varying stage numbers. ID Enlarge Methods mIoU (%) I Baseline 73.0 II Multi-head Self-attention 73.5 III Grouped Vector Attention 74.3 IV Pyramid Pooling 75.0 V ARConv 76.1 Table 6. Ablation studies on the different methods com- monly used for enlarging receptive fields. ID Conv Groups mIoU (%) I Grouped [2, 2, 4, 8] 75.0 II Grouped [4, 4, 8, 16] 75.4 III Depthwise - 76.1 Table 7. Performance Comparison of Depthwise Convolution and Regular Grouped Convolution. ID Type mIoU (%) I Pos 75.3 II Pos+Ctr 75.9 III Ctr 76.1 Table 8. Comparison of various weight generation methods. Epoch Epoch Val / Loss Val / mIoU OursMulti-head Self-attentionGroup Vector AttentionPyramid Pooling Figure 8. Compared with other classical modules to expand the receptive fields, our proposed method is more stable, has faster convergence during training, and acquires better performance. ditional pretraining or auxiliary methods. The results are shown in Tabs. 1, 2, 3, 4. Our proposed model exhibits su- perior performance over prior state-of-the-art point-based frameworks and transformer architectures in both indoor and outdoor scenes. Indeed, these results highlight the su- perior generalization capability of OA-CNNs, demonstrat- ing their potential to outperform point-based and trans- former models in various benchmarks even without any self-attention modules. 4.3. Ablation Study Efficiency. We also compare our models with various CNN-Based and transformer-based methods [7, 13, 21, 61, 74] regarding accuracy, inference speed, and GPU mem- ory consumption, as shown in Fig. 2. We can observe that, while transformer-based methods have demonstrated im- pressive performance, they come with a drawback – they require extensive time and memory for frequently querying nearest neighbors, attention computation, and other point- based operations. Differently, thanks to the CNN architec- Type Blocks Time Mem. mIoU (ms) (G) (%) OA-CNN (S) [ 2, 2, 2, 2] 117 2.1 73.6 OA-CNN (B) [ 3, 3, 9, 3] 190 3.3 75.3 OA-CNN (L) [ 3, 3, 9, 8] 213 3.6 76.1 Table 9. Comparison between various versions of our proposed models. The channels for each stage are set to [64, 64, 128, 256] and kept the same. ture that exploits the structural data arrangement and hash acceleration to attain notable efficiency and low memory consumption, our method takes the performance lead but still preserves a superior balance between effectiveness and efficiency. Receptive field expansion. We verify the effectiveness of our proposed Adaptive Relation Convolution (ARConv) by the comparison with three alternative modules com- monly used for receptive field expansion: 1) multi-head self-attention [55]; 2) grouped vector attention [61]; and 3) pyramid pooling [73]. For attention-based modules, we operate the voxels like nearest neighbor finding and grouping following the point transformer [74]. The test results are shown in Tab. 6, where our ARConv outperforms other competitors. Moreover, Fig. 8 presents the comparison of the validation loss/mIoU during the training process, and ARConv exhibits a supe- rior capacity for mitigating overfitting than the others, as evidenced by the lack of considerable deterioration in vali- dation loss during the later period of training. Aggregation methods. We validate the effectiveness and superiority of the pyramid grid partition and proposed adap- tive aggregator, and the experimental results are shown in 7Input Ground Truth Prediction Figure 9. Visualization of semantic segmentation results on Scan- Net v2. Tab. 5. The first row shows the result of the model with the single-scale partition, where the additional aggregation is not necessary. The second experiment, which adopts direct concatenation for aggregation, leads to a marginal improve- ment in performance. Then, by introducing our proposed adaptive aggregator that adjusts the receptive field of each voxel based on its intrinsic properties, we observed a sig- nificant improvement in performance as compared to using concatenation. Also, we investigate the effects of the num- ber of pyramid stages and find that the three stages acquire the best result, and all experiments follow this configuration without otherwise specified. Depthwise convolution. In contrast to regular convolu- tion that applies one filter W ∈ Rc×l×c, where c repre- sents the channels andl represents the inputs’ length, across all input channels, depthwise convolution applies a single filter for each input channel independently. Initially, we attempted to implement regular convolution with the pro- posed dynamic kernel weights but found it to be unstable and non-convergent, particularly during the early training stages. Consequently, we replaced it with both grouped convolution [8] and depthwise convolution. The outcomes are presented in Tab. 7. Our adoption of depthwise con- volution with dynamically generated weights W ∈ Rl×c yields linear complexity to input channels, showcasing the dual benefits of efficiency and performance. Dynamic kernel weights. Previous point-based methods, such as [49, 65], have also explored dynamically generated kernel weights. However, their approaches aim to incor- porate geometric information from points rather than local semantic relationships, mimicking convolutional operations while still following the PointNet paradigm. Differently, our work is based on sparse convolution networks. We assess the effectiveness of our design for dynamic kernel weight generation by comparing it with other alternatives in Tab. 8, where ctr represents our adaptive relation kernel and pos denotes kernel weight generation using relative po- sitions. Our adaptive relation kernel demonstrates superior performance to other methods. Multiple versions. We present multiple versions of OA- CNNs, achieved by adjusting the number of blocks in each stage while keeping other configurations consistent. In all models, the number of channels per block is set to [64, 64, 128, 256]. The impact on performance and effi- ciency is demonstrated in Tab. 9, where all models are eval- uated on a single RTX 3090 to ensure a fair comparison. 4.4. Visual Analysis Predictions. The qualitative results of point cloud seman- tic segmentation are presented in Fig. 9. Our model ex- hibits exceptional predictive accuracy on the ScanNet v2 dataset, with results that demonstrate high consistency with the ground truth. Receptive fields. Fig. 1 visualizes the varying receptive field sizes of different objects and parts with distinct geo- metric structures and appearances within a 3D indoor scene. We calculate the sizes of the receptive fields as follows: ri = XK k=1 wi,kgk, (8) where gk represents the k-th grid size and w ∈ Rn×K in- dicates preference weights predicted by the learnable adap- tive aggregator in Eq. (2). We then map different sizesri to the corresponding colors. Fig. 1 confirms our intuition that 3D scenes’ flatter areas with simplistic structures, such as walls and floors, require larger receptive fields. Conversely, smaller objects and more intricate areas, such as edges and junctions, need smaller ones. Additionally, we observe that the floor generally requires a smaller receptive field than the wall and ceiling, since it is necessary to exploit more local contexts to distinguish itself from the objects placed on the floor. More visual comparisons of the receptive field are put in the supplementary materials. 5. Conclusion This study highlights the potential of sparse convolution networks to surpass transformer architectures in both ef- ficiency and performance. To achieve this, we introduce omni-adaptive 3D CNNs (OA-CNNs), which consist of two key components: spatially dynamic receptive fields and adaptive relation convolution. As for limitations, the cur- rent pyramid grid sizes are set empirically, highlighting the need for future research to develop more scientifically and logically grounded search algorithms. 8Acknowledgements. This work receives partial support from the Shenzhen Science and Technology Program under No. KQTD20210811090149095. References [1] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioan- nis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman- tic parsing of large-scale indoor spaces. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 1534–1543, 2016. 6, 12 [2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen- zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se- mantickitti: A dataset for semantic scene understanding of lidar sequences. In Proceedings of the IEEE/CVF inter- national conference on computer vision , pages 9297–9307, 2019. 2, 6 [3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fis- cher. 3dmfv: Three-dimensional point cloud classification in real-time using convolutional neural networks. IEEE Robotics and Automation Letters, 3(4):3145–3152, 2018. 3 [4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020. 2, 6 [5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1907–1915, 2017. 3 [6] Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Scaling up kernels in 3d cnns.CoRR, abs/2206.10555, 2022. 2, 4, 6 [7] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3075–3084, 2019. 1, 2, 6, 7, 13 [8] Taco Cohen and Max Welling. Group equivariant convo- lutional networks. In International conference on machine learning, pages 2990–2999. PMLR, 2016. 8 [9] Pointcept Contributors. Pointcept: A codebase for point cloud perception research. https://github.com/ Pointcept/Pointcept, 2023. 1 [10] Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and Jiaya Jia. Reslt: Residual learning for long-tailed recogni- tion. TPAMI, 2023. 1 [11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828–5839, 2017. 2, 6, 12 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1 [13] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submani- fold sparse convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 9224–9232, 2018. 1, 2, 3, 5, 6, 7, 13 [14] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 11108– 11117, 2020. 2 [15] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur- rent slice networks for 3d segmentation of point clouds. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2626–2635, 2018. 2 [16] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation. arXiv preprint arXiv:2309.00616, 2023. 1 [17] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. Advances in neural informa- tion processing systems, 29, 2016. 3 [18] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi- Wing Fu, and Jiaya Jia. Hierarchical point-edge interaction network for point cloud semantic segmentation. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision, pages 10433–10441, 2019. 2 [19] Li Jiang, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Guided point contrastive learn- ing for semi-supervised point cloud semantic segmentation. In ICCV, 2021. 1 [20] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang, and Jiaya Jia. Semi-supervised semantic seg- mentation with directional context-aware consistency. In CVPR, 2021. 1 [21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans- former for 3d point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8500–8509, 2022. 1, 2, 3, 6, 7 [22] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical transformer for lidar-based 3d recognition. In CVPR, 2023. 6 [23] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023. 1 [24] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12697–12705, 2019. 3 [25] Felix J ¨aremo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Deep projective 3d semantic segmentation. In Computer 9Analysis of Images and Patterns: 17th International Confer- ence, CAIP 2017, Ystad, Sweden, August 22-24, 2017, Pro- ceedings, Part I 17, pages 95–107. Springer, 2017. [26] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from 3d lidar using fully convolutional network. arXiv preprint arXiv:1608.07916, 2016. 3 [27] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec- tive kernel networks. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 510–519, 2019. 3 [28] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. Advances in neural information processing systems, 31, 2018. 6 [29] Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang Liu, Shuguang Cui, and Xiaoguang Han. Fpconv: Learn- ing local flattening for point convolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4293–4302, 2020. 3 [30] Jianhui Liu, Yukang Chen, Xiaoqing Ye, Zhuotao Tian, Xiao Tan, and Xiaojuan Qi. Spatial pruned sparse convolution for efficient 3d object detection. In NeurIPS, 2022. 1 [31] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, and Chunhong Pan. Densepoint: Learning densely contextual representation for efficient point cloud process- ing. In Proceedings of the IEEE/CVF international confer- ence on computer vision, pages 5239–5248, 2019. 2 [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 1 [33] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht- enhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pages 11976–11986, 2022. 4 [34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [35] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolu- tional neural networks. Advances in neural information pro- cessing systems, 29, 2016. 15 [36] Xiaoliu Luo, Zhuotao Tian, Taiping Zhang, Bei Yu, Yuan Yan Tang, and Jiaya Jia. Pfenet++: Boosting few-shot semantic segmentation with the noise-filtered context-aware prior mask. TPAMI, 2024. 1 [37] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel transformer for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3164–3173, 2021. 1, 5 [38] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con- volutional neural network for real-time object recognition. In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 922–928. IEEE, 2015. 1, 3 [39] Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, and Dinesh Manocha. Vv-net: V oxel vae net with group convolutions for point cloud segmentation. In Proceedings of the IEEE/CVF international conference on computer vision , pages 8500– 8508, 2019. 3 [40] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jae- sik Park. Fast point transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16949–16958, 2022. 1, 4, 6 [41] Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang, Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense cor- relation distillation for few-shot segmentation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23641–23651, 2023. 6 [42] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652–660, 2017. 1, 2, 7, 13 [43] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. 1, 2, 5, 6, 13 [44] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. arXiv preprint arXiv:2206.04670, 2022. 6 [45] David Rozenberszki, Or Litany, and Angela Dai. Language- grounded indoor 3d semantic segmentation in the wild. In Proceedings of the European Conference on Computer Vi- sion (ECCV), 2022. 6, 12 [46] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano- lis Savva, and Thomas Funkhouser. Semantic scene com- pletion from a single depth image. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1746–1754, 2017. 1, 3 [47] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE in- ternational conference on computer vision , pages 945–953, 2015. 3 [48] Haotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d ar- chitectures with sparse point-voxel convolution. InEuropean Conference on Computer Vision, 2020. 6 [49] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6411–6420, 2019. 3, 6, 8 [50] Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao Zhou, Xiaoyong Shen, and Jiaya Jia. Learning shape-aware embedding for scene text detection. In CVPR, 2019. 1 [51] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalized few-shot se- mantic segmentation. In CVPR, 2022. 1 10[52] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrich- ment network for few-shot segmentation. TPAMI, 2022. 1 [53] Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu, Hengshuang Zhao, Bei Yu, Ming-Chang Yang, and Jiaya Jia. Adaptive perspective distillation for semantic segmentation. TPAMI, 2023. 1 [54] Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, and Jiaya Jia. Learning context-aware classifier for semantic segmentation. In AAAI, 2023. 1 [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 2, 3, 5, 7 [56] Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bo- hao Peng, Hengshuang Zhao, and Jiaya Jia. Groupcontrast: Semantic-aware self-supervised representation learning for 3d understanding. arXiv preprint arXiv:2403.09639, 2024. 1 [57] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. arXiv preprint arXiv:2305.03045, 2023. 6 [58] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con- vnext v2: Co-designing and scaling convnets with masked autoencoders. arXiv preprint arXiv:2301.00808, 2023. 4 [59] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dy- namic convolutions. arXiv preprint arXiv:1901.10430, 2019. 2, 3, 4 [60] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 9621–9630, 2019. 3, 6 [61] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vec- tor attention and partition-based pooling. arXiv preprint arXiv:2210.05666, 2022. 1, 2, 4, 5, 6, 7, 12, 14 [62] Xiaoyang Wu, Xin Wen, Xihui Liu, and Hengshuang Zhao. Masked scene contrast: A scalable framework for unsu- pervised 3d representation learning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, 2023. 6 [63] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xi- hui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In CVPR, 2024. 2 [64] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large- scale 3d representation learning with multi-dataset point prompt training. In CVPR, 2024. 2 [65] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiao- juan Qi. Paconv: Position adaptive convolution with dy- namic kernel assembling on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3173–3182, 2021. 3, 8 [66] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed- ded convolutional detection. Sensors, 18(10):3337, 2018. 1 [67] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolu- tions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019. 3 [68] Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu, Binbin Lin, Xiaofei He, and Wanli Ouyang. Unipad: A universal pre-training paradigm for autonomous driving. In CVPR, 2024. 2 [69] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. 2 [70] Senqiao Yang, Jiarui Wu, Jiaming Liu, Xiaoqi Li, Qizhe Zhang, Mingjie Pan, Mingjie Pan, and Shanghang Zhang. Exploring sparse visual prompt for cross-domain semantic segmentation. arXiv e-prints, pages arXiv–2303, 2023. 2 [71] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. Sam3d: Segment anything in 3d scenes.arXiv preprint arXiv:2306.03908, 2023. 1 [72] Dong Zhang, Yi Lin, Hao Chen, Zhuotao Tian, Xin Yang, Jinhui Tang, and Kwang-Ting Cheng. Deep learning for medical image segmentation: Tricks, challenges and future directions. arXiv preprint arXiv:2209.10307, 2022. 1 [73] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881–2890, 2017. 7 [74] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16259–16268, 2021. 1, 2, 3, 5, 6, 7, 12 [75] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xi- aojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. 2023. 1 [76] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chun- hua Shen, Yu Qiao, and Wanli Ouyang. Ponderv2: Pave the way for 3d foundation model with a universal pre-training paradigm. arXiv preprint arXiv:2310.08586, 2023. 1 [77] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and asymmetrical 3d convolution networks for lidar segmenta- tion. arXiv preprint arXiv:2011.10033, 2020. 6 11Appendix A. Implementation Details In this section, we present further details and configurations utilized in our experiments. A.1. Environment Experimental environment. • PyTorch version: 1.10.1 • CUDA version: 11.1 • cuDNN version: 1.10.1 • GPU: Nvidia RTX 3090 × 4 A.2. Data Propocessing Data preprocessing and augmentation. This work maintains consistency in data preprocessing and augmentation with PTv1 and Ptv2 [61, 74] for the ScanNet series and S3DIS datasets [1, 11, 45]. The specific data augmentation strategies employed during training are outlined in Tab. 10. Drop Rotate Scale Flip Jitter Disort Chromatic ScanNet v2 [11] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ScanNet200 [45] ✓ ✓ ✓ ✓ ✓ ✓ ✓ S3DIS [1] ✓ ✓ ✓ ✓ Table 10. Data augmentation strategies on various datasets. Voxelization. • voxel size: 0.02m • hash type: Fowler-Noll-V o (FNV) A.3. Training Setting This subsection offers additional details on our training settings for the three standard benchmarks, including optimizer and learning configurations. More details are listed in Tab. 11. Epoch LR Weight Decay Scheduler Optimizer Batch Size ScanNet v2 [11] 600 1e-3 0.02 Cosine AdamW 16 ScanNet200 [45] 900 1e-3 0.02 Cosine AdamW 12 S3DIS [1] 3000 1e-3 0.05 MultiStep AdamW 16 Table 11. Training settings on various datasets. B. Experimental Results B.1. Test Benchmarks In this section, we present detailed results for each category on the ScanNet v2 and ScanNet200 test set. For more detailed information refer to the official benchmarks [11, 45]. ScanNet v2 contains over 1, 513 RGB-D indoor scans of various environments, including apartments, offices, and public spaces. The dataset includes high-quality 3D point clouds with per-point semantic annotations. On the other hand, the ScanNet200 benchmark extends the class categories to 200, an order of magnitude more than the previous, significantly increasing the difficulty and generalizability requirements. Moreover, ScanNet200 partitioned the 200 categories into three distinct subsets based on the labeled surface points’ frequency in the train set: head, common, and tail, comprising 66, 68, and 66 categories, respectively, for a more granular understanding of the segmentation performance. As for the evaluation, we follow the standard protocol using the mean class-wise intersection over union (mIoU) for both ScanNet v2 and ScanNet200. 12Specifically, Tab. 12 presents comprehensive results on the ScanNet v2, offering a detailed breakdown of the performance for each semantic class. Similarly, Tab. 13 provides the results of the head, common, and tail subsets on the ScanNet200 benchmark, offering a more nuanced understanding of the performance across different levels of class imbalance. Further- more, Fig. 10 visually represents the segmentation performance for each specific class in the ScanNet200 benchmark. Category A VG bathtub bed bookshelf cabinet chair counter curtain desk door shower curtain mIoU 75.6 78.3 82.6 85.8 77.6 83.7 54.8 89.6 64.9 67.5 80.2(%) Category picture floor refrigerator sink sofa table toilet wall window otherfurniture mIoU 33.5 96.2 77.1 77.0 78.7 69.1 93.6 88.0 76.1 58.6(%) Table 12. Results for each category on the ScanNet v2 test benchmark. Set Head Common Tail All mIoU 55.8 26.9 12.4 33.3(%) Table 13. Results for various sets on the ScanNet200 test benchmark. Figure 10. Results for each category on the ScanNet200 test benchmark. B.2. Raw Points and Structural Voxels The Point Transformer methods, building upon the fundamental principles of the PointNet series [42, 43], emphasize the ad- vantages of operating directly on raw point data to capture finer-grained local features and preserve the underlying geometric structure of the data. In contrast, traditional CNN-based methods typically require voxelization preprocessing, which in- volves partitioning the 3D space into a regular grid of equally-sized cubic volumes (voxels). This mapping allows the points’ positions to be transformed into discrete indices [7, 13], which can be used for convolutional and index retrieval operations. However, voxelization may result in losing fine-grained geometric details and potential aliasing effects. To test the in- fluence of voxelization on performance, we conducted an experiment where we input the discretized voxels into the Point Transformer with normalized indices instead of the original positional information while keeping all other configurations the same. The voxelization used in this experiment was the same as for our OA-CNNs’ input. The results are shown in Tab. 14, and we observed that the degradation in performance due to discretization was acceptable with appropriate granularity. B.3. Decoder Design Typically, U-Net architectures are adopted by 3D semantic segmentation models, which split the entire process into feature encoding and decoding. The encoder processes the input point cloud features and generates downsampled pyramid features using multi-scale and multi-revolution techniques, while the decoder integrates all the cues. Previous 3D semantic models 13Method Input mIoU Input Size Hash mIoU (%) (%) PointTransformer v2 [61] Point 75.6 V oxel 0.02m FNV 75.5 Table 14. Comparison between point and voxel inputs. have constructed decoder blocks using the same components, replacing the downsample sparse modules with upsample modules. In this study, we have constructed our decoder blocks with only essential upsample modules and a single MLP layer, resulting in an extremely lightweight and simple design. Additionally, we have transferred the main components to the encoder section, ensuring the lightweight decoder’s effectiveness. To be specific, our initial model construction adhered to the typical pipeline, which involves constructing the decoder in a manner similar to the encoder, while replacing the downsample modules with upsample modules for the basic blocks. Subsequently, we designed the decoder block to comprise only a single upsample and MLP layer. The experimental results are shown in Tab. 15 and more detailed architectural comparison is displayed in Fig. 11. Method Encoder Blocks Decoder Blocks mIoU (%) Basic Blocks (upsample) [ 2, 2, 6, 6] [ 2, 2, 2, 2] 75.0 MLP [ 3, 3, 9, 8] - 76.1 Table 15. Performance comparison between different decoder designs. LinearUpSample𝑓௜ᇱ𝑓௜ାଵᇱUpSample𝑓௜ᇱ𝑓௜ାଵᇱUpSample𝑓௜ᇱ𝑓௜ାଵᇱUpSample𝑓௜ᇱ𝑓௜ାଵᇱSubmanifold ConvSubmanifold ConvKernel(3, 3, 3)  Stride1ConvBlock×  𝑛௜×  𝑛௜(a) Our Decoder Block (b) Typical Decoder Block Figure 11. Comparison between our and typical decoder blocks. B.4. Decoder Design Typically, U-Net architectures are adopted by 3D semantic segmentation models, which split the entire process into feature encoding and decoding. The encoder processes the input point cloud features and generates downsampled pyramid features using multi-scale and multi-revolution techniques, while the decoder integrates all the cues. Previous 3D semantic models have constructed decoder blocks using the same components, replacing the downsample sparse modules with upsample modules. In this study, we have constructed our decoder blocks with only essential upsample modules and a single MLP layer, resulting in an extremely lightweight and simple design. Additionally, we have transferred the main components to the encoder section, ensuring the lightweight decoder’s effectiveness. To be specific, our initial model construction adhered to the typical pipeline, which involves constructing the decoder in a manner similar to the encoder, while replacing the downsample modules with upsample modules for the basic blocks. Subsequently, we designed the decoder block to comprise only a single upsample and MLP layer. The experimental results are shown in Tab. 15 and more detailed architectural comparison is displayed in Fig. 11. 14C. Impact of the grid size. To examine the impact of grid size, we supplement ablation experiments by adjusting the grid size to 0.5x, 0.67x, 0.75x, and 1.25x times compared to the original setting. The experimental results are shown in Fig. 12, which shows that: (1) Significantly reducing the grid size leads to notable performance degradation, attributed to insufficient receptive range. (2) Continuing to expand the grid size does not yield improvements and may even cause minor negative impacts. This could be because fine-grained local details are overwhelmed by the surrounding context, especially for small objects. The time consumption generally remains consistent across different grid sizes, which shows the robustness of our method. 74.375.275.176.175.87475760.5x 0.67x 0.75x Ours 1.25xGrid SizemIoU (%)Small grid size Large grid size  𝒈𝒌 Figure 12. Comparative analysis of the impact of various grid sizes. D. Visualization Studies D.1. Receptive Fields Comparison In this subsection, we present the Effective Receptive Field (ERF) [35] visualization for the feature of interest in the first stage, denoted by red and yellow stars representing the table and wall, respectively. Effective Receptive Field (ERF) is used to measure the ability of a deep neural network to capture the contextual information of an input image or feature map. The ERF of a neuron in a deep network is defined as the area in the input space that influences the neuron’s activation, which helps to explain the network’s behavior and performance. We conducted ablation experiments to assess the effectiveness of our proposed ARConv and adaptive aggregator on distinct 3D scene parts with different spatial structures and appearances. The visualization results are shown in Fig. 13. The experimental results demonstrate that our proposed ARConv can significantly expand the receptive range compared to the baseline. Moreover, the adaptive aggregator can dynamically adjust the receptive fields based on the specific geometric and appearance features, allocating a larger receptive field for the wall and a smaller one for the table. These findings suggest that our proposed methods can effectively capture the key features of different parts of the 3D scene and improve the model’s overall performance on 3D point cloud tasks. D.2. Prediction Visualization In this subsection, we provide additional visualizations of our proposed model’s predictions on the ScanNet dataset. Fig. 14 showcases a diverse set of indoor scenes to demonstrate our model’s performance across different environments. The vi- sualizations demonstrate that our model performs remarkably well in various indoor scenes, regardless of complexity and structural variations. Specifically, the model accurately segments different indoor objects such as furniture, walls, and floors, and effectively captures their fine details and shapes. Furthermore, the model generates consistent and coherent predictions even in complex indoor environments, where objects are densely packed and occluded. These visualizations provide compelling evidence of the effectiveness of our proposed approach in achieving accurate and robust 3D semantic segmentation results on the ScanNet dataset. 15Raw Points Wall(a) w/ (b) w/o Table (c) w/ (d) w/o Figure 13. Visualization comparison of the receptive fields on various 3D scene parts. 16Input Ground TruthPrediction floorwallcabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainrefrigeratorbathtubshower curtaintoiletsinkother furnitureignore Figure 14. Visualization results of the raw point cloud, ground truth, and our model’s prediction. 17",
      "references": [
        "3d semantic parsing of large-scale indoor spaces.",
        "Semantickitti: A dataset for semantic scene understanding of lidar sequences.",
        "3dmfv: Three-dimensional point cloud classification in real-time using convolutional neural networks.",
        "nuscenes: A multi-modal dataset for autonomous driving.",
        "Multi-view 3d object detection network for autonomous driving.",
        "Scaling up kernels in 3d cnns.",
        "4d spatio-temporal convnets: Minkowski convolutional neural networks.",
        "Group equivariant convolutional networks.",
        "Pointcept: A codebase for point cloud perception research.",
        "Reslt: Residual learning for long-tailed recognition.",
        "Scannet: Richly-annotated 3d reconstructions of indoor scenes.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "3d semantic segmentation with submanifold sparse convolutional networks.",
        "Randla-net: Efficient semantic segmentation of large-scale point clouds.",
        "Recurrent slice networks for 3d segmentation of point clouds.",
        "Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation.",
        "Dynamic filter networks.",
        "Hierarchical point-edge interaction network for point cloud semantic segmentation.",
        "Guided point contrastive learning for semi-supervised point cloud semantic segmentation.",
        "Semi-supervised semantic segmentation with directional context-aware consistency.",
        "Stratified transformer for 3d point cloud segmentation.",
        "Spherical transformer for lidar-based 3d recognition.",
        "LISA: reasoning segmentation via large language model.",
        "Pointpillars: Fast encoders for object detection from point clouds.",
        "Deep projective 3d semantic segmentation.",
        "Vehicle detection from 3d lidar using fully convolutional network.",
        "Selective kernel networks.",
        "Pointcnn: Convolution on x-transformed points.",
        "Fpconv: Learning local flattening for point convolution.",
        "Spatial pruned sparse convolution for efficient 3d object detection.",
        "Densepoint: Learning densely contextual representation for efficient point cloud processing.",
        "Swin transformer: Hierarchical vision transformer using shifted windows.",
        "A convnet for the 2020s.",
        "Decoupled weight decay regularization.",
        "Understanding the effective receptive field in deep convolutional neural networks.",
        "Pfenet++: Boosting few-shot semantic segmentation with the noise-filtered context-aware prior mask.",
        "Voxel transformer for 3d object detection.",
        "Voxnet: A 3d convolutional neural network for real-time object recognition.",
        "Vv-net: Voxel vae net with group convolutions for point cloud segmentation.",
        "Fast point transformer.",
        "Hierarchical dense correlation distillation for few-shot segmentation.",
        "Pointnet: Deep learning on point sets for 3d classification and segmentation.",
        "Pointnet++: Deep hierarchical feature learning on point sets in a metric space.",
        "Pointnext: Revisiting pointnet++ with improved training and scaling strategies.",
        "Language-grounded indoor 3d semantic segmentation in the wild.",
        "Semantic scene completion from a single depth image.",
        "Multi-view convolutional neural networks for 3d shape recognition.",
        "Searching efficient 3d architectures with sparse point-voxel convolution.",
        "Kpconv: Flexible and deformable convolution for point clouds.",
        "Learning shape-aware embedding for scene text detection.",
        "Generalized few-shot semantic segmentation.",
        "Prior guided feature enrichment network for few-shot segmentation.",
        "Adaptive perspective distillation for semantic segmentation.",
        "Learning context-aware classifier for semantic segmentation.",
        "Attention is all you need.",
        "Groupcontrast: Semantic-aware self-supervised representation learning for 3d understanding.",
        "Octformer: Octree-based transformers for 3d point clouds.",
        "Convnext v2: Co-designing and scaling convnets with masked autoencoders.",
        "Pay less attention with lightweight and dynamic convolutions.",
        "Pointconv: Deep convolutional networks on 3d point clouds.",
        "Point transformer v2: Grouped vector attention and partition-based pooling.",
        "Masked scene contrast: A scalable framework for unsupervised 3d representation learning.",
        "Point transformer v3: Simpler, faster, stronger.",
        "Towards large-scale 3d representation learning with multi-dataset point prompt training.",
        "Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds.",
        "Second: Sparsely embedded convolutional detection.",
        "Condconv: Conditionally parameterized convolutions for efficient inference.",
        "Unipad: A universal pre-training paradigm for autonomous driving.",
        "An improved baseline for reasoning segmentation with large language model.",
        "Exploring sparse visual prompt for cross-domain semantic segmentation.",
        "Sam3d: Segment anything in 3d scenes.",
        "Deep learning for medical image segmentation: Tricks, challenges and future directions.",
        "Pyramid scene parsing network.",
        "Point transformer.",
        "Understanding imbalanced semantic segmentation through neural collapse.",
        "Ponderv2: Pave the way for 3d foundation model with a universal pre-training paradigm.",
        "Cylindrical and asymmetrical 3d convolution networks for lidar segmentation."
      ],
      "meta_data": {
        "arxiv_id": "2403.14418v1",
        "authors": [
          "Bohao Peng",
          "Xiaoyang Wu",
          "Li Jiang",
          "Yukang Chen",
          "Hengshuang Zhao",
          "Zhuotao Tian",
          "Jiaya Jia"
        ],
        "published_date": "2024-03-21T14:06:38Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies lack of adaptivity as chief reason sparse CNNs trail point-transformers in 3-D semantic segmentation and proposes Omni-Adaptive CNNs (OA-CNNs) that introduce (1) spatially adaptive receptive fields via multi-scale pyramid voxel grids plus an adaptive aggregator, and (2) Adaptive Relation Convolution (ARConv) with dynamic, relation-aware depth-wise kernels. Without any self-attention, OA-CNNs achieve state-of-the-art accuracy on indoor (ScanNet v2 76.1 % mIoU) and outdoor (nuScenes 78.9 %, SemanticKITTI 70.6 %) benchmarks while being up to 5× faster and more memory-efficient than transformer counterparts.",
        "methodology": "1) Voxelization of point clouds followed by sparse/submanifold convolutions. 2) Pyramid grid partition at several voxel sizes; each voxel grid processed by ARConv at multiple scales. 3) Adaptive aggregator predicts per-voxel weights (via MLP + softmax) to fuse multi-scale outputs, yielding spatially variable receptive fields. 4) ARConv employs a multi-one-multi scheme: for each grid, compute centroid feature; generate depth-wise convolution weights for each non-empty voxel based on its relation (difference) to centroid; apply normalized dynamic kernel (linear complexity). 5) Encoder–decoder U-Net style architecture with lightweight decoder; trained with AdamW, cosine LR; data augmentations align with PointTransformer v2.",
        "experimental_setup": "Training on 4× RTX-3090 GPUs, batch 16, voxel size 0.02 m. Datasets: ScanNet v2 (1,201 train/312 val scenes, 20 classes); ScanNet200 (extended to 200 classes); S3DIS Area-5 split (13 classes); nuScenes lidar segmentation (≈1,000 scenes, 16 classes); SemanticKITTI (22 sequences, 19 classes). Metrics: mean IoU on validation/test sets. Baselines: point-based transformers (PointTransformer, PointTransformer v2, Stratified Transformer, OctFormer, FastPointTransformer), sparse CNNs (MinkowskiNet, SparseUNet, LargeKernel3D, SPVNAS, Cylinder3D). Evaluated accuracy, inference latency, and GPU memory; ablation studies on module variants (ARConv vs attention, aggregator vs concatenation, grid sizes, depth-wise vs grouped conv).",
        "limitations": "• Pyramid grid sizes chosen heuristically; no automatic search or theoretical guidance.\n• Still relies on voxelization—potential loss of fine geometric details and dependence on voxel size.\n• Evaluation limited to semantic segmentation; effectiveness on other 3-D tasks (detection, completion) untested.\n• Absence of global self-attention may restrict modeling of extremely long-range dependencies in very sparse scenes.\n• Experiments use high-end GPUs; real-time performance on edge or embedded hardware not verified.",
        "future_research_directions": "1) Develop learnable or auto-searched grid sizing/partition strategies to further optimize adaptive receptive fields.\n2) Extend OA-CNN framework to other 3-D vision tasks, e.g., instance/ panoptic segmentation, object detection, scene completion.\n3) Explore hybrid models that combine OA-CNN efficiency with selective self-attention for very long-range context.\n4) Investigate unsupervised or self-supervised pretraining for OA-CNNs to reduce label dependence.\n5) Optimize implementation for low-power or real-time edge devices and study quantization or pruning impacts.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization",
      "full_text": "Published as a conference paper at ICLR 2021 SALIENCY MIX: A S ALIENCY GUIDED DATA AUG- MENTATION STRATEGY FOR BETTER REGULARIZA - TION A. F. M. Shahab Uddin∗ uddin@khu.ac.kr Mst. Sirazam Monira∗ monira@khu.ac.kr Wheemyung Shin∗ wheemi@khu.ac.kr TaeChoong Chung∗† tcchung@khu.ac.kr Sung-Ho Bae∗† shbae@khu.ac.kr ABSTRACT Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. How- ever, such information removal is undesirable. On the other hand, recent strate- gies suggest to randomly cut and mix patches and their labels among train- ing images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selec- tion strategies of the patches may not necessarily represent sufﬁcient informa- tion about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature repre- sentation. Therefore, we propose SaliencyMix that carefully selects a repre- sentative image patch with the help of a saliency map and mixes this indica- tive patch with the target image, thus leading the model to learn more appro- priate feature representation. SaliencyMix achieves the best known top-1 error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on ImageNet classiﬁcation, respectively, and also improves the model robustness against ad- versarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix. 1 I NTRODUCTION Machine learning has achieved state-of-the-art (SOTA) performance in many ﬁelds, especially in computer vision tasks. This success can mainly be attributed to the deep architecture of convolu- tional neural networks (CNN) that typically have 10 to 100 millions of learnable parameters. Such a huge number of parameters enable the deep CNNs to solve complex problems. However, besides the powerful representation ability, a huge number of parameters increase the probability of overﬁtting when the number of training examples is insufﬁcient, which results in a poor generalization of the model. In order to improve the generalization ability of deep learning models, several data augmentation strategies have been studied. Random feature removal is one of the popular techniques that guides the CNNs not to focus on some small regions of input images or on a small set of internal activations, thereby improving the model robustness. Dropout (Nitish et al., 2014; Tompson et al., 2015) and regional dropout (Junsuk & Hyunjung, 2019; Terrance & Graham, 2017; Golnaz et al., 2018; Singh & Lee, 2017; Zhun et al., 2017) are two established training strategies where the former randomly turns off some internal activations and later removes and/or alters random regions of the input im- ages. Both of them force a model to learn the entire object region rather than focusing on the most ∗Department of Computer Science & Engineering, Kyung Hee University, South Korea. †Corresponding author. 1 arXiv:2006.01791v2  [cs.LG]  27 Jul 2021Published as a conference paper at ICLR 2021 Target Image Source Image Dog - 80% ? Cat - 20% ? Augmented Image  Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Target Image Source Image Dog - 80% ? Cat - 20% ? Augmented Image  Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Target ImageSource Image Dog - 80% ? Cat - 20% ? Augmented Image Augmented Label Dog - 80% ? Cat - 20% ? LabelOriginal Image Dog Cat Augmented Image Target Image Mixed label for randomly mixed images Source Image Dog - 80% & Cat 20% ? Dog - 80% & Cat 20% ? 20% Figure 1: Problem of randomly selecting image patch and mixing labels according to it. When the selected source patch does not represent the source object, the interpolated label misleads the model to learn unexpected feature representation. important features and thereby improving the generalization of the model. Although dropout and regional dropout improve the classiﬁcation performance, this kind of feature removal is undesired since they discard a notable portion of informative pixels from the training images. Recently, Yun et al. (2019) proposed CutMix, that randomly replaces an image region with a patch from another training image and mixes their labels according to the ratio of mixed pixels. Unlike Cutout (Devries & Taylor, 2017), this method can enjoy the properties of regional dropout without having any blank image region. However, we argue that the random selection process may have some possibility to select a patch from the background region that is irrelevant to the target objects of the source image, by which an augmented image may not contain any information about the corresponding object as shown in Figure 1. The selected source patch (background) is highlighted with a black rectangle on the source image. Two possible augmented images are shown wherein both of the cases, there is no information about the source object (cat) in the augmented images despite their mixing location on the target image. However, their interpolated labels encourage the model to learn both objects’ features (dog and cat) from that training image. But we recognize that it is undesirable and misleads the CNN to learn unexpected feature representation. Because, CNNs are highly sensitive to textures (Geirhos et al., 2019) and since the interpolated label indicates the selected background patch as the source object, it may encourage the classiﬁer to learn the background as the representative feature for the source object class. We address the aforementioned problem by carefully selecting the source image patch with the help of some prior information. Speciﬁcally, we ﬁrst extract a saliency map of the source image that highlights important objects and then select a patch surrounding the peak salient region of the source image to assure that we select from the object part and then mix it with the target image. Now the selected patch contains relevant information about the source object that leads the model to learn more appropriate feature representation. This more effective data augmentation strategy is what we call, ”SaliencyMix”. We present extensive experiments on various standard CNN architectures, benchmark datasets, and multiple tasks, to evaluate the proposed method. In summary, SaliencyMix has obtained the new best known top-1 error of 2.76% and 16.56% for WideResNet (Zagoruyko & Komodakis, 2016) on CIFAR-10 and CIFAR-100 (Krizhevsky, 2012), respectively. Also, on Ima- geNet (Olga et al., 2015) classiﬁcation problem, SaliencyMix has achieved the best known top-1 and top-5 error of 21.26% and 5.76% for ResNet-50 and 20.09% and 5.15% for ResNet-101 (He et al., 2016). In object detection task, initializing the Faster RCNN (Shaoqing et al., 2015) with Salien- cyMix trained model and then ﬁne-tuning the detector has improved the detection performance on Pascal VOC (Everingham et al., 2010) dataset by +1.77 mean average precision (mAP). Moreover, SaliencyMix trained model has proved to be more robust against adversarial attack and improves the top-1 accuracy by 1.96% on adversarially perturbed ImageNet validation set. All of these re- sults clearly indicate the effectiveness of the proposed SaliencyMix data augmentation strategy to enhance the model performance and robustness. 2 R ELATED WORKS 2.1 D ATA AUGMENTATION The success of deep learning models can be accredited to the volume and diversity of data. But col- lecting labeled data is a cumbersome and time-consuming task. As a result, data augmentation has 2Published as a conference paper at ICLR 2021 been introduced that aims to increase the diversity of existing data by applying various transforma- tions e.g., rotation, ﬂip, etc. Since this simple and inexpensive technique signiﬁcantly improves the model performance and robustness, data augmentation has widely been used to train deep learning models. Lecun et al. (1998) applied data augmentation to train LeNet for hand written character recognition. They performed several afﬁne transformations such as translation, scaling, shearing, etc. For the same task, Bengio et al. (2011) applied more diverse transformation such as Gaussian noise, salt and pepper noise, Gaussian smoothing, motion blur, local elastic deformation, and various occlusions to the images. Krizhevsky et al. (2012) applied random image patch cropping, horizontal ﬂipping and random color intensity changing based on principal component analysis (PCA). In Deep Image (Wu et al., 2015), color casting, vignetting, and lens distortion are applied besides ﬂipping and cropping to improve the robustness of a very deep network. Besides these manually designed data augmentations, Lemley et al. (2017) proposed an end-to-end learnable augmentation process, called Smart Augmentation. They used two different networks where one is used to learn the suitable augmentation type and the other one is used to train the actual task. Devries & Taylor (2017) proposed Cutout that randomly removes square regions of the input training images to improve the robustness of the model. Zhang et al. (2017) proposed MixUp that blends two training images to some degree where the labels of the augmented image are assigned by the linear interpolation of those two images. But the augmented images look unnatural and locally ambiguous. Recently, Cubuk et al. (2019) proposed an effective data augmentation method called AutoAugment that deﬁnes a search space of various augmentation techniques and selects the best suitable one for each mini-batch. Kim et al. (2020) proposed PuzzleMix that jointly optimize two objectives i.e., selecting an optimal mask and an optimal mixing plan. The mask tries to reveal most salient data of two images and the optimal transport plan aims to maximize the saliency of the revealed portion of the data. Yun et al. (2019) proposed CutMix that randomly cuts and mixes image patches among training samples and mixes their labels proportionally to the size of those patches. However, due to the randomness in the source patch selection process, it may select a region that does not contain any informative pixel about the source object, and the label mixing according to those uninformative patches misleads the classiﬁer to learn unexpected feature representation. In this work, the careful selection of the source patch always helps to contain some information about the source object and thereby solves the class probability assignment problem and helps to improve the model performance and robustness. 2.2 L ABEL SMOOTHING In object classiﬁcation, the class labels are usually represented by one-hot code i.e., the true labels are expected to have the probability of exactly 1 while the others to have exactly 0. In other words, it suggests the model to be overconﬁdent which causes overﬁtting to training dataset. As a result, the models have low performance on unseen test dataset. To alleviate this problem, label smoothing allows to relax the model conﬁdence on the true label by setting the class probability to a slightly lower value e.g., lower than 1. As a result, it guides the model to be more adaptive instead of being over-conﬁdent and ultimately improves the model robustness and performance (Szegedy et al., 2016). Our method also mixes the class labels and enjoys the beneﬁt of label smoothing. 2.3 S ALIENCY DETECTION Saliency detection aims to simulate the natural attention mechanism of human visual system (HVS) and can be classiﬁed into two main categories. The ﬁrst one is a bottom-up approach (Cheng et al., 2014; Zhu et al., 2014; Li et al., 2015; Zhou et al., 2015; Achanta et al., 2009; Li et al., 2013; Hou & Zhang, 2007; Qin et al., 2015; Peng et al., 2016; Lei et al., 2016; Montabone & Soto, 2010) that focuses on exploring low-level vision features. Some visual priors that are inspired by the HVS properties are utilized to describe a salient object. Cheng et al. (2014) utilized a contrast prior and proposed a regional contrast based salient object detection algorithm. Zhu et al. (2014) introduced a robust background measure in an optimization framework to integrate multiple low level cues to obtain clean and uniform saliency maps. Li et al. (2015) optimized the image boundary selection by a boundary removal mechanism and then used random walks ranking to formulate pixel-wised saliency maps. Zhou et al. (2015) proposed a saliency detection model where the saliency infor- mation is propagated using a manifold ranking diffusion process on a graph. In addition, some 3Published as a conference paper at ICLR 2021 Saliency  Detection  Peak Salient Region Source Image Saliency  Detection Peak Salient  Region  Target ImageSource Patch  Augmented Image Source Image  Saliency  Map of  the Source Image Selecting the Peak  Salient Region of  the Saliency Map  Target Image Selecting the  Source Patch Based  on the Peak Salient  Region  Augmented Image Mixing the Source  Patch with the  Target Image Source Image  Saliency  Map of  the Source Image Selecting the Peak  Salient Region of  the Saliency Map  Target Image Selecting the  Source Patch Based  on the Peak Salient  Region  Augmented Image Mixing the Source  Patch with the  Target Image Figure 2: The proposed SaliencyMix data augmentation. We ﬁrst extract the saliency map of the source image that highlights the regions of interest. Then we select a patch around the peak salient pixel location and mix it with the target image. traditional techniques are also introduced to achieve image saliency detection, such as frequency domain analysis (Achanta et al., 2009), sparse representation (Li et al., 2013), log-spectrum (Hou & Zhang, 2007) cellular automata (Qin et al., 2015), low-rank recovery (Peng et al., 2016), and Bayesian theory (Lei et al., 2016). Hou & Zhang (2007) proposed a spectral residual method that focuses on the properties of background. Achanta et al. (2009) proposed a frequency tuned approach that preserves the boundary information by retaining sufﬁcient amount of high frequency contents. Montabone & Soto (2010) introduced a method that was originally designed for a fast human detec- tion in a scene by proposing novel features derived from a visual saliency mechanism. Later on this feature extraction mechanism was generalized for other forms of saliency detection. The second one is a top-down approach which is task-driven and utilizes supervised learning with labels. Several deep learning based methods have been proposed for saliency detection (Deng et al., 2018; Liu et al., 2018; Zhang et al., 2018a;b; Qin et al., 2019). Deng et al. (2018) proposed a recur- rent residual reﬁnement network (R3Net) equipped with residual reﬁnement blocks (RRBs) to more accurately detect salient regions. Contexts play an important role in the saliency detection task and based on that Liu et al. (2018) proposed a pixel-wise contextual attention network, called PiCANet, to learn selectively attending to informative context locations for each pixel. Zhang et al. (2018a) in- troduced multi-scale context-aware feature extraction module and proposed a bi-directional message passing model for salient object detection. Zhang et al. (2018b) focused on powerful feature extrac- tion and proposed an attention guided network which selectively integrates multi-level contextual information in a progressive manner. Recently, Qin et al. (2019) proposed a predict and reﬁne ar- chitecture for salient object detection called boundary-aware saliency detection network (BASNet). The author introduced a hybrid loss to train a densely supervised Encoder-Decoder network. However, despite the high performance of top-down approach, there is a lack of generalization for various applications since they are biased towards the training data and limited to the speciﬁc objects. In this study, we require a saliency model to focus on the important object/region in a given scene without knowing their labels. As a result, we rely on bottom-up approach which are unsupervised, scale-invariant and more robust for unseen data. It is worth noting that the training based saliency methods can also be applied where the quality and quantity of data for training the saliency methods may be correlated with the effectiveness of data augmentation. Section 3.3 further explains the effects of different saliency detection algorithm on the proposed data augmentation method. 3 P ROPOSED METHOD Similar to Yun et al. (2019), we cut a source patch and mix it to the target image and also mix their labels proportionally to the size of the mixed patches. But in order to prevent the model from learning any irrelevant feature representation, the proposed method enforces to select a source patch in a way so that it must contains information about the source object. It ﬁrst extracts a saliency map of the source image to highlight the objects of interest and then selects a patch surrounding the peak salient region to mix with the target image. Here we explain the process in detail. 3.1 S ELECTION OF THE SOURCE PATCH The goal of saliency detection is to ﬁnd out the pixels or regions that are attractive to the HVS and to assign them with higher intensity values (Cong et al., 2019). A saliency detection method produces the visual saliency map, a gray-scale image, that highlights the objects of interest and thereby mostly 4Published as a conference paper at ICLR 2021 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Saliency Detection Method 40 35 30 25 20 15 10 5 0 Saliency Detection Method 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Source and Target Patch Mixing Style 4.5 CIFAR10 Tiny ImageNet CIFAR10 Tiny ImageNet 40 35 30 25 20 15 10 5 0 Source and Target Patch Mixing Style 45 (a) (b) (c) (d) 5.0 4.5 Figure 3: The effect of using different saliency detection methods in the proposed data augmentation on (a) CIFAR10 and (b) Tiny-ImageNet classiﬁcation tasks. Different ways of selection and mixing of the source patch with the target image and their effects on (c) CIFAR 10 and (d) Tiny-ImageNet classiﬁcation tasks. Performance is reported from the average of ﬁve and three runs for CIFAR10 and Tiny-ImageNet, respectively. focuses on the foreground. Let Is ∈RW×H×C is a randomly selected training (source) image with label ys from which a patch will be cut. Then its saliency map detection can be represented as Ivs = f(Is), (1) where Ivs ∈RW×H represents the visual saliency map of the given source image Is as shown in Figure 2 where the objects of interest have higher intensity values and f(·) represents a saliency detection model. Then we search for a pixel Ii,j vs in the saliency map that has the maximum intensity value. The i,j represent the xand ycoordinates of the most salient pixel and can be found as i,j = argmax(Ivs), (2) Then we select a patch, either by centering on theIi,j vs −thpixel if possible, or keeping theIi,j vs −th pixel on the selected patch. It ensures that the patch is selected from the object region, not from the background. The size of the patch is determined based on a combination ratio λwhich is sampled from the uniform distribution (0,1) to decide the percentage of an image to be cropped. 3.2 M IXING THE PATCHES AND LABELS Let It ∈RW×H×C is another randomly selected training (target) image with label yt, to where the source patch will be mixed. SaliencyMix partially mixes It and Is to produce a new training sample Ia, the augmented image, with label ya. The mixing of two images can be deﬁned as Ia = M ⊙Is + M′⊙It, (3) where Ia denotes the augmented image, M ∈ {0,1}W×H represents a binary mask, M′ is the complement of M and ⊙represents element-wise multiplication. First, the source patch location is deﬁned by using the peak salient information and the value ofλand then the corresponding location of the mask M is set to 1 and others to 0. The element-wise multiplication of M with the source image results with an image that removes everything except the region decided to keep. In contrast, M′performs in an opposite way of M i.e., the element-wise multiplication of M′with the target image keeps all the regions except the selected patch. Finally, the addition of those two creates a new training sample that contains the target image with the selected source patch in it (See Figure 2). Besides mixing the images we also mix their labels based on the size of the mixed patches as ya = λyt + (1−λ)ys, (4) where ya denotes the label for the augmented sample and λis the combination ratio. Other ways of mixing are investigated in Section 3.4. 3.3 I MPACT OF DIFFERENT SALIENCY DETECTION METHODS Here we investigate the effect of incorporating various saliency detection methods in our Salien- cyMix data augmentation technique. We use four well-recognized saliency detection algorithms 5Published as a conference paper at ICLR 2021 Table 1: Classiﬁcation performance (average of ﬁve runs) of SOTA data augmentation methods on CIFAR-10 and CIFAR-100 datasets using popular standard architectures. An additional ”+” sign after the dataset name indicates that the traditional data augmentation techniques have also been used during training. METHOD TOP-1 ERROR(%)CIFAR-10 CIFAR-10+ CIFAR-100 CIFAR-100+ RESNET-18 (BASELINE) 10.63 ±0.26 4.72±0.21 36.68±0.57 22.46±0.31RESNET-18 + CUTOUT 9.31±0.18 3.99 ±0.13 34.98±0.29 21.96±0.24RESNET-18 + CUTMIX 9.44±0.34 3.78 ±0.12 34.42±0.27 19.42±0.23RESNET-18 + SALIENCYMIX 7.59±0.22 3.65±0.10 28.73±0.13 19.29±0.21 RESNET-50 (BASELINE) 12.14 ±0.95 4.98 ±0.14 36.48±0.50 21.58±0.43RESNET-50 + CUTOUT 8.84±0.77 3.86 ±0.25 32.97±0.74 21.38±0.69RESNET-50 + CUTMIX 9.16±0.38 3.61 ±0.13 31.65±0.61 18.72±0.23RESNET-50 + SALIENCYMIX 6.81±0.30 3.46±0.08 24.89±0.39 18.57±0.29 WIDERESNET-28-10 (BASELINE) 6.97 ±0.22 3.87 ±0.08 26.06±0.22 18.80±0.08WIDERESNET-28-10 + CUTOUT 5.54±0.08 3.08 ±0.16 23.94±0.15 18.41±0.27WIDERESNET-28-10 + AUTOAUGMENT - 2.60±0.10 - 17.10 ±0.30WIDERESNET-28-10 + PUZZLEMIX(200EPOCHS) - - - 16.23WIDERESNET-28-10 + CUTMIX 5.18±0.20 2.87 ±0.16 23.21±0.20 16.66±0.20WIDERESNET-28-10 + SALIENCYMIX 4.04±0.13 2.76±0.07 19.45±0.32 16.56±0.17 (Montabone & Soto, 2010; Hou & Zhang, 2007; Achanta et al., 2009; Qin et al., 2019), and perform experiments using ResNet-18 as a baseline model on CIFAR-10 dataset and ResNet-50 as a baseline model on Tiny-ImageNet dataset for 200 epochs and 100 epochs, respectively. Note that the statis- tical saliency models (Montabone & Soto, 2010; Hou & Zhang, 2007; Achanta et al., 2009) work on any size of images. But the learning based model i.e., Qin et al. (2019) are not scale invariant and it should resizes the input image to 224 ×224 and the resulting saliency map is scaled back to the original size. Figure 3(a-b) show that Montabone & Soto (2010) performs better on both the datasets and the effects are identical on CIFA10 and ImageNet datasets. As a result, Montabone & Soto (2010) is used to extract the saliency map in the proposed data augmentation technique. 3.4 D IFFERENT WAYS OF SELECTING AND MIXING THE SOURCE PATCH There are several ways to select the source patch and mix it with the target image. In this section, we explore those possible schemes and examine their effect on the proposed method. We use ResNet- 18 and ResNet-50 architectures with SaliencyMix data augmentation and perform experiments on CIFAR-10 and Tiny-ImageNet datasets, respectively. We consider ﬁve possible schemes:(i) Salient to Corresponding, that selects the source patch from the most salient region and mix it to the corre- sponding location of the target image; (ii) Salient to Salient, that selects the source patch from the most salient region and mix it to the salient region of the target image; (iii) Salient to Non-Salient, that selects the source patch from the most salient region but mix it to the non-salient region of the target image; (iv) Non-Salient to Salient, that selects the source patch from the non-salient region of the source image but mix it to the salient region of the target image; and (v) Non-Salient to Non- Salient, that selects the source patch from the non-salient region of the source image and also mix it to the non-salient region of the target image. To ﬁnd out the non-salient region, we use the least important pixel of an image. Figure 3(c-d) show the classiﬁcation performance of the proposed SaliencyMix data augmentation with the above mentioned selection and mixing schemes. Similar to the Section 3.3, the effects of the proposed method are similar for CIFAR10 and Tiny-ImageNet datasets. Both the Non-Salient to Salient and Non-Salient to Non-Salient select the source patch from the non-salient region of the source image that doesn’t contain any information about the source object and thereby produce large classiﬁcation error compared to the other three options where the patch is selected from the most salient region of the source image. It justiﬁes our SaliencyMix i.e., the source patch should be selected in such a way so that it must contain information about the source object. On the other hand, Salient to Salient covers the most signiﬁcant part of the target image that restricts the model from learning its most important feature andSalient to Non-Salient may not occlude the target object which is necessary to improve the regularization. But Salient to Corresponding keeps balance by changeably occluding the most important part and other based on the orientation of the source and target object. Consequently, it produces more variety of augmented data and thereby achieves the lowest classiﬁcation error. Also, it introduces less computational burden since only the source image saliency detection is required. Therefore, the proposed method uses Salient to Corresponding as the default selection and mixing scheme. 6Published as a conference paper at ICLR 2021 Table 2: Performance comparison (the best performance) of SOTA data augmentation strategies on ImageNet classiﬁcation with stan- dard model architectures. METHOD TOP-1 T OP-5 ERROR(%) E RROR(%) RESNET-50 (BASELINE) 23.68 7.05 RESNET-50 + CUTOUT 22.93 6.66 RESNET-50 + STOCHASTICDEPTH 22.46 6.27 RESNET-50 + MIXUP 22.58 6.40 RESNET-50 + MANIFOLDMIXUP 22.50 6.21 RESNET-50 + AUTOAUGMENT 22.40 6.20 RESNET-50 + DROPBLOCK 21.87 5.98 RESNET-50 + CUTMIX 21.40 5.92 RESNET-50 + PUZZLEMIX 21.24 5.71 RESNET-50 + SALIENCYMIX 21.26 5.76 RESNET-101 (BASELINE) 21.87 6.29 RESNET-101 + CUTOUT 20.72 5.51 RESNET-101 + MIXUP 20.52 5.28 RESNET-101 + CUTMIX 20.17 5.24 RESNET-101 + SALIENCYMIX 20.09 5.15 Table 3: Impact of SaliencyMix trained model on transfer learning to object detec- tion task. The results are reported from the average of three runs. BACKBONENETWORK IMAGENET DETECTION CLS. ERR. (F-RCNN) TOP-1 (%) ( MAP) RESNET-50 (BASELINE) 23.68 76.71 (+0.00) CUTOUT-TRAINED 22.93 77.17 (+0.46) MIXUP-TRAINED 22.58 77.98 (+1.27) CUTMIX-TRAINED 21.40 78.31 (+1.60) SALIENCYMIX-TRAINED 21.26 78.48 (+1.77) 4 E XPERIMENTS We verify the effectiveness of the proposed SaliencyMix data augmentation strategy on multiple tasks. We evaluate our method on image classiﬁcation by applying it on several benchmark image recognition datasets using popular SOTA architectures. We also use the SaliencyMix trained model and ﬁne-tune it for object detection task to verify its usefulness in enhancing the detection perfor- mance. Furthermore, we validate the robustness of the proposed method against adversarial attacks. All experiments were performed on PyTorch platform with four NVIDIA GeForce RTX 2080 Ti GPUs. 4.1 I MAGE CLASSIFICATION 4.1.1 CIFAR-10 AND CIFAR-100 There are 60,000 color images of size 32×32 pixels in both the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2012) where CIFAR-10 has 10 distinct classes and CIFAR-100 has 100 classes. The number of training and test images in each dataset is 50,000 and 10,000, respectively. We apply several standard architectures: a deep residual network (He et al., 2016) with a depth of18 (ResNet- 18) and 50 (ResNet-50), and a wide residual network (Zagoruyko & Komodakis, 2016) with a depth of 28, a widening factor of 10, and dropout with a drop probability of p= 0.3 in the convolutional layers (WideResNet-28-10). We train the networks for 200 epochs with a batch size of 256 using stochastic gradient descent (SGD), Nesterov momentum of 0.9, and weight decay of 5e−4. The initial learning rate was 0.1 and decreased by a factor of 0.2 after each of the 60,120, and 160 epochs. The images are normalized using per-channel mean and standard deviation. We perform experiments with and without a traditional data augmentation scheme where the traditional data augmentation includes zero-padding, random cropping, and horizontal ﬂipping. Table 1 presents the experimental results on CIFAR datasets where the results are reported on ﬁve runs average. It can be seen that for each of the architectures, the proposed SaliencyMix data aug- mentation strategy outperforms all other methods except PuzzleMix (Kim et al., 2020). It is worth noting that PuzzleMix (Kim et al., 2020) and AutoAugment (Cubuk et al., 2019) require additional optimization process to ﬁnd out the best augmentation criterion, thereby introduces computational burden. On the other hand, rest of the methods do not require such process. The proposed method achieves the best known top-1 error of2.76% and 16.56% for WideResNet-28-10 on CIFAR-10 and CIFAR-100 datasets, respectively. Moreover, SaliencyMix shows signiﬁcant performance improve- ment over CutMix (Yun et al., 2019) when applied without any traditional augmentation technique. It reduces the error rate by1.85%,2.35%,and 1.14% on CIFAR-10 dataset when applied with ResNet- 18, ResNet-50 and WideResNet-28-10 architectures, respectively. Using the same architectures, it reduces the error rate by 5.69%,6.76%,and 3.76% on CIFAR-100 dataset, respectively. 4.1.2 I MAGE NET ImageNet (Olga et al., 2015) contains 1.2 million training images and 50,000 validation images of 1000 classes. To perform experiments on ImageNet dataset, we apply the same settings as used in 7Published as a conference paper at ICLR 2021 Baseline (ResNet50) Cutout Mixup CutMix SaliencyMix Vizsla Mountain Tent Marmot (a) CAM visualizations on un-augmented images. The proposed data augmentation method guides the model to precisely focus on target object. CAM for   Golden  Retriever  CAM for   Tiger Cat  Augmented  Image Mixup Cutout Cutmix SaliencyMix Golden Retriever Tiger Cat Target ImageSource Image (b) CAM visualizations on augmented images. Left most column shows the original images, top row shows the input augmented images by different meth- ods, middle and bottom rows show the CAM for ’Golden Retriever’, and ’Tiger Cat’ class, respectively. The proposed method and Yun et al. (2019) improves the localization ability of the model. Figure 4: Class activation map (CAM) for models that are trained with various data augmentation techniques. The images are randomly taken from ImageNet (Olga et al., 2015) validation set. Yun et al. (2019) for a fair comparison. We have trained our SaliencyMix for 300 epochs with an initial learning rate of 0.1 and decayed by a factor of 0.1 at epochs 75,150,and 225, with a batch size of 256. Also, the traditional data augmentations such as resizing, cropping, ﬂipping, and jitters have been applied during the training process. Table 2 presents the ImageNet experimental results where the best performance of each method is reported. SaliencyMix outperforms all other methods in comparison and shows competitive results with PuzzleMix (Kim et al., 2020). It drops top-1 error for ResNet-50 by 1.66%,1.31%, and 0.14% over Cutout, Mixup and CutMix data augmentation, respectively. For ResNet-101 architecture, SaliencyMix achieves the new best result of 20.09% top-1 error and 5.15% top-5 error. 4.2 O BJECT DETECTION USING PRE-TRAINED SALIENCY MIX In this section, we use the SaliencyMix trained model to initialize the Faster RCNN (Shaoqing et al., 2015) that uses ResNet-50 as a backbone network and examine its effect on object detection task. The model is ﬁne-tuned on Pascal VOC 2007 and 2012 datasets and evaluated on VOC 2007 test data using the mAP metric. We follow the ﬁne-tuning strategy of the original method (Shaoqing et al., 2015). The batch size, learning rate, and training iterations are set to8,4e−3,and 41K, respectively and the learning rate is decayed by a factor of0.1 at 33Kiterations. The results are shown in Table 3. Pre-training with CutMix and SaliencyMix signiﬁcantly improves the performance of Faster RCNN. This is because in object detection, foreground information (positive data) is much more important than the background (Lin et al., 2017). Since SaliencyMix helps the augmented image to have more foreground or object part than the background, it leads to better detection performance. It can be seen that SaliencyMix trained model outperforms other methods and achieves a performance gain of +1.77 mAP. 4.3 C LASS ACTIVATION MAP (CAM) A NALYSIS Class Activation Map (CAM) (Zhou et al., 2016) ﬁnds out the regions of input image where the model focuses to recognize an object. To investigate this, we extract CAM of models that are trained with various data augmentation techniques. Here we use a vanilla ResNet-50 model equipped with various data augmentation techniques and trained on ImageNet (Olga et al., 2015). Then we extract CAM for un-augmented images as well as for augmented images. Figure 4 presents the experimental results. Figure 4a shows that the proposed data augmentation technique guides the model to precisely focus on the target object compared to others. Also, Figure 4b shows the similar effect when we search for a speciﬁc object in a scene with multiple objects. It can be seen that Mixup (Zhang et al., 2017) has a severe problem of being confused when trying to recognize an object because the pixels are mixed and it is not possible to extract class speciﬁc features. Also, Cutout (Devries & Taylor, 2017) suffers disadvantages due to the uninformative image region. On the other hand, both the CutMix (Yun et al., 2019) and SaliencyMix effectively focuses on the corresponding features and precisely localizes the two objects in the scene. 8Published as a conference paper at ICLR 2021 Table 4: Performance comparison on adversar- ial robustness. Top-1 accuracy (%) of various data augmentation techniques on adversarially perturbed ImageNet validation set. BASELINECUTOUTMIXUP CUTMIX SALIENCYMIX ACC. (%) 8.2 11.5 24.4 31.0 32.96 Table 5: Training time comparison of various data augmentation techniques using ResNet-18 architecture on CIFAR-10 dataset. BASELINECUTOUTMIXUP CUTMIX SALIENCYMIX TIME(HOUR) 0.83 0.84 0.87 0.89 0.91 4.4 R OBUSTNESS AGAINST ADVERSARIAL ATTACK Deep learning based models are vulnerable to adversarial examples i.e., they can be fooled by slightly modiﬁed examples even when the added perturbations are small and unrecognizable (Szegedy et al., 2014; Goodfellow et al., 2015; Madry et al., 2017). Data augmentation helps to increase the robustness against adversarial perturbations since it introduces many unseen image samples during the training (Madry et al., 2017). Here we verify the adversarial robustness of a model that is trained using various data augmentation techniques and compare their effectiveness. Fast Gradient Sign Method (FGSM) (Madry et al., 2017) is used to generate the adversarial examples and ImageNet pre-trained models of each data augmentation techniques with ResNet-50 architecture is used in this experiment. Table 4 reports top-1 accuracy of various augmentation techniques on adversarially attacked ImageNet validation set. Due to the appropriate feature representation learn- ing and focusing on the overall object rather than a small part, SaliencyMix signiﬁcantly improves the robustness against the adversarial attack and achieves1.96% performance improvement over the nearly comparable method CutMix (Yun et al., 2019). 4.5 C OMPUTATIONAL COMPLEXITY We investigate the computational complexity of the proposed method and compare it with other data augmentation techniques in terms of training time. All the models are trained on CIFAR-10 dataset using ResNet-18 architecture for 200 epochs. Table 5 presents the training time comparison. It can be seen that SaliencyMix requires a slightly longer training time compared to others, due to saliency map generation. But considering the performance improvement, it can be negligible. 5 C ONCLUSION We have introduced an effective data augmentation strategy, called SaliencyMix, that is carefully designed for training CNNs to improve their classiﬁcation performance and generalization ability. The proposed SaliencyMix guides the models to focus on the overall object regions rather than a small region of input images and also prevents the model from learning in-appropriate feature representation by carefully selecting the representative source patch. It introduces a little compu- tational burden due to saliency detection, while signiﬁcantly boosts up the model performance and strengthen the model robustness on various computer vision tasks. Applying SaliencyMix with WideResNet achieves the new best known top-1 error of 2.76% and 16.56% on CIFAR-10 and CIFAR-100, respectively. On ImageNet classiﬁcation, applying SaliencyMix with ResNet-50 and ResNet-101 obtains the new best known top-1 error of 21.26% and 20.09%, respectively. On ob- ject detection, using the SaliencyMix trained model to initialize the Faster RCNN (ResNet-50 as a backbone network) and ﬁne-tuning leads to a performance improvement by +1.77 mAP. Further- more, SaliencyMix trained model is found to be more robust against adversarial attacks and achieves 1.96% accuracy improvement on adversarially perturbed ImageNet validation set compared to the nearly comparable augmentation method. Considering more detailed and/or high level semantic information for data augmentation will be our future work. ACKNOWLEDGMENTS This research was supported by Basic Science Research Program through the National Re- search Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (2018R1C1B3008159) and Basic Science Research Program through the National Research Foun- dation of Korea (NRF) under Grant [NRF-020R1F1A1050014]. 9Published as a conference paper at ICLR 2021 REFERENCES R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1597–1604, June 2009. doi: 10.1109/CVPR.2009.5206596. Yoshua Bengio, Fr ´ed´eric Bastien, Arnaud Bergeron, Nicolas Boulanger–Lewandowski, Thomas Breuel, Youssouf Chherawala, Moustapha Cisse, Myriam C ˆot´e, Dumitru Erhan, Jeremy Eu- stache, Xavier Glorot, Xavier Muller, Sylvain Pannetier Lebeuf, Razvan Pascanu, Salah Rifai, Franc ¸ois Savard, and Guillaume Sicard. Deep learners beneﬁt more from out-of-distribution ex- amples. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research , pp. 164–172, Fort Laud- erdale, FL, USA, 11–13 Apr 2011. PMLR. URL http://proceedings.mlr.press/ v15/bengio11b.html. Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intelligence , 37(3):569–582, 2014. R. Cong, J. Lei, H. Fu, M. Cheng, W. Lin, and Q. Huang. Review of visual saliency detection with comprehensive information. IEEE Transactions on Circuits and Systems for Video Technology , 29(10):2941–2959, Oct 2019. ISSN 1558-2205. doi: 10.1109/TCSVT.2018.2870832. E. D. Cubuk, B. Zoph, D. Man´e, V . Vasudevan, and Q. V . Le. Autoaugment: Learning augmentation strategies from data. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 113–123, 2019. doi: 10.1109/CVPR.2019.00020. Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, and Pheng-Ann Heng. R3net: Recurrent residual reﬁnement network for saliency detection. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pp. 684–690. AAAI Press, 2018. Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. ArXiv, abs/1708.04552, 2017. Mark Everingham, Luc Van Gool, Christopher Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88:303– 338, 06 2010. doi: 10.1007/s11263-009-0275-4. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias im- proves accuracy and robustness. In International Conference on Learning Representations, 2019. Ghiasi Golnaz, Lin Tsung-Yi, and V . Le Quoc. Dropblock: A regularization method for convolu- tional networks. In Neural Information Processing Systems (NeurIPS), 2018. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint, 01 2015. URL https://arxiv.org/abs/1412.6572. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, June 2016. doi: 10.1109/CVPR.2016.90. X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8, June 2007. doi: 10.1109/CVPR.2007.383267. Choe Junsuk and Shim Hyunjung. Attention-based dropout layer for weakly supervised object localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2219–2228, 2019. Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012. 10Published as a conference paper at ICLR 2021 Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In Neural Information Processing Systems (NeurIPS), pp. 1097–1105, January 2012. doi: 10.1145/3065386. Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, Nov 1998. ISSN 1558-2256. doi: 10.1109/5.726791. Jianjun Lei, Bingren Wang, Yuming Fang, Weisi Lin, Patrick Le Callet, Nam Ling, and Chunping Hou. A universal framework for salient object detection. IEEE Transactions on Multimedia, 18 (9):1783–1795, 2016. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. Changyang Li, Yuchen Yuan, Weidong Cai, Yong Xia, and David Dagan Feng. Robust saliency detection via regularized random walks ranking. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pp. 2710–2717, 2015. Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via dense and sparse reconstruction. In Proceedings of the IEEE international conference on computer vision, pp. 2976–2983, 2013. T. Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar. Focal loss for dense object detection. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999–3007, Oct 2017. doi: 10.1109/ICCV .2017.324. Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3089–3098, 2018. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2017. Sebastian Montabone and Alvaro Soto. Human detection using a mobile platform and novel features derived from a visual saliency mechanism. Image and Vision Computing, 28(3):391–402, 2010. Srivastava Nitish, Hinton Geoffrey, Krizhevsky Alex, Sutskever Ilya, and Salakhutdinov Rus- lan. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Ma- chine Learning Research , 15:1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html. Russakovsky Olga, Deng Jia, Su Hao, Krause Jonathan, Satheesh Sanjeev, Ma Sean, Huang Zhi- heng, Karpathy Andrej, Khosla Aditya, Bernstein Michael, C. Berg Alexander, and Fei-Fei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115 (3):211–252, 2015. Houwen Peng, Bing Li, Haibin Ling, Weiming Hu, Weihua Xiong, and Stephen J Maybank. Salient object detection via structured matrix decomposition. IEEE transactions on pattern analysis and machine intelligence, 39(4):818–832, 2016. Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Yao Qin, Huchuan Lu, Yiqun Xu, and He Wang. Saliency detection via cellular automata. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 110–119, 2015. Ren Shaoqing, He Kaiming, Girshick Ross, and Sun Jian. Faster r-cnn: Towards real-time object detection with region proposal networks. In Neural Information Processing Systems (NeurIPS), 2015. 11Published as a conference paper at ICLR 2021 K. K. Singh and Y . J. Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3544–3553, 2017. C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel- low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199. DeVries Terrance and W Taylor Graham. Improved regularization of convolutional neural networks with cutout. arXiv preprint, 2017. URL https://arxiv.org/abs/1708.04552. Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann Lecun, and Christoph Bregler. Efﬁcient object localization using convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 648–656, 06 2015. doi: 10.1109/CVPR.2015.7298664. Ren Wu, Shengen Yan, Yi Shan, Qingqing Dang, and Gang Sun. Deep image: Scaling up image recognition. arXiv preprint, 01 2015. URL https://arxiv.org/abs/1501.02876. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Interna- tional Conference on Computer Vision (ICCV), 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British Ma- chine Vision Conference 2016, 2016. doi: 10.5244/c.30.87. URL http://dx.doi.org/10. 5244/C.30.87. Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em- pirical risk minimization. arXiv preprint, 2017. URL https://arxiv.org/abs/1710. 09412. Lu Zhang, Ju Dai, Huchuan Lu, You He, and Gang Wang. A bi-directional message passing model for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1741–1750, 2018a. Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu, and Gang Wang. Progressive attention guided recurrent network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 714–722, 2018b. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discrimina- tive localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2921–2929, June 2016. doi: 10.1109/CVPR.2016.319. Li Zhou, Zhaohui Yang, Qing Yuan, Zongtan Zhou, and Dewen Hu. Salient region detection via inte- grating diffusion-based compactness and local contrast. IEEE Transactions on Image Processing, 24(11):3308–3320, 2015. Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun. Saliency optimization from robust back- ground detection. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 2814–2821, 2014. Zhong Zhun, Zheng Liang, Kang Guoliang, Li Shaozi, and Yang Yi. Random erasing data augmen- tation. arXiv preprint, 2017. URL https://arxiv.org/abs/1708.04896. 12",
      "references": [
        "Frequency-tuned salient region detection.",
        "Deep learners benefit more from out-of-distribution examples.",
        "Global contrast based salient region detection.",
        "Review of visual saliency detection with comprehensive information.",
        "Autoaugment: Learning augmentation strategies from data.",
        "R3net: Recurrent residual refinement network for saliency detection.",
        "Improved regularization of convolutional neural networks with cutout.",
        "The pascal visual object classes (voc) challenge.",
        "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.",
        "Dropblock: A regularization method for convolutional networks.",
        "Explaining and harnessing adversarial examples.",
        "Deep residual learning for image recognition.",
        "Saliency detection: A spectral residual approach.",
        "Attention-based dropout layer for weakly supervised object localization.",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup",
        "Learning multiple layers of features from tiny images.",
        "Imagenet classification with deep convolutional neural networks.",
        "Gradient-based learning applied to document recognition.",
        "Smart augmentation learning an optimal data augmentation strategy.",
        "Robust saliency detection via regularized random walks ranking.",
        "Saliency detection via dense and sparse reconstruction.",
        "Focal loss for dense object detection.",
        "Picanet: Learning pixel-wise contextual attention for saliency detection.",
        "Towards deep learning models resistant to adversarial attacks.",
        "Human detection using a mobile platform and novel features derived from a visual saliency mechanism.",
        "Dropout: A simple way to prevent neural networks from overfitting.",
        "Imagenet large scale visual recognition challenge.",
        "Salient object detection via structured matrix decomposition.",
        "Basnet: Boundary-aware salient object detection.",
        "Saliency detection via cellular automata.",
        "Faster r-cnn: Towards real-time object detection with region proposal networks.",
        "Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization.",
        "Rethinking the inception architecture for computer vision.",
        "Intriguing properties of neural networks.",
        "Efficient object localization using convolutional networks.",
        "Deep image: Scaling up image recognition.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Wide residual networks.",
        "mixup: Beyond empirical risk minimization.",
        "A bi-directional message passing model for salient object detection.",
        "Progressive attention guided recurrent network for salient object detection.",
        "Learning deep features for discriminative localization.",
        "Salient region detection via integrating diffusion-based compactness and local contrast.",
        "Saliency optimization from robust background detection.",
        "Random erasing data augmentation."
      ],
      "meta_data": {
        "arxiv_id": "2006.01791v2",
        "authors": [
          "A. F. M. Shahab Uddin",
          "Mst. Sirazam Monira",
          "Wheemyung Shin",
          "TaeChoong Chung",
          "Sung-Ho Bae"
        ],
        "published_date": "2020-06-02T17:18:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces SaliencyMix, a saliency-guided data-augmentation technique that improves regularization by cutting a representative patch from a source image (identified via visual–saliency map) and pasting it into a target image, thereby avoiding background-only patches that can mislead label mixing; demonstrates state-of-the-art or competitive performance on image classification (CIFAR-10/100, ImageNet), boosts object-detection transfer learning, and increases adversarial robustness.",
        "methodology": "1. Generate a saliency map of a randomly chosen source image using an unsupervised bottom-up saliency detector (best results with Montabone & Soto 2010).\n2. Locate the peak salient pixel and define a square patch around it; patch size determined by a mixing ratio λ~U(0,1).\n3. Paste this patch into the same spatial location of a randomly chosen target image (\"Salient-to-Corresponding\").\n4. Construct a binary mask to form the augmented image I_a = M⊙I_s + (1−M)⊙I_t.\n5. Mix labels proportionally: y_a = λ y_t + (1−λ) y_s (label smoothing effect).\n6. Integrate seamlessly into standard CNN training without altering network architecture.",
        "experimental_setup": "• Datasets: CIFAR-10 & CIFAR-100 (50K train / 10K test, 32×32); Tiny-ImageNet (100K train, 200 classes, 64×64) for ablations; ImageNet ILSVRC-2012 (1.2M train, 50K val, 1000 classes).\n• Architectures: ResNet-18, ResNet-50, WideResNet-28-10, ResNet-101 for classification; Faster R-CNN with ResNet-50 backbone for detection transfer.\n• Training details: SGD with momentum 0.9, weight decay 5e-4; CIFAR trained 200 epochs (lr 0.1 decayed at 60/120/160), ImageNet 300 epochs (lr 0.1 decayed at 75/150/225); batch size 256.\n• Baselines/Comparisons: CutOut, MixUp, CutMix, AutoAugment, PuzzleMix, DropBlock, Stochastic Depth.\n• Metrics: Top-1/Top-5 error (classification), mean Average Precision (VOC detection), Top-1 accuracy under FGSM attack (robustness).\n• Validation: Average of multiple runs (5 for CIFAR, 3 for Tiny-ImageNet, 3 for detection).",
        "limitations": "1. Additional computational cost (~2–3% longer training) from generating saliency maps for every source image.\n2. Effectiveness depends on the quality of the external saliency detector; failure cases may arise when saliency maps are inaccurate (e.g., cluttered scenes, small objects).\n3. Patch size and placement still rely on heuristic λ sampling; not jointly optimized with network learning.\n4. Evaluations limited to image classification, VOC-scale detection, and single-step FGSM; generalization to more complex tasks (e.g., segmentation, large-scale detection, video) unverified.\n5. Requires per-image saliency computation at training time, hindering on-the-fly data-pipeline efficiency for very large datasets.",
        "future_research_directions": "• Develop end-to-end learnable or adaptive saliency estimation so the augmentation is jointly optimized with the task network.\n• Explore multi-patch or semantic-level mixing guided by object detectors or segmentation masks.\n• Apply SaliencyMix to other domains such as video, 3D data, medical imaging, and natural language.\n• Combine with search-based policies (e.g., AutoAugment) or adversarial training for further robustness gains.\n• Investigate lightweight saliency extraction or caching strategies to reduce computational overhead.\n• Extend evaluation to dense prediction tasks (semantic/instance segmentation) and large-scale detection benchmarks to test generality.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "A Baseline for Few-Shot Image Classification",
      "full_text": "Published as a conference paper at ICLR 2020 A BASELINE FOR FEW-SHOT IMAGE CLASSIFICATION Guneet S. Dhillon1, Pratik Chaudhari2∗, Avinash Ravichandran1, Stefano Soatto1,3 1Amazon Web Services, 2University of Pennsylvania, 3University of California, Los Angeles {guneetsd, ravinash, soattos}@amazon.com, pratikac@seas.upenn.edu ABSTRACT Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When ﬁne-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered- ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the ﬁrst few-shot learning results on the ImageNet-21k dataset. We ﬁnd that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantiﬁes the “hardness” of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way. 1 I NTRODUCTION Prototypical Networks [2017] MAML [2017] LEO [2018] MetaOpt SVM [2019] Transductive Fine-Tuning 0 20 40 60 80 1001-shot, 5-way accuracy on Mini-Imagenet (%) Figure 1: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shot algorithms on the Mini-ImageNet (Vinyals et al., 2016) dataset for the 1-shot 5-way protocol. The boxes show the ± 25% quantiles of the accuracy while the notches indicate the median and its 95% conﬁdence interval. Whiskers denote the 1.5 × interquartile range which captures 99.3% of the probability mass for a normal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shot accuracies is large too. This suggests that progress may be illusory, especially considering that none outperform the simple transductive ﬁne-tuning baseline discussed in this paper (rightmost). As image classiﬁcation systems begin to tackle more and more classes, the cost of annotating a massive number of images and the difﬁculty of procuring images of rare categories increases. This has fueled interest in few-shot learning, where only few labeled samples per class are available for training. Fig. 1 displays a snapshot of the state-of-the-art. We estimated this plot by using published ∗Work done while at Amazon Web Services 1 arXiv:1909.02729v5  [cs.LG]  21 Oct 2020Published as a conference paper at ICLR 2020 numbers for the estimate of the mean accuracy, the 95% conﬁdence interval of this estimate and the number of few-shot episodes. For MAML (Finn et al., 2017) and MetaOpt SVM (Lee et al., 2019), we use the number of episodes in the author’s Github implementation. The ﬁeld appears to be progressing steadily albeit slowly based on Fig. 1. However, the variance of the estimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero (e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance of the accuracies is extremely large in Fig. 1. This suggests that progress in the past few years may be less signiﬁcant than it seems if one only looks at the mean accuracies. To compound the problem, many algorithms report results using different models for different number of ways (classes) and shots (number of labeled samples per class), with aggressive hyper-parameter optimization.1 Our goal is to develop a simple baseline for few-shot learning, one that does not require specialized training depending on the number of ways or shots, nor hyper-parameter tuning for different protocols. The simplest baseline we can think of is to pre-train a model on the meta-training dataset using the standard cross-entropy loss, and then ﬁne-tune on the few-shot dataset. Although this approach is basic and has been considered before (Vinyals et al., 2016; Chen et al., 2018), it has gone unnoticed that it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing ﬁne-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard benchmarks and few-shot protocols (cf. Table 1). Our contribution is to develop a transductive ﬁne-tuning baseline for few-shot learning, our approach works even for a single labeled example and a single test datum per class. Our baseline outperforms the state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al., 2018), all with the same hyper-parameters. Current approaches to few-shot learning are hard to scale to large datasets. We report the ﬁrst few-shot learning results on the ImageNet-21k dataset (Deng et al., 2009) which contains 14.2 million images across 21,814 classes. The rare classes in ImageNet-21k form a natural benchmark for few-shot learning. The empirical performance of this baseline, should not be understood as us suggesting that this is the right way of performing few-shot learning. We believe that sophisticated meta-training, understanding taxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective few-shot learning. The performance of the simple baseline however indicates that we need to interpret existing results2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate that, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically report performance for different few-shot protocols. 2 P ROBLEM DEFINITION AND RELATED WORK We ﬁrst introduce some notation and formalize the few-shot image classiﬁcation problem. Let (x,y) denote an image and its ground-truth label respectively. The training and test datasets are Ds = {(xi,yi)}Ns i=1 and Dq = {(xi,yi)}Nq i=1 respectively, where yi ∈Ct for some set of classes Ct. In the few-shot learning literature, training and test datasets are referred to as support and query datasets respectively, and are collectively called a few-shot episode. The number of ways, or classes, is |Ct|. The set {xi |yi = k, (xi,yi) ∈Ds}is the support of class kand its cardinality is ssupport shots (this is non-zero and is generally shortened to shots). The number sis small in the few-shot setting. The set {xi |yi = k,(xi,yi) ∈Dq}is the query of class kand its cardinality is qquery shots. The goal is to learn a function F to exploit the training set Ds to predict the label of a test datum x, 1For instance, Rusu et al. (2018) tune for different few-shot protocols, with parameters changing by up to six orders of magnitude; Oreshkin et al. (2018) use a different query shot for different few-shot protocols. 2For instance, Vinyals et al. (2016); Ravi & Larochelle (2016) use different versions of Mini-ImageNet; Oreshkin et al. (2018) report results for meta-training on the training set while Qiao et al. (2018) use both the training and validation sets; Chen et al. (2018) use full-sized images from the parent ImageNet-1k dataset (Deng et al., 2009); Snell et al. (2017); Finn et al. (2017); Oreshkin et al. (2018); Rusu et al. (2018) use different model architectures of varying sizes, which makes it difﬁcult to disentangle the effect of their algorithmic contributions. 2Published as a conference paper at ICLR 2020 where (x,y) ∈Dq, by ˆy= F(x; Ds). (1) Typical approaches for supervised learning replace Ds above with a statistic, θ∗= θ∗(Ds) that is, ideally, sufﬁcient to classify Ds, as measured by, say, the cross-entropy loss θ∗(Ds) = arg min θ 1 Ns ∑ (x,y)∈Ds −log pθ(y|x), (2) where pθ(·|x) is the probability distribution on Ct as predicted by the model in response to input x. When presented with a test datum, the classiﬁcation rule is typically chosen to be of the form Fθ∗ (x; Ds) ≜ arg max k pθ∗ (k|x), (3) where Ds is represented by θ∗. This form of the classiﬁer entails a loss of generality unless θ∗is a sufﬁcient statistic, pθ∗ (y|x) = p(y|x), which is of course never the case, especially given few labeled data in Ds. However, it conveniently separates training and inference phases, never having to revisit the training set. This might be desirable in ordinary image classiﬁcation, but not in few-shot learning. We therefore adopt the more general form of F in (1). If we call the test datum x= xNs+1 , then we can obtain the general form of the classiﬁer by ˆy= F(x; Ds) = arg min yNs+1 min θ 1 Ns + 1 Ns+1∑ i=1 −log pθ(yi|xi). (4) In addition to the training set, one typically also has a meta-training set, Dm = {(xi,yi)}Nm i=1, where yi ∈ Cm, with set of classes Cm disjoint from Ct. The goal of meta-training is to use Dm to infer the parameters of the few-shot learning model: ˆθ(Dm; (Ds,Dq)) = arg minθ 1 Nm ∑ (x,y)∈Dm ℓ(y,Fθ(x; (Ds,Dq))),where meta-training loss ℓdepends on the method. 2.1 R ELATED WORK Learning to learn: The meta-training loss is designed to make few-shot training efﬁcient (Utgoff, 1986; Schmidhuber, 1987; Baxter, 1995; Thrun, 1998). This approach partitions the problem into a base-level that performs standard supervised learning and a meta-level that accrues information from the base-level. Two main approaches have emerged to do so. Gradient-based approaches: These approaches treat the updates of the base-level as a learnable mapping (Bengio et al., 1992). This mapping can be learnt using temporal models (Hochreiter et al., 2001; Ravi & Larochelle, 2016), or one can back-propagate the gradients across the base-level updates (Maclaurin et al., 2015; Finn et al., 2017). It is challenging to perform this dual or bi-level optimization, respectively. These approaches have not been shown to be competitive on large datasets. Recent approaches learn the base-level in closed-form using SVMs (Bertinetto et al., 2018; Lee et al., 2019) which restricts the capacity of the base-level although it alleviates the optimization problem. Metric-based approaches: A majority of the state-of-the-art algorithms are metric-based approaches. These approaches learn an embedding that can be used to compare (Bromley et al., 1994; Chopra et al., 2005) or cluster (Vinyals et al., 2016; Snell et al., 2017) query samples. Recent approaches build upon this idea with increasing levels of sophistication in learning the embedding (Vinyals et al., 2016; Gidaris & Komodakis, 2018; Oreshkin et al., 2018), creating exemplars from the support set and picking a metric for the embedding (Gidaris & Komodakis, 2018; Allen et al., 2018; Ravichandran et al., 2019). There are numerous hyper-parameters involved in implementing these approaches which makes it hard to evaluate them systematically (Chen et al., 2018). Transductive learning: This approach is more efﬁcient at using few labeled data than supervised learning (Joachims, 1999; Zhou et al., 2004; Vapnik, 2013). The idea is to use information from the test datum xto restrict the hypothesis space while searching for the classiﬁer F(x,Ds) at test time. Our approach is closest to this line of work. We train a model on the meta-training set Dm and 3Published as a conference paper at ICLR 2020 initialize a classiﬁer using the support set Ds. The parameters are then ﬁne-tuned to adapt to the new test datum x. There are recent papers in few-shot learning such as Nichol et al. (2018); Liu et al. (2018a) that are motivated from transductive learning and exploit the unlabeled query samples. The former updates batch-normalization parameters using query samples while the latter uses label propagation to estimate labels of all query samples at once. Semi-supervised learning: We penalize the Shannon Entropy of the predictions on the query samples at test time. This is a simple technique in the semi-supervised learning literature, closest to Grandvalet & Bengio (2005). Modern augmentation techniques such as Miyato et al. (2015); Sajjadi et al. (2016); Dai et al. (2017) or graph-based approaches (Kipf & Welling, 2016) can also be used with our approach; we used the entropic penalty for the sake of simplicity. Semi-supervised few-shot learning is typically formulated as having access to extra unlabeled data during meta-training or few-shot training (Garcia & Bruna, 2017; Ren et al., 2018). This is different from our approach which uses the unlabeled query samples for transductive learning. Initialization for ﬁne-tuning: We use recent ideas from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Chen et al., 2018; Gidaris & Komodakis, 2018) to initialize the meta-trained model for ﬁne-tuning. These works connect the softmax cross-entropy loss with cosine distance and are discussed further in Section 3.1. 3 A PPROACH The simplest form of meta-training is pre-training with the cross-entropy loss, which yields ˆθ= arg min θ 1 Nm ∑ (x,y)∈Dm −log pθ(y|x) + R(θ), (5) where the second term denotes a regularizer, say weight decay R(θ) = ∥θ∥2/2. The model predicts logits zk(x; θ) for k ∈Cm and the distribution pθ(·|x) is computed from these logits using the softmax operator. This loss is typically minimized by stochastic gradient descent-based algorithms. If few-shot training is performed according to the general form in (4), then the optimization is identical to that above and amounts to ﬁne-tuning the pre-trained model. However, the model needs to be modiﬁed to account for the new classes. Careful initialization can make this process efﬁcient. 3.1 S UPPORT -BASED INITIALIZATION Given the pre-trained model (called the “backbone”), pθ (dropping the hat from ˆθ), we append a new fully-connected “classiﬁer” layer that takes the logits of the backbone as input and predicts the labels in Ct. For a support sample (x,y), denote the logits of the backbone by z(x; θ) ∈R|Cm|; the weights and biases of the classiﬁer by w∈R|Ct|×|Cm|and b∈R|Ct|respectively; and the kth row of wand bby wk and bk respectively. The ReLU non-linearity is denoted by (·)+. If the classiﬁer’s logits are z′ = wz(x; θ)+ + b, the ﬁrst term in the cross-entropy loss: −log pΘ(y|x) = −wyz(x; θ)+ −by + log∑ kewkz(x;θ)++bk would be the cosine distance between wy and z(x; θ)+ if both were normalized to unit ℓ2 norm and bias by = 0. This suggests wy = z(x; θ)+ ∥z(x; θ)+∥ and by = 0 (6) as a candidate for initializing the classiﬁer, along with normalizing z(x; θ)+ to unit ℓ2 norm. It is easy to see that this maximizes the cosine similarity between features z(x; θ)+ and weights wy. For multiple support samples per class, we take the Euclidean average of features z(x; θ)+ for each class in Ct, before ℓ2 normalization in (6). The logits of the classiﬁer are thus given by R|Ct|∋z(x; Θ) = w z(x; θ)+ ∥z(x; θ)+∥+ b, (7) 4Published as a conference paper at ICLR 2020 where Θ = {θ,w,b }, the combined parameters of the backbone and the classiﬁer. Note that we have added a ReLU non-linearity between the backbone and the classiﬁer, before the ℓ2 normalization. All the parameters Θ are trainable in the ﬁne-tuning phase. Remark 1 (Relation to weight imprinting). The support-based initialization is motivated from previous papers (Hu et al., 2015; Movshovitz-Attias et al., 2017; Chen et al., 2018; Gidaris & Komodakis, 2018). In particular, Qi et al. (2018) use a similar technique, with minor differences, to expand the size of the ﬁnal fully-connected layer (classiﬁer) for low-shot continual learning. The authors call their technique “weight imprinting” because wk can be thought of as a template for class k. In our case, we are only interested in performing well on the few-shot classes. Remark 2 (Using logits of the backbone instead of features as input to the classiﬁer). A natural way to adapt the backbone to predict new classes is to re-initialize its ﬁnal fully-connected layer (classiﬁer). We instead append a new classiﬁer after the logits of the backbone. This is motivated from Frosst et al. (2019) who show that for a trained backbone, outputs of all layers are entangled, without class-speciﬁc clusters; but the logits are peaked on the correct class, and are therefore well-clustered. The logits are thus better inputs to the classiﬁer as compared to the features. We explore this choice via an experiment in Appendix C.6. 3.2 T RANSDUCTIVE FINE -TUNING In (4), we assumed that there is a single query sample. However, we can also process multiple query samples together, and perform the minimization over all unknown query labels. We introduce a regularizer, similar to Grandvalet & Bengio (2005), as we seek outputs with a peaked posterior, or low Shannon Entropy H. So the transductive ﬁne-tuning phase solves for Θ∗= arg min Θ 1 Ns ∑ (x,y)∈Ds −log pΘ (y|x) + 1 Nq ∑ (x,y)∈Dq H(pΘ(·|x)). (8) Note that the data ﬁtting term uses the labeled support samples whereas the regularizer uses the unlabeled query samples. The two terms can be highly imbalanced (due to the varying range of values for the two quantities, or due to the variance in their estimates which depend onNs and Nq). To allow ﬁner control on this imbalance, one can use a coefﬁcient for the entropic term and/or a temperature in the softmax distribution of the query samples. Tuning these hyper-parameters per dataset and few-shot protocol leads to uniform improvements in the results in Section 4 by 1-2%. However, we wish to keep in line with our goal of developing a simple baseline and refrain from optimizing these hyper-parameters, and set them equal to 1 for all experiments on benchmark datasets. 4 E XPERIMENTAL RESULTS We show results of transductive ﬁne-tuning on benchmark datasets in few-shot learning, namely Mini-ImageNet (Vinyals et al., 2016), Tiered-ImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto et al., 2018) and FC-100 (Oreshkin et al., 2018), in Section 4.1. We also show large-scale experiments on the ImageNet-21k dataset (Deng et al., 2009) in Section 4.2. Along with the analysis in Section 4.3, these help us design a metric that measures the hardness of an episode in Section 4.4. We sketch key points of the experimental setup here; see Appendix A for details. Pre-training: We use the WRN-28-10 (Zagoruyko & Komodakis, 2016) model as the backbone. We pre-train using standard data augmentation, cross-entropy loss with label smoothing (Szegedy et al., 2016) of ϵ=0.1, mixup regularization (Zhang et al., 2017) of α=0.25, SGD with batch-size of 256, Nesterov’s momentum of 0.9, weight-decay of10−4 and no dropout. We use batch-normalization (Ioffe & Szegedy, 2015) but exclude its parameters from weight decay (Jia et al., 2018). We use cyclic learning rates (Smith, 2017) and half-precision distributed training on 8 GPUs (Howard et al., 2018) to reduce training time. Each dataset has a training, validation and test set consisting of disjoint sets of classes. Some algorithms use only the training set as the meta-training set (Snell et al., 2017; Oreshkin et al., 2018), while others use both training and validation sets (Rusu et al., 2018). For completeness we report 5Published as a conference paper at ICLR 2020 results using both methodologies; the former is denoted as (train) while the latter is denoted as (train + val). All experiments in Sections 4.3 and 4.4 use the (train + val) setting. Fine-tuning: We perform ﬁne-tuning on one GPU in full-precision for 25 epochs and a ﬁxed learning rate of 5 ×10−5 with Adam (Kingma & Ba, 2014) without any regularization. We make two weight updates in each epoch: one for the cross-entropy term using support samples and one for the Shannon Entropy term using query samples (cf. (8)). Hyper-parameters: We used images from ImageNet-1k belonging to the training classes of Mini- ImageNet as the validation set for pre-training the backbone for Mini-ImageNet. We used the validation set of Mini-ImageNet to choose hyper-parameters for ﬁne-tuning. All hyper-parameters are kept constant for experiments on benchmark datasets. Evaluation: Few-shot episodes contain classes sampled uniformly from classes in the test sets of the respective datasets; support and query samples are further sampled uniformly for each class; the query shot is ﬁxed to 15 for all experiments unless noted otherwise. All networks are evaluated over 1,000 few-shot episodes unless noted otherwise. To enable easy comparison with existing literature, we report an estimate of the mean accuracy and the 95% conﬁdence interval of this estimate. However, we encourage reporting the standard deviation in light of Section 1 and Fig. 1. 4.1 R ESULTS ON BENCHMARK DATASETS Table 1: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. Best results in each column are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider ResNet-12 which we denote as ResNet-12 ∗. Mini-ImageNet Tiered-ImageNet CIFAR-FS FC-100Algorithm Architecture 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%)Matching networks (Vinyals et al., 2016) conv(64)×4 46.6 60LSTM meta-learner (Ravi & Larochelle,2016) conv(64)×443.44±0.77 60.60±0.71Prototypical Networks (Snell et al., 2017) conv(64)×449.42±0.78 68.20±0.66MAML (Finn et al., 2017) conv(32)×448.70±1.84 63.11±0.92R2D2 (Bertinetto et al., 2018) conv(96k)×451.8±0.2 68.4±0.2 65.4 ±0.2 79.4±0.2TADAM (Oreshkin et al., 2018) ResNet-12 58.5±0.3 76.7±0.3 40.1 ±0.4 56.1±0.4Transductive Propagation (Liu et al.,2018b) conv(64)×455.51±0.86 69.86±0.65 59.91±0.94 73.30±0.75Transductive Propagation (Liu et al.,2018b) ResNet-12 59.46 75.64MetaOpt SVM (Lee et al., 2019) ResNet-12∗62.64±0.6178.63±0.4665.99±0.72 81.56±0.53 72.0±0.7 84.2±0.5 41.1±0.6 55.5±0.6Support-based initialization (train) WRN-28-10 56.17±0.64 73.31±0.53 67.45±0.70†82.88±0.53†70.26±0.70 83.82±0.49†36.82±0.51 49.72±0.55Fine-tuning (train) WRN-28-10 57.73±0.6278.17±0.4966.58±0.7085.55±0.4868.72±0.6786.11±0.4738.25±0.5257.19±0.57Transductive ﬁne-tuning (train) WRN-28-1065.73±0.68 78.40±0.52 73.34±0.71 85.50±0.50 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55Activation to Parameter (Qiao et al., 2018)(train + val)WRN-28-10 59.60±0.41 73.74±0.19LEO (Rusu et al., 2018) (train + val) WRN-28-10 61.76±0.08 77.59±0.12 66.33±0.05 81.44±0.09MetaOpt SVM (Lee et al., 2019) (train +val) ResNet-12∗64.09±0.6280.00±0.4565.81±0.74 81.75±0.53 72.8±0.7 85.0±0.5 47.2±0.6 62.5±0.6Support-based initialization (train + val) WRN-28-10 58.47±0.66 75.56±0.52 67.34±0.69†83.32±0.51†72.14±0.69†85.21±0.49†45.08±0.61 60.05±0.60Fine-tuning (train + val) WRN-28-10 59.62±0.6679.93±0.4766.23±0.6886.08±0.4770.07±0.6787.26±0.4543.80±0.58 64.40±0.58Transductive ﬁne-tuning (train + val) WRN-28-1068.11±0.69 80.36±0.50 72.87±0.71 86.15±0.50 78.36±0.70 87.54±0.49 50.44±0.68 65.74±0.60 Table 1 shows the results of transductive ﬁne-tuning on benchmark datasets for standard few-shot protocols. We see that this simple baseline is uniformly better than state-of-the-art algorithms. We include results for support-based initialization, which does no ﬁne-tuning; and for ﬁne-tuning, which involves optimizing only the cross-entropy term in (8) using the labeled support samples. The support-based initialization is sometimes better than or comparable to state-of-the-art algorithms (marked †). The few-shot literature has gravitated towards larger backbones (Rusu et al., 2018). Our results indicate that for large backbones even standard cross-entropy pre-training and support-based initialization work well, similar to observation made by Chen et al. (2018). 6Published as a conference paper at ICLR 2020 For the 1-shot 5-way setting, ﬁne-tuning using only the labeled support examples leads to minor improvement over the initialization, and sometimes marginal degradation. However,for the 5-shot 5-way setting non-transductive ﬁne-tuning is better than the state-of-the-art. In both (train) and (train + val) settings, transductive ﬁne-tuning leads to 2-7% improvement for 1-shot 5-way setting over the state-of-the-art for all datasets. It results in an increase of 1.5-4% for the 5-shot 5-way setting except for the Mini-ImageNet dataset, where the performance is matched. This suggests that the use of the unlabeled query samples is vital for the few-shot setting. For the Mini-ImageNet, CIFAR-FS and FC-100 datasets, using additional data from the valida- tion set to pre-train the backbone results in 2-8% improvements; the improvement is smaller for Tiered-ImageNet. This suggests that having more pre-training classes leads to improved few-shot performance as a consequence of a better embedding. See Appendix C.5 for more experiments. 4.2 L ARGE -SCALE FEW -SHOT LEARNING The ImageNet-21k dataset (Deng et al., 2009) with 14.2M images across 21,814 classes is an ideal large-scale few-shot learning benchmark due to the high class imbalance. The simplicity of our approach allows us to present the ﬁrst few-shot learning results on this large dataset. We use the 7,491 classes having more than 1,000 images each as the meta-training set and the next 13,007 classes with at least 10 images each for constructing few-shot episodes. See Appendix B for details. Table 2: Accuracy (%) on the few-shot data of ImageNet-21k. The conﬁdence intervals are large because we compute statistics only over 80 few-shot episodes so as to test for large number of ways. Way Algorithm Model Shot 5 10 20 40 80 160 Support-based initialization WRN-28-10 1 87.20±1.72 78.71±1.63 69.48±1.30 60.55±1.03 49.15±0.68 40.57±0.42 Transductive ﬁne-tuning WRN-28-10 1 89.00±1.86 79.88±1.70 69.66±1.30 60.72±1.04 48.88±0.66 40.46±0.44 Support-based initialization WRN-28-10 5 95.73±0.84 91.00±1.09 84.77±1.04 78.10±0.79 70.09±0.71 61.93±0.45 Transductive ﬁne-tuning WRN-28-10 5 95.20±0.94 90.61±1.03 84.21±1.09 77.13±0.82 68.94±0.75 60.11±0.48 Table 2 shows the mean accuracy of transductive ﬁne-tuning evaluated over 80 few-shot episodes on ImageNet-21k. The accuracy is extremely high as compared to corresponding results in Table 1 even for large way. E.g., the 1-shot 5-way accuracy on Tiered-ImageNet is 72.87 ±0.71% while it is 89 ± 1.86% here. This corroborates the results in Section 4.1 and indicates that pre-training with a large number of classes may be an effective strategy to build large-scale few-shot learning systems. The improvements of transductive ﬁne-tuning are minor for ImageNet-21k because the support-based initialization accuracies are extremely high. We noticed a slight degradation of accuracies due to transductive ﬁne-tuning at high ways because the entropic term in (8) is much larger than the the cross-entropy loss. The experiments for ImageNet-21k therefore scale down the entropic term by log |Ct|and forego the ReLU in (6) and (7). This reduces the difference in accuracies at high ways. 4.3 A NALYSIS This section presents a comprehensive analysis of transductive ﬁne-tuning on the Mini-ImageNet, Tiered-ImageNet and ImageNet-21k datasets. Robustness of transductive ﬁne-tuning to query shot: Fig. 2a shows the effect of changing the query shot on the mean accuracy. For the 1-shot 5-way setting, the entropic penalty in (8) helps as the query shot increases. This effect is minor in the 5-shot 5-way setting as more labeled data is available. Query shot of 1 achieves a relatively high mean accuracy because transductive ﬁne-tuning can adapt to those few queries. One query shot is enough to beneﬁt from transductive ﬁne-tuning : for Mini-ImageNet, the 1-shot 5-way accuracy with query shot of 1 is 66.94 ±1.55% which is better than non-transductive ﬁne-tuning (59.62 ±0.66% in Table 1) and higher than other approaches. Performance for different way and support shot: A few-shot system should be able to robustly handle different few-shot scenarios. Figs. 2b and 2c, show the performance of transductive ﬁne-tuning 7Published as a conference paper at ICLR 2020 1 5 10 15 20 Query shot 65 75 85 95Mean accuracy (%) 1 shot 5-way Mini-Imagenet 5 shot         \"  1 shot 5-way Tiered-Imagenet 5 shot         \"  (a) 101 102 Way 0 20 40 60 80 100Mean accuracy (%)  1 shot   Tiered-ImageNet  5 shot       \"  10 shot       \"   1 shot   ImageNet-21k  5 shot       \"  10 shot       \" (b) 100 101 Support Shot 20 40 60 80 100Mean accuracy (%)  5 way Tiered-Imagenet 20 way       \" 80 way       \" 160 way       \" (c) Figure 2: Mean accuracy of transductive ﬁne-tuning for different query shot, way and support shot. Fig. 2a shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for Tiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive ﬁne-tuning can specialize to those queries. Fig. 2b shows that the mean accuracy degrades logarithmically with way for ﬁxed support shot and query shot (15). Fig. 2c suggests that the mean accuracy improves logarithmically with the support shot for ﬁxed way and query shot (15). These trends suggest thumb rules for building few-shot systems. with changing way and support shot. The mean accuracy changes logarithmically with the way and support shot which provides thumb rules for building few-shot systems. Different backbone architectures: We include experiments using conv(64)×4 (Vinyals et al., 2016) and ResNet-12 (He et al., 2016a; Oreshkin et al., 2018) in Table 3, in order to facilitate comparisons for different backbone architectures. The results for transductive ﬁne-tuning are comparable or better than state-of-the-art for a given backbone architecture, except for those in Liu et al. (2018b) who use a more sophisticated transductive algorithm using graph propagation, with conv (64)×4. In line with our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results show that transductive ﬁne-tuning is a sound baseline for a variety of backbone architectures. Computational complexity: There is no free lunch and our advocated baseline has its limitations. It performs gradient updates during the ﬁne-tuning phase which makes it slow at inference time. Speciﬁcally, transductive ﬁne-tuning is about 300 ×slower (20.8 vs. 0.07 seconds) for a 1-shot 5-way episode with 15 query shot as compared to Snell et al. (2017) with the same backbone architecture (prototypical networks (Snell et al., 2017) do not update model parameters at inference time). The latency factor reduces with higher support shot. Interestingly, for a single query shot, the former takes 4 seconds vs. 0.07 seconds. This is a more reasonable factor of 50 ×, especially considering that the mean accuracy of the former is 66.2% compared to about 58% of the latter in our implementation. Experiments in Appendix C.3 suggest that using a smaller backbone architecture partially compensates for the latency with some degradation of accuracy. A number of approaches such as Ravi & Larochelle (2016); Finn et al. (2017); Rusu et al. (2018); Lee et al. (2019) also perform additional processing at inference time and are expected to be slow, along with other transductive approaches (Nichol et al., 2018; Liu et al., 2018b). Additionally, support-based initialization has the same inference time as Snell et al. (2017). 4.4 A PROPOSAL FOR REPORTING FEW -SHOT CLASSIFICATION PERFORMANCE As discussed in Section 1, we need better metrics to report the performance of few-shot algorithms. There are two main issues: (i) standard deviation of the few-shot accuracy across different sampled episodes for a given algorithm, dataset and few-shot protocol is very high (cf. Fig. 1), and (ii) different models and hyper-parameters for different few-shot protocols makes evaluating algorithmic contributions difﬁcult (cf. Table 1). This section takes a step towards resolving these issues. Hardness of an episode: Classiﬁcation performance on a few-shot episode is determined by the relative location of the features corresponding to labeled and unlabeled samples. If the unlabeled 8Published as a conference paper at ICLR 2020 features are close to the labeled features from the same class, the classiﬁer can distinguish between the classes easily to obtain a high accuracy. Otherwise, the accuracy would be low. The following deﬁnition characterizes this intuition. For training (support) set Ds and test (query) set Dq, we will deﬁne the hardness Ωϕ as the average log-odds of a test datum being classiﬁed incorrectly. More precisely, Ωϕ(Dq; Ds) = 1 Nq ∑ (x,y)∈Dq log 1 −p(y|x) p(y|x) , (9) where p(·|x) is a softmax distribution with logits zy = wϕ(x). wis the weight matrix constructed using (6) and Ds; and ϕis the ℓ2 normalized logits computed using a rich-enough feature generator, say a deep network trained for standard image classiﬁcation. This is a clustering loss where the labeled support samples form class-speciﬁc cluster centers. The cluster afﬁnities are calculated using cosine-similarities, followed by the softmax operator to get the probability distribution p(·|x). Note that Ωϕ does not depend on the few-shot learner and gives a measure of how difﬁcult the classiﬁcation problem is for any few-shot episode, using a generic feature extractor. 1 2 3 4 5 Hardness 20 40 60 80 100Accuracy (%) CIFAR-FS FC-100 Tiered-Imagenet Mini-Imagenet Imagenet-21k Figure 3: Comparing the accuracy of transductive ﬁne-tuning (solid lines) vs. support-based initialization (dotted lines) for different datasets, ways (5, 10, 20, 40, 80 and 160) and support shots (1 and 5).Abscissae are computed using (9) and a Resnet-152 (He et al., 2016b) network trained for standard image classiﬁcation on the ImageNet-1k dataset. Each marker indicates the accuracy of transductive ﬁne-tuning on a few-shot episode; markers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes different ways; ways increase from left to right (5, 10, 20, 40, 80 and 160). Size of the markers denotes different support shot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot 10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combining the episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing two algorithms for different few-shot scenarios. The areas in the ﬁrst quadrant under the ﬁtted regression lines are 295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet) and 475 vs. 484 (ImageNet-21k) for transductive ﬁne-tuning and support-based initialization. 9Published as a conference paper at ICLR 2020 Fig. 3 demonstrates how to use the hardness metric. Few-shot accuracy degrades linearly with hardness. Performance for all hardness can thus be estimated by testing for two different ways. We advocate selecting hyper-parameters using the area under the ﬁtted curve as a metric instead of tuning them speciﬁcally for each few-shot protocol. The advantage of such a test methodology is that it predicts the performance of the model across multiple few-shot protocols systematically. Different algorithms can be compared directly , e.g., transductive ﬁne-tuning (solid lines) and support-based initialization (dotted lines). For instance, the former leads to large improvements on easy episodes, the performance is similar for hard episodes, especially for Tiered-ImageNet and ImageNet-21k. The high standard deviation of accuracy of few-shot learning algorithms in Fig. 1 can be seen as the spread of the cluster corresponding to each few-shot protocol, e.g., the ellipse in Fig. 3 denotes the 5-shot 10-way protocol for ImageNet-21k. It is the nature of few-shot learning that episodes have varying hardness even if the way and shot are ﬁxed. However, episodes within the ellipse lie on a different line (with a large negative slope) which indicates that given a few-shot protocol, hardness is a good indicator of accuracy. Fig. 3 also shows that due to fewer test classes, CIFAR-FS, FC-100 and Mini-ImageNet have less diversity in the hardness of episodes while Tiered-ImageNet and ImageNet-21k allow sampling of both very hard and very easy diverse episodes. For a given few-shot protocol, the hardness of episodes in the former three is almost the same as that of the latter two datasets. This indicates that CIFAR-FS, FC-100 and Mini-ImageNet may be good benchmarks for applications with few classes. The hardness metric in (9) naturally builds upon existing ideas in deep metric learning (Qi et al., 2018). We propose it as a means to evaluate few-shot learning algorithms uniformly across different few-shot protocols for different datasets; ascertaining its efﬁcacy and comparisons to other metrics will be part of future work. 5 D ISCUSSION Our aim is to provide grounding to the practice of few-shot learning. The current literature is in the spirit of increasingly sophisticated approaches for modest improvements in mean accuracy using an inadequate evaluation methodology. This is why we set out to establish a baseline, namely transductive ﬁne-tuning, and a systematic evaluation methodology, namely the hardness metric. We would like to emphasize that our advocated baseline, namely transductive ﬁne-tuning, is not novel and yet performs better than existing algorithms on all standard benchmarks. This is indeed surprising and indicates that we need to take a step back and re-evaluate the status quo in few-shot learning. We hope to use the results in this paper as guidelines for the development of new algorithms. REFERENCES Kelsey R Allen, Hanul Shin, Evan Shelhamer, and Josh B Tenenbaum. Variadic learning by bayesian nonpara- metric deep embedding. 2018. Jonathan Baxter. Learning internal representations. Flinders University of S. Aust., 1995. Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pp. 6–8. Univ. of Texas, 1992. Luca Bertinetto, Jo ˜ao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. arXiv:1805.08136, 2018. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁcation using a” siamese” time delay neural network. In Advances in neural information processing systems, pp. 737–744, 1994. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. 2018. Sumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric discriminatively, with application to face veriﬁcation. In CVPR (1), pp. 539–546, 2005. 10Published as a conference paper at ICLR 2020 Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised learning that requires a bad gan. In Advances in neural information processing systems, pp. 6510–6520, 2017. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126– 1135. JMLR. org, 2017. Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft nearest neighbor loss. arXiv:1902.01889, 2019. Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv:1711.04043, 2017. Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural information processing systems, pp. 529–536, 2005. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv:1603.05027, 2016b. Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001. Jeremy Howard et al. fastai. https://github.com/fastai/fastai, 2018. Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 325–333, 2015. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015. Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al. Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes. arXiv:1807.11205, 2018. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Icml, volume 99, pp. 200–209, 1999. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv:1609.02907, 2016. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. arXiv:1904.03758, 2019. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learning. 2018a. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot learning. arXiv:1805.10002, 2018b. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:1608.03983, 2016. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579–2605, 2008. Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pp. 2113–2122, 2015. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv:1710.03740, 2017. 11Published as a conference paper at ICLR 2020 Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with virtual adversarial training. arXiv:1507.00677, 2015. Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In Proceedings of the IEEE International Conference on Computer Vision, pp. 360–368, 2017. Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv:1803.02999, 2018. Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, pp. 719–729, 2018. Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5822–5830, 2018. Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7229–7238, 2018. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016. Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class models and shot-free meta training, 2019. Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv:1803.00676, 2018. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv:1807.05960, 2018. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163–1171, 2016. Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987. Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 464–472. IEEE, 2017. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998. Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019. Paul E Utgoff. Shift of bias for inductive concept learning.Machine learning: An artiﬁcial intelligence approach, 2:107–148, 1986. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Junyuan Xie, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. arXiv:1812.01187, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv:1605.07146, 2016. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv:1710.09412, 2017. Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf. Learning with local and global consistency. In Advances in neural information processing systems, pp. 321–328, 2004. 12Published as a conference paper at ICLR 2020 A S ETUP A.1 D ATASETS We use the following datasets for our benchmarking experiments. • The Mini-ImageNet dataset (Vinyals et al., 2016) which is a subset of ImageNet-1k (Deng et al., 2009) and consists of 84 ×84 sized images with 600 images per class. There are 64 training, 16 validation and 20 test classes. There are multiple versions of this dataset in the literature; we obtained the dataset from the authors of Gidaris & Komodakis (2018)3. • The Tiered-ImageNet dataset (Ren et al., 2018) is a larger subset of ImageNet-1k with 608 classes split as 351 training, 97 validation and 160 testing classes, each with about 1300 images of size 84 ×84. This dataset ensures that training, validation and test classes do not have a semantic overlap and is a potentially harder few-shot learning dataset. • We also consider two smaller CIFAR-100 (Krizhevsky & Hinton, 2009) derivatives, both with 32 ×32 sized images and 600 images per class. The ﬁrst is the CIFAR-FS dataset (Bertinetto et al., 2018) which splits classes randomly into 64 training, 16 validation and 20 test. The second is the FC-100 dataset (Oreshkin et al., 2018) which splits CIFAR-100 into 60 training, 20 validation and 20 test classes with minimal semantic overlap. Each dataset has a training, validation and test set. The set of classes for each of these sets are disjoint from each other. For meta-training, we ran two sets of experiments: the ﬁrst, where we only use the training set as the meta-training dataset, denoted by (train); the second, where we use both the training and validation sets as the meta-training dataset, denoted by (train + val). We use the test set to construct few-shot episodes. A.2 P RE-TRAINING We use a wide residual network (Zagoruyko & Komodakis, 2016; Qiao et al., 2018; Rusu et al., 2018) with a widening factor of 10 and a depth of 28 which we denote as WRN-28-10. The smaller networks: conv (64)×4 (Vinyals et al., 2016; Snell et al., 2017), ResNet-12 (He et al., 2016a; Oreshkin et al., 2018; Lee et al., 2019) and WRN-16-4 (Zagoruyko & Komodakis, 2016), are used for analysis in Appendix C. All networks are trained using SGD with a batch-size of 256, Nesterov’s momentum set to 0.9, no dropout, weight decay of 10−4. We use batch-normalization (Ioffe & Szegedy, 2015). We use two-cycles of learning rate annealing (Smith, 2017), these are 40 and 80 epochs each for all datasets except ImageNet-21k, which uses cycles of 8 and 16 epochs each. The learning rate is set to 10−i at the beginning of the ith cycle and decreased to 10−6 by the end of that cycle with a cosine schedule (Loshchilov & Hutter, 2016). We use data parallelism across 8 Nvidia V100 GPUs and half-precision training using techniques from Micikevicius et al. (2017); Howard et al. (2018). We use the following regularization techniques that have been discovered in the non-few-shot, standard image classiﬁcation literature (Xie et al., 2018) for pre-training the backbone. • Mixup (Zhang et al., 2017): This augments data by a linear interpolation between input images and their one-hot labels. If (x1,y1),(x2,y2) ∈D are two samples, mixup creates a new sample (˜x,˜y) where ˜x= λx1 + (1−λ)x2 and its label ˜y= λey1 + (1−λ)ey2 ; here ek is the one-hot vector with a non-zero kth entry and λ∈[0,1] is sampled from Beta(α,α) for a hyper-parameter α. • Label smoothing (Szegedy et al., 2016): When using a softmax operator, the logits can increase or decrease in an unbounded manner causing numerical instabilities while training. Label smoothing sets pθ(k|x) = 1 −ϵ if k = y and ϵ/(K −1) otherwise, for a small constant ϵ> 0 and number of classes K. The ratio between the largest and smallest output neuron is thus ﬁxed which helps large-scale training. • We exclude the batch-normalization parameters from weight-decay (Jia et al., 2018). 3https://github.com/gidariss/FewShotWithoutForgetting 13Published as a conference paper at ICLR 2020 We set ϵ=0.1 for label smoothing cross-entroy loss and α=0.25 for mixup regularization for all our experiments. A.3 F INE -TUNING HYPER -PARAMETERS We used 1-shot 5-way episodes on the validation set of Mini-ImageNet to manually tune hyper- parameters. Fine-tuning is done for 25 epochs with a ﬁxed learning rate of 5 ×10−5 with Adam (Kingma & Ba, 2014). Adam is used here as it is more robust to large changes in the magnitude of the loss and gradients which occurs if the number of classes in the few-shot episode (ways) is large. We do not use any regularization (weight-decay, mixup, dropout, or label smoothing) in the ﬁne-tuning phase. These hyper-parameters are kept constant on all benchmark datasets, namely Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100. All ﬁne-tuning and evaluation is performed on a single GPU in full-precision. We update the parameters sequentially by computing the gradient of the two terms in (8) independently. This updates both the weights of the model and the batch-normalization parameters. A.4 D ATA AUGMENTATION Input images are normalized using the mean and standard-deviation computed on ImageNet-1k. Our Data augmentation consists of left-right ﬂips with probability of 0.5, padding the image with 4px and adding brightness and contrast changes of ±40%. The augmentation is kept the same for both pre-training and ﬁne-tuning. We explored augmentation using afﬁne transforms of the images but found that adding this has minor effect with no particular trend on the numerical results. A.5 E VALUATION PROCEDURE The few-shot episode contains classes that are uniformly sampled from the test classes of correspond- ing datasets. Support and query samples are further uniformly sampled for each class. The query shot is ﬁxed to 15 for all experiments unless noted otherwise. We evaluate all networks over 1,000 episodes unless noted otherwise. For ease of comparison, we report the mean accuracy and the 95% conﬁdence interval of the estimate of the mean accuracy. B S ETUP FOR IMAGE NET-21 K The ImageNet-21k dataset (Deng et al., 2009) has 14.2M images across 21,814 classes. The blue region in Fig. 4 denotes our meta-training set with 7,491 classes, each with more than 1,000 images. The green region shows 13,007 classes with at least 10 images each, the set used to construct few-shot episodes. We do not use the red region consisting of 1,343 classes with less than 10 images each. We train the same backbone (WRN-28-10) with the same procedure as that in Appendix A on 84 × 84 resized images, albeit for only 24 epochs. Since we use the same hyper-parameters as the other benchmark datasets, we did not create validation sets for pre-training or the ﬁne-tuning phases. The few-shot episodes are constructed in the same way as Appendix A. We evaluate using fewer few-shot episodes (80) on this dataset because we would like to demonstrate the performance across a large number of different ways. C A DDITIONAL ANALYSIS This section contains additional experiments and analysis, complementing Section 4.3. All ex- periments use the (train + val) setting, pre-training on both the training and validation sets of the corresponding datasets, unless noted otherwise. 14Published as a conference paper at ICLR 2020 0 5000 10000 15000 20000 classes 100 101 102 103 images per class Figure 4: ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images while the rarest class has a single image. Figure 5: t-SNE (Maaten & Hinton, 2008) embedding of the logits for 1-shot 5-way few-shot episode of Mini-ImageNet. Colors denote the ground-truth labels; crosses denote the support samples; circles denote the query samples; translucent markers and opaque markers denote the embeddings before and after transductive ﬁne-tuning respectively. Even though query samples are far away from their respective supports in the beginning, they move towards the supports by the end of transductive ﬁne-tuning. Logits of support samples are relatively unchanged which suggests that the support-based initialization is effective. C.1 T RANSDUCTIVE FINE -TUNING CHANGES THE EMBEDDING DRAMATICALLY Fig. 5 demonstrates this effect. The logits for query samples are far from those of their respective support samples and metric-based loss functions, e.g., those for prototypical networks (Snell et al., 2017) would have a high loss on this episode; indeed the accuracy after the support-based initialization is 64%. Logits for the query samples change dramatically during transductive ﬁne-tuning and majority of the query samples cluster around their respective supports. The post transductive ﬁne-tuning accuracy of this episode is 73.3%. This suggests that modifying the embedding using the query samples is crucial to obtaining good performance on new classes. This example also demonstrates that the support-based initialization is efﬁcient, logits of the support samples are relatively unchanged during the transductive ﬁne-tuning phase. 15Published as a conference paper at ICLR 2020 C.2 L ARGE VS . SMALL BACKBONES The expressive power of the backbone plays an important role in the efﬁcacy of ﬁne-tuning. We observed that a WRN-16-4 architecture (2.7M parameters) performs worse than WRN-28-10 (36M parameters). The former obtains 63.28 ±0.68% and 77.39 ±0.5% accuracy on Mini-ImageNet and 69.04 ±0.69% and 83.55 ±0.51% accuracy on Tiered-ImageNet on 1-shot 5-way and 5-shot 5-way protocols respectively. While these numbers are comparable to those of state-of-the-art algorithms, they are lower than their counterparts for WRN-28-10 in Table 1. This suggests that a larger network is effective in learning richer features from the meta-training classes, and ﬁne-tuning is effective in taking advantage of this to further improve performance on samples belonging to few-shot classes. C.3 L ATENCY WITH A SMALLER BACKBONES The WRN-16-4 architecture (2.7M parameters) is much smaller than WRN-28-10 (36M parameters) and transductive ﬁne-tuning on the former is much faster. As compared to our implementation of Snell et al. (2017) with the same backbone, WRN-16-4 is 20-70×slower (0.87 vs. 0.04 seconds for a query shot of 1, and 2.85 vs. 0.04 seconds for a query shot of 15) for the 1-shot 5-way scenario. Compare this to the computational complexity experiment in Section 4.3. As discussed in Appendix C.2, the accuracy of WRN-16-4 is 63.28 ±0.68% and 77.39 ±0.5% for 1-shot 5-way and 5-shot 5-way on Mini-ImageNet respectively. As compared to this, our implementation of (Snell et al., 2017) using a WRN-16-4 backbone obtains 57.29 ±0.40% and 75.34 ±0.32% accuracies for the same settings respectively; the former number in particular is signiﬁcantly worse than its transductive ﬁne-tuning counterpart. C.4 C OMPARISONS AGAINST BACKBONES IN THE CURRENT LITERATURE We include experiments using conv (64)×4 and ResNet-12 in Table 3, in addition to WRN-28-10 in Section 4, in order to facilitate comparisons of the proposed baseline for different backbone architectures. Our results are comparable or better than existing results for a given backbone architecture, except for those in Liu et al. (2018b) who use a graph-based transduction algorithm, for conv (64)×4 on Mini-ImageNet. In line with our goal for simplicity, we kept the hyper-parameters for pre-training and ﬁne-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results suggest that transductive ﬁne-tuning is a sound baseline for a variety of backbone architectures. C.5 U SING MORE META -TRAINING CLASSES In Section 4.1 we observed that having more pre-training classes improves few-shot performance. But since we append a classiﬁer on top of a pre-trained backbone and use the logits of the backbone as inputs to the classiﬁer, a backbone pre-trained on more classes would also have more parameters as compared to one pre-trained on fewer classes. However, this difference is not large: WRN-28-10 for Mini-ImageNet has 0.03% more parameters for (train + val) as compared to (train). However, in order to facilitate a fair comparison, we ran an experiment where we use the features of the backbone, instead of the logits, as inputs to the classiﬁer. By doing so, the number of parameters in the pre-trained backbone that are used for few-shot classiﬁcation remain the same for both the (train) and (train + val) settings. For Mini-ImageNet, (train + val) obtains 64.20 ±0.65% and 81.26 ±0.45%, and (train) obtains 62.55 ±0.65% and 78.89 ±0.46%, for 1-shot 5-way and 5-shot 5-way respectively. These results corroborate the original statement that more pre-training classes improves few-shot performance. C.6 U SING FEATURES OF THE BACKBONE AS INPUT TO THE CLASSIFIER Instead of re-initializing the ﬁnal fully-connected layer of the backbone to classify new classes, we simply append the classiﬁer on top of it. We implemented the former, more common, approach and found that it achieves an accuracy of 64.20 ±0.65% and 81.26 ±0.45% for 1-shot 5-way and 5-shot 5-way respectively on Mini-ImageNet, while the accuracy on Tiered-ImageNet is 67.14 ± 16Published as a conference paper at ICLR 2020 Table 3: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64k)×4 denotes a CNN with 4 layers and 64k channels in the kth layer. The rows are grouped by the backbone architectures. Best results in each column and for a given backbone architecture are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by †. The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise. (Lee et al., 2019) uses a 1.25× wider ResNet-12 which we denote as ResNet-12 ∗. Mini-ImageNet Tiered-ImageNet CIFAR-FS FC-100Algorithm Architecture 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%) 1-shot (%) 5-shot (%)MAML (Finn et al., 2017) conv(32)×448.70±1.84 63.11±0.92Matching networks (Vinyals et al., 2016) conv(64)×4 46.6 60LSTM meta-learner (Ravi & Larochelle,2016) conv(64)×443.44±0.77 60.60±0.71Prototypical Networks (Snell et al., 2017) conv(64)×449.42±0.78 68.20±0.66Transductive Propagation (Liu et al.,2018b) conv(64)×455.51±0.86 69.86±0.65 59.91±0.9473.30±0.75Support-based initialization (train) conv(64)×450.69±0.63 66.07±0.53 58.42±0.6973.98±0.58†61.77±0.73 76.40±0.54 36.07±0.5448.72±0.57Fine-tuning (train) conv(64)×449.43±0.62 66.42±0.53 57.45±0.6873.96±0.5659.74±0.72 76.37±0.53 35.46±0.53 49.43±0.57Transductive ﬁne-tuning (train) conv(64)×450.46±0.62 66.68±0.52 58.05±0.6874.24±0.56 61.73±0.72 76.92±0.52 36.62±0.55 50.24±0.58R2D2 (Bertinetto et al., 2018) conv(96k)×451.8±0.2 68.4±0.2 65.4 ±0.2 79.4±0.2TADAM (Oreshkin et al., 2018) ResNet-12 58.5±0.376.7±0.3 40.1±0.456.1±0.4Transductive Propagation (Liu et al.,2018b) ResNet-12 59.46 75.64Support-based initialization (train) ResNet-12 54.21±0.64 70.58±0.54 66.39±0.73 81.93±0.54 65.69±0.72 79.95±0.51 35.51±0.53 48.26±0.54Fine-tuning (train) ResNet-12 56.67±0.62 74.80±0.51 64.45±0.7083.59±0.5164.66±0.7382.13±0.5037.52±0.53 55.39±0.57Transductive ﬁne-tuning (train) ResNet-1262.35±0.6674.53±0.5468.41±0.73 83.41±0.52 70.76±0.7481.56±0.5341.89±0.5954.96±0.55MetaOpt SVM (Lee et al., 2019) ResNet-12∗62.64±0.61 78.63±0.46 65.99±0.72 81.56±0.53 72.0±0.7 84.2±0.5 41.1±0.6 55.5±0.6Support-based initialization (train) WRN-28-10 56.17±0.64 73.31±0.53 67.45±0.70 82.88±0.53 70.26±0.70 83.82±0.49 36.82±0.51 49.72±0.55Fine-tuning (train) WRN-28-10 57.73±0.6278.17±0.4966.58±0.7085.55±0.4868.72±0.6786.11±0.4738.25±0.5257.19±0.57Transductive ﬁne-tuning (train) WRN-28-1065.73±0.68 78.40±0.52 73.34±0.71 85.50±0.50 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55 Support-based initialization (train + val) conv(64)×452.77±0.64 68.29±0.54 59.08±0.70 74.62±0.57 64.01±0.71 78.46±0.53 40.25±0.5654.53±0.57Fine-tuning (train + val) conv(64)×451.40±0.6168.58±0.5258.04±0.6874.48±0.5662.12±0.7177.98±0.5239.09±0.55 54.83±0.55Transductive ﬁne-tuning (train + val) conv(64)×452.30±0.61 68.78±0.53 58.81±0.69 74.71±0.56 63.89±0.71 78.48±0.52 40.33±0.56 55.60±0.56Support-based initialization (train + val) ResNet-12 56.79±0.65 72.94±0.55 67.60±0.71 83.09±0.53 69.39±0.71 83.27±0.50 43.11±0.58 58.16±0.57Fine-tuning (train + val) ResNet-12 58.64±0.6476.83±0.5065.55±0.7084.51±0.5068.11±0.7085.19±0.4842.84±0.5763.10±0.57Transductive ﬁne-tuning (train + val) ResNet-1264.50±0.68 76.92±0.55 69.48±0.73 84.37±0.51 74.35±0.7184.57±0.5348.29±0.63 63.38±0.58MetaOpt SVM (Lee et al., 2019) (train +val) ResNet-12∗64.09±0.62 80.00±0.45 65.81±0.74 81.75±0.53 72.8±0.7 85.0±0.5 47.2±0.6 62.5±0.6 Activation to Parameter (Qiao et al., 2018)(train + val)WRN-28-10 59.60±0.41 73.74±0.19LEO (Rusu et al., 2018) (train + val) WRN-28-10 61.76±0.08 77.59±0.12 66.33±0.05 81.44±0.09Support-based initialization (train + val) WRN-28-10 58.47±0.66 75.56±0.52 67.34±0.69†83.32±0.51†72.14±0.69 85.21±0.49 45.08±0.61 60.05±0.60Fine-tuning (train + val) WRN-28-10 59.62±0.6679.93±0.4766.23±0.6886.08±0.4770.07±0.6787.26±0.4543.80±0.58 64.40±0.58Transductive ﬁne-tuning (train + val) WRN-28-1068.11±0.69 80.36±0.50 72.87±0.71 86.15±0.50 78.36±0.70 87.54±0.49 50.44±0.68 65.74±0.60 0.74% and 86.67 ±0.46% for 1-shot 5-way and 5-shot 5-way respectively. These numbers are signiﬁcantly lower for the 1-shot 5-way protocol on both datasets compared to their counterparts in Table 1. However, the 5-shot 5-way accuracy is marginally higher in this experiment than that in Table 1. As noted in Remark 2, logits of the backbone are well-clustered and that is why they work better for few-shot scenarios. 17Published as a conference paper at ICLR 2020 C.7 F REEZING THE BACKBONE RESTRICTS PERFORMANCE The previous observation suggests that the network changes a lot in the ﬁne-tuning phase. Freezing the backbone severely restricts the changes in the network to only changes to the classiﬁer. As a consequence, the accuracy of freezing the backbone is 58.38 ±0.66 % and 75.46 ±0.52% on Mini-ImageNet and 67.06 ±0.69% and 83.20 ±0.51% on Tiered-ImageNet for 1-shot 5-way and 5-shot 5-way respectively. While the 1-shot 5-way accuracies are much lower than their counterparts in Table 1, the gap in the 5-shot 5-way scenario is smaller. C.8 U SING MIXUP DURING PRE -TRAINING Mixup improves the few-shot accuracy by about 1%; the accuracy for WRN-28-10 trained without mixup is 67.06 ±0.71% and 79.29 ±0.51% on Mini-ImageNet for 1-shot 5-way and 5-shot 5-way respectively. C.9 M ORE FEW -SHOT EPISODES Fig. 1 suggests that the standard deviation of the accuracies achieved by few-shot algorithms is high. Considering this randomness, evaluations were done over 10,000 few-shot episodes as well. The accuracies on Mini-ImageNet are 67.77 ±0.21 % and 80.24 ±0.16 % and on Tiered-ImageNet are 72.36 ±0.23 % and 85.70 ±0.16 % for 1-shot 5-way and 5-shot 5-way respectively. The numbers are consistent with the ones for 1,000 few-shot episodes in Table 1, though the conﬁdence intervals decreased as the number of episodes sampled increased. C.10 E VALUATION ON META-DATASET Table 4: Few-shot accuracies on Meta-Dataset. Best results in each row are shown in bold. 600 few-shot episodes were used to compare to the results reported in Triantaﬁllou et al. (2019). Results where the support- based initialization is better than or comparable to existing algorithms are denoted by †. Best in Triantaﬁllou et al. (2019) Support-based initialization Fine-tuning Transductive ﬁne-tuningILSVRC 50.50 ±1.08 60.05±1.03† 60.42±1.03 60.53±1.03Omniglot 63.37 ±1.33 76.32±0.96† 80.73±0.9582.07±0.93Aircraft 68.69 ±1.26 61.99±0.87 70.08±0.9772.40±0.97Birds 68.79 ±1.01 80.16±0.84† 81.46±0.84 82.05±0.85Textures 69.05 ±0.90 74.31±0.67† 79.92±0.81 80.47±0.79Quick Draw 51.52±1.00 58.75±0.92† 56.10±1.04 57.36±1.04Fungi 39.96 ±1.14 48.29±1.10† 48.43±1.03 47.72±1.02VGG Flowers 87.15±0.69 91.23±0.57† 91.78±0.55 92.01±0.55Trafﬁc Signs 66.79±1.31 53.44±1.14 60.79±1.27 64.37±1.27MSCOCO 43.74±1.12 43.84±1.00† 43.77±1.03 42.86±1.03 We ran experiments on Meta-Dataset (Triantaﬁllou et al., 2019), and compared the performance of support-based initialization, ﬁne-tuning and transductive ﬁne-tuning to the best results in Triantaﬁllou et al. (2019) for meta-training done on ImageNet-1k (ILSVRC) in Table 4. We observe that support- based initialization is better than or comparable to state-of-the-art on 8 out of 10 tasks. Additionally, transductive ﬁne-tuning is better, most times signiﬁcantly, than state-of-the-art on 8 out of 10 tasks; it trails the state-of-the-art closely on the remaining tasks. The few-shot episode sampling was done the same way as described in Triantaﬁllou et al. (2019); except for the few-shot class sampling for ImageNet-1k (ILSVRC) and Omniglot, which was done uniformly over all few-shot classes (Triantaﬁllou et al. (2019) use a hierarchical sampling technique to sample classes that are far from each other in the hierarchy, and hence easier to distinguish between). The hyper-parameters used for meta-training and few-shot ﬁne-tuning are kept the same as the ones in Section 4 and are not tuned for these experiments. Images of size 84 ×84 are used. Similar to the ImageNet-21k experiments, these experiments scale down the entropic term in (8) by log |Ct|and forego the ReLU in (6) and (7). 18Published as a conference paper at ICLR 2020 D F REQUENTLY ASKED QUESTIONS 1. Why has it not been noticed yet that this simple approach works so well? Non-transductive ﬁne-tuning as a baseline has been considered before (Vinyals et al., 2016; Chen et al., 2018). The fact that this is comparable to state-of-the-art has probably gone unnoticed because of the following reasons: • Given that there are only a few labeled support samples provided in the few-shot setting, initializing the classiﬁer becomes important. The support-based initialization (cf. Section 3.1) motivated from the deep metric learning literature (Hu et al., 2015; Movshovitz-Attias et al., 2017; Qi et al., 2018; Gidaris & Komodakis, 2018) classiﬁes support samples correctly (for a support shot of 1, this may not be true for higher support shots). This initialization, as opposed to initializing the weights of the classiﬁer randomly, was critical to performance in our experiments. • In our experience, existing meta-training methods, both gradient-based ones and metric- based ones, are difﬁcult to tune for larger architectures. We speculate that this is the reason a large part of the existing literature focuses on smaller backbone architectures. The few-shot learning literature has only recently started to move towards bigger backbone architectures (Oreshkin et al., 2018; Rusu et al., 2018). From Table 3 we see that non-tranductive ﬁne- tuning gets better with a deeper backbone architecture. A similar observation was made by (Chen et al., 2018). The observation that we can use “simple” well-understood training techniques from standard supervised learning that scale up to large backbone architectures for few-shot classiﬁcation is a key contribution of our paper. Transductive methods have recently started to become popular in the few-shot learning literature (Nichol et al., 2018; Liu et al., 2018a). Because of the scarcity of labeled support samples, it is crucial to make use of the unlabeled query samples in the few-shot regime. Our advocated baseline makes use of both a good initialization and transduction, relatively new in the few-shot learning literature, which makes this simplistic approach go unrecognized till now. 2. Transductive ﬁne-tuning works better than existing algorithms because of a big backbone architecture. One should compare on the same backbone architectures as the existing algo- rithms for a fair comparison. The current literature is in the spirit of increasingly sophisticated approaches for modest perfor- mance gains, often with different architectures (cf. Table 1). This is why we set out to establish a baseline. Our simple baseline is comparable or better than existing approaches. The backbone we have used is common in the recent few-shot learning literature (Rusu et al., 2018; Qiao et al., 2018) (cf. Table 1). Additionally, we have included results on smaller common backbone architec- tures, namely conv (64)×4 and ResNet-12 in Appendix C.4, and some additional experiments in Appendix C.2. These experiments suggest that transductive ﬁne-tuning is a sound baseline for a variety of different backbone architectures. This indicates that we should take results on existing benchmarks with a grain of salt. Also see the response to question 1 above. 3. There are missing entries in Tables 1 and 3. Is it still a fair comparison? Tables 1 and 3 show all relevant published results by the original authors. Re-implementing existing algorithms to ﬁll missing entries without access to original code is impractical and often yields results inferior to those published, which may be judged as unfair. The purpose of a benchmark is to enable others to test their method easily. This does not exist today due to myriad performance-critical design choices often not detailed in the papers. In fact, missing entries in the table indicate the inadequate state of the current literature. Our work enables benchmarking relative to a simple, systematic baseline. 4. Fine-tuning for few-shot learning is not novel. We do not claim novelty in this paper. Transductive ﬁne-tuning is our advocated baseline for few-shot classiﬁcation. It is a combination of different techniques that are not novel. Yet, it performs better than existing algorithms on all few-shot protocols with ﬁxed hyper-parameters. We emphasize that this indicates the need to re-interpret existing results on benchmarks and re-evaluate the status quo in the literature. 19Published as a conference paper at ICLR 2020 5. Transductive ﬁne-tuning has a very high latency at inference time, this is not practical. Our goal is to establish a systematic baseline for accuracy, which might help judge the accuracy of few-shot learning algorithms in the future. The question of test-time latency is indeed important but we have not focused on it in this paper. Appendix C.3 provides results using a smaller backbone where we see that the WRN-16-4 network is about 20-70x slower than metric-based approaches employing the same backbone while having signiﬁcantly better accuracy. The latencies with WRN-28-10 are larger (see the computational complexity section in Section 4.3) but with a bigger advantage in terms of accuracy. There are other transductive methods used for few-shot classiﬁcation (Nichol et al., 2018; Liu et al., 2018a), that are expected to be slow as well. 6. Transductive ﬁne-tuning does not make sense in the online setting when query samples are shown in a sequence. Transductive ﬁne-tuning can be performed even with a single test datum. Indeed, the network can specialize itself completely to classify this one datum. We explore a similar scenario in Section 4.3 and Fig. 2a, which discuss the performance of transductive ﬁne-tuning with a query shot of 1 (this means 5 query samples one from each class for 5-way evaluation). Note that the loss function in (8) leverages multiple query samples when available. It does not require that the query samples be balanced in terms of their ground-truth classes. In particular, the loss function in (8) is well-deﬁned even for a single test datum. For concerns about latency, see the question 5 above. 7. Having transductive approaches will incentivize hacking the query set. There are already published methods that use transductive methods (Nichol et al., 2018; Liu et al., 2018a), and it is a fundamental property of the transductive paradigm to be dependent on the query set, in addition to the support set. In order to prevent query set hacking, we will make the test episodes public which will enable consistent benchmarking, even for transductive methods. 8. Why is having the same hyper-parameters for different few-shot protocols so important? A practical few-shot learning algorithm should be able to handle any few-shot protocol. Having one model for each different scenario is unreasonable in the real-world, as the number of different scenarios is, in principle, inﬁnite. Current algorithms do not handle this well. A single model which can handle any few-shot scenario is thus desirable. 9. Is this over-ﬁtting to the test datum? No, label of the test datum is not used in the loss function. 10. Can you give some intuition about the hardness metric? How did you come up with the formula? The hardness metric is the clustering loss where the labeled support samples form the centers of the class-speciﬁc clusters. The special form, namely, E(x,y)∈Dq log 1−p(y|x) p(y|x) (cf. (9)) allows an interpretation of log-odds. We used this form because it is sensitive to the number of few-shot classes (cf. Fig. 3). Similar metrics, e.g., E(x,y)∈Dq [−log p(y|x)] can also be used but they come with a few caveats. Note that it is easier for p(y|x) to be large for small way because the normalization constant in softmax has fewer terms. For large way, p(y|x) could be smaller. This effect is better captured by our metric. 11. How does Fig. 3 look for algorithm X, Y, Z? We compared two algorithms in Fig. 3, namely transductive ﬁne-tuning and support-based initial- ization. Section 4.4 and the caption of Fig. 3 explains how the former algorithm is better. We will consider adding comparisons to other algorithms to this plot in the future. 20",
      "references": [
        "Variadic learning by bayesian nonpara- metric deep embedding.",
        "Learning internal representations.",
        "On the optimization of a synaptic learning rule.",
        "Meta-learning with differentiable closed-form solvers.",
        "Signature veriﬁcation using a” siamese” time delay neural network.",
        "A closer look at few-shot classiﬁcation.",
        "Learning a similarity metric discriminatively, with application to face veriﬁcation.",
        "Good semi-supervised learning that requires a bad gan.",
        "Imagenet: A large-scale hierarchical image database.",
        "Model-agnostic meta-learning for fast adaptation of deep networks.",
        "Analyzing and improving representations with the soft nearest neighbor loss.",
        "Few-shot learning with graph neural networks.",
        "Dynamic few-shot visual learning without forgetting.",
        "Semi-supervised learning by entropy minimization.",
        "Deep residual learning for image recognition.",
        "Identity mappings in deep residual networks.",
        "Learning to learn using gradient descent.",
        "fastai.",
        "Deep transfer metric learning.",
        "Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
        "Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes.",
        "Transductive inference for text classiﬁcation using support vector machines.",
        "Adam: A method for stochastic optimization.",
        "Semi-supervised classiﬁcation with graph convolutional networks.",
        "Learning multiple layers of features from tiny images.",
        "Meta-learning with differentiable convex optimization.",
        "Learning to propagate labels: Transductive propagation network for few-shot learning.",
        "Transductive propagation network for few-shot learning.",
        "Sgdr: Stochastic gradient descent with warm restarts.",
        "Visualizing data using t-SNE.",
        "Gradient-based hyperparameter optimization through reversible learning.",
        "Mixed precision training.",
        "Distributional smoothing with virtual adversarial training.",
        "No fuss distance metric learning using proxies.",
        "On first-order meta-learning algorithms.",
        "Tadam: Task dependent adaptive metric for improved few-shot learning.",
        "Low-shot learning with imprinted weights.",
        "Few-shot image recognition by predicting parameters from activations.",
        "Optimization as a model for few-shot learning.",
        "Few-shot learning with embedded class models and shot-free meta training,",
        "Meta-learning for semi-supervised few-shot classiﬁcation.",
        "Meta-learning with latent embedding optimization.",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning.",
        "Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.)",
        "Cyclical learning rates for training neural networks.",
        "Prototypical networks for few-shot learning.",
        "Rethinking the inception architecture for computer vision.",
        "Lifelong learning algorithms.",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples.",
        "Shift of bias for inductive concept learning.",
        "The nature of statistical learning theory.",
        "Matching networks for one shot learning.",
        "Bag of tricks for image classiﬁcation with convolutional neural networks.",
        "Wide residual networks.",
        "mixup: Beyond empirical risk minimization.",
        "Learning with local and global consistency."
      ],
      "meta_data": {
        "arxiv_id": "1909.02729v5",
        "authors": [
          "Guneet S. Dhillon",
          "Pratik Chaudhari",
          "Avinash Ravichandran",
          "Stefano Soatto"
        ],
        "published_date": "2019-09-06T06:14:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces an extremely simple yet strong baseline for few-shot image classification consisting of (1) standard cross-entropy pre-training of a deep backbone, (2) support-based weight imprinting to initialize a new classifier for novel classes, and (3) transductive fine-tuning that jointly minimizes cross-entropy on the labeled support set and an entropy penalty on unlabeled query samples. With a single set of fixed hyper-parameters this baseline surpasses state-of-the-art meta-learning methods on Mini-ImageNet, Tiered-ImageNet, CIFAR-FS, FC-100, and delivers the first few-shot results on the large-scale ImageNet-21k. The paper also proposes a dataset-agnostic hardness metric for few-shot episodes and shows how variance in reported accuracies can be explained by episode hardness.",
        "methodology": "1. Pre-training: Wide ResNet-28-10 (and other standard CNNs) trained with cross-entropy, label smoothing, mixup, SGD with cyclic learning rates. 2. Support-based initialization: For each novel class, average ℓ2-normalized ReLU-activated backbone logits of its support images and copy them as the classifier weights (weight imprinting); biases set to zero. 3. Transductive fine-tuning: Optimize all network parameters at test time with Adam for 25 epochs on each episode, minimizing cross-entropy over support samples plus Shannon-entropy regularization over query samples (coefficient 1). 4. Hardness metric: Average log-odds of misclassification computed with a fixed ImageNet feature extractor to quantify episode difficulty.",
        "experimental_setup": "Benchmarks: Mini-ImageNet (64/16/20 split), Tiered-ImageNet (351/97/160), CIFAR-FS (64/16/20), FC-100 (60/20/20), and ImageNet-21k (7,491 meta-train, 13,007 novel classes). Additional evaluation on Meta-Dataset. Episodes: mostly 5-way, 1- or 5-shot, 15 query images per class, 1,000 episodes for benchmarks and 80 for ImageNet-21k. Models trained on 8 V100 GPUs with half-precision; fine-tuning runs on a single GPU. Metrics: mean accuracy and 95% confidence interval across episodes; hardness metric used for further analysis. Baselines compared include Prototypical Nets, MAML, MetaOpt-SVM, LEO, TADAM, Transductive Propagation, etc.",
        "limitations": "1. High inference latency: fine-tuning for dozens of epochs per episode is 20–300× slower than non-adaptive metric methods. 2. Requires access to the entire unlabeled query set (transductive assumption), less applicable to streaming or single-example settings. 3. Performance gains diminish with smaller backbones; method benefits mainly from large, well-pre-trained networks. 4. Provides limited algorithmic novelty; improvements stem from pragmatic choices rather than new learning principles. 5. Hyper-parameters tuned on Mini-ImageNet; although fixed later, may still bias results. 6. Entropy regularization coefficient and temperature may need dataset-specific adjustment for very high way settings.",
        "future_research_directions": "1. Develop faster or one-step adaptation procedures to cut inference latency while retaining accuracy. 2. Extend the approach to online or incremental scenarios where query examples arrive sequentially. 3. Combine weight-imprinting fine-tuning with advanced semi-supervised or graph-based transductive techniques to exploit unlabeled data more effectively. 4. Investigate alternative, theoretically grounded hardness metrics and use them for curriculum design or adaptive evaluation. 5. Explore cross-domain and cross-modal few-shot tasks, leveraging large diverse meta-training corpora beyond ImageNet. 6. Study mechanisms to reduce dependency on large backbones, e.g., through parameter-efficient adapters or knowledge distillation.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
      "full_text": "Frozen Feature Augmentation for Few-Shot Image Classification Andreas B¨ar1 2 * Neil Houlsby1 Mostafa Dehghani1 Manoj Kumar1 † 1Google DeepMind 2Technische Universit¨at Braunschweig andreas.baer@tu-braunschweig.de {neilhoulsby, dehghani, mechcoder}@google Abstract Vision foundation models are currently one of the main driving forces in computer vision research. Simply training a linear classifier or a lightweight model on top of model outputs or so-called ‘frozen features’ leads to impressive performance on a number of tasks. Currently, frozen fea- tures are not modified during training of such lightweight models. On the other hand, when networks are trained di- rectly on images, data augmentation is a standard recipe that improves performance with no additional overhead. In this paper, we conduct an extensive pilot study that ex- plores applying data augmentations in the frozen feature space for few-shot image classification. We dub this type of augmentation ‘frozen feature augmentation (FroFA)’. Our study demonstrates that adopting deceptively simple point- wise FroFAs, such as brightness, can improve few-shot per- formance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction A prevalent trend now is to pretrain vision models on large datasets and adapt them downstream [5, 41, 56]. Notable, even training a simple linear layer or a light-weight model on top of vision transformer (ViT) outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream tasks [13, 19, 43]. However, there is still an interest in training ViTs to achieve good performance on ImageNet-sized [36, 52] or smaller [31, 34] datasets. In this setting, a crucial ingre- dient is data augmentation — a predefined set of simple, stochastic input transformations. Simple but effective ex- amples for image augmentations include random cropping which extracts a fixed-sized region from an image of ar- bitrary resolution, or pixel-wise modifications that change brightness, saturation, or contrast. These are complemented by more advanced augmentation strategies such as mixup [58] or RandAugment [10]. *Work conducted as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0.0 2.5 5.0 top-1 acc. (abs. gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Few-shot results averaged across eight test sets, in- cluding ILSVRC-2012 [14, 44]. We use cached features from an L/16 model [16] pretrained on JFT-3B [56] (left) or WebLI [5] following a sigmoid language-image pretraining (SigLIP) [57] (right). Our method, i.e., a multi-head attention pooling [30] head trained with weight decay (MAPwd) and frozen feature augmenta- tion (FroFA), shows significant gains across all shots with respect to a weight-decayed MAP, i.e., MAPwd, or an L2-regularized lin- ear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques in a data-constrained, few-shot frozen feature setting. In particular, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmenta- tions on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eighteen augmentations applied to frozen features extracted from vision transformers pretrained on JFT-3B [56], ImageNet-21k [14, 44], or WebLI [5]. We train a small lightweight multi-head attention pooling (MAP) [30, 56] head using these augmented inputs and evaluate its performance across eight downstream image classification datasets, where we on average achieve signif- icant gains (see Fig. 1). Our major insights are as follows: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ImageNet. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady im- 1 arXiv:2403.10519v2  [cs.CV]  26 Jul 2024provements on 1-, 5-, and 10-shot settings. 2. Unlike traditional image augmentations that apply a sin- gle randomly sampled value across the entire image, we introduce per-channel stochasticity by sampling inde- pendent random values for each channel. For example, on the 5-shot setting, we improve accuracy over a well- tuned MAP and linear probe baseline by 0.5% absolute and 0.8% absolute, respectively. 3. While FroFA provides modest but significant improve- ments on ImageNet, it excels on smaller transfer datasets. Across seven downstream datasets, FroFA out- performs the mean accuracy of the MAP baseline in the 5 shot setting by 3.2% absolute and the linear probe base- line by 4.2% absolute. 2. Related Works Transfer learning on few-shot data : State-of-the-art vi- sion models [5, 13, 16, 56] are typically pretrained on large-scale datasets, e.g., ImageNet-21k [14, 44] or ver- sions of JFT [21, 56], before transferred to other middle- scale to small-scale ones,e.g., CIFAR10 [1], ILSVRC-2012 [14, 44], or SUN397 [53, 54]. Depending on the model size, efficient transfer learning becomes a challenge. Many meth- ods have been proposed for large language models (LLMs), e.g., adapters [22], low-rank adaptation (LoRA) [23], or prompt tuning [32], of which some have been successfully adapted to computer vision [4, 17, 24, 59]. CLIP-Adapter [17] builds on the power of contrastive language-image pre- training (CLIP) [43] and combines it with adapters [22]. A follow-up work [59] proposes TiP-Adapter which uses a query-key cache model [18, 42] instead of a gradient de- scent approach. Inspired by the success of prompt tuning in LLMs [32], Jia et al. propose visual prompt tuning at the model input [24]. On the other hand, AdaptFormer [4] uses additional intermediate trainable layers to finetune a frozen vision transformer [16]. In contrast, we do not introduce additional prompts [24] or intermediate parameters [4, 17] that require backprop- agating through the network. Instead, we train a small network on top of frozen features coming from a vision transformer. This aligns with linear probing [43] which is typically used to transfer vision models to other tasks [13, 19, 56] — our objective. In addition, we focus our experiments around transfer learning on few-shot data [29, 51]. Although not surprising, few-shot results obtained by Dehghani et al . [13] clearly show significant gaps between linear probing and full fine- tuning. We take these results as an incentive to improve upon linear probing. Data augmentation: One go-to method to improve per- formance while training in a low-data regime is data aug- mentation [46]. Some prominent candidates in computer vision are AutoAugment [9], AugMix [20], RandAugment [9], and TrivialAugment [39]. These methods typically combine low-level image augmentations together to aug- ment the input. Although some works propose augmen- tations in feature space [15, 28, 33, 37, 50], a large-scale empirical study on frozen features of single-modal vision models does not exist. To this end, we investigate frozen feature augmentation (FroFA) by reformulating eighteen image augmentations. In particular, we consider a subset used in AutoAugment [9], inception crop [48], mixup [50, 58], and the recently introduced patch dropout [35]. 3. Framework Overview In this section, we give an overview of our framework. 3.1. Notation Let x ∈ IH×W×3 be an RGB image of height H, width W, and I = [0, 1]. A classification model processes x and outputs class scores y ∈ [0, 1]S for each class in a pre- defined set of classes S, with S = |S|. Let L and D be the number of intermediate layers and the number of fea- tures of a multi-layer classification model, respectively. We describe the intermediate feature representations of x as f = f(ℓ) = (f(ℓ) d ) ∈ RD, with layer index ℓ ∈ {1, ..., L} and feature index d ∈ {1, ..., D}. In the vision trans- former [26] architecture, f = f(ℓ) = (f(ℓ) n,c) ∈ RN×C is a two-dimensional entity, where N and C are the number of patches and number of per-patch channels, respectively. In addition, we introduce the patch index n ∈ {1, ..., N} and the per-patch channel index c ∈ {1, ..., C}. 3.2. Training on Cached Features We investigate pretrained vision transformers [26] with L transformer blocks (TBs) followed by a multi-head atten- tion pooling (MAP) [30] and a classification layer (CL). Fig. 2a presents a simplified illustration. For simplicity, we neglect all operations before the first transformer block (e.g., patchifying, positional embedding, etc.). To cache intermediate feature representations, we pro- cess each image x from an image dataset Dx through the network up until transformer blockL. Next, we store the re- sulting features f. After processing Dx we obtain a (frozen) feature dataset Df , with f ∈ Df (Fig. 2b). Finally, we train a lightweight model using the cached (frozen) features. Fig. 2c shows an example where a single MAP layer followed by a classification layer is trained using the feature dataset Df . Since our focus is fast training, we defer a detailed analysis on larger models to future work. 3.3. Frozen Feature Augmentation (FroFA) Data augmentation is a common tool to improve general- ization and is typically applied on the input, or in our case: 2(Frozen) Pretrained Model TB TB TB MAP CL (a) Step 1: Select a (frozen) pretrained model and a layer for caching. (Frozen) Pretrained Model image dataset (frozen) feature dataset TB TB TB (b) Step 2: Process an image dataset and cache the (frozen) features. Lightweight Model (frozen) feature dataset MAP CL frozen feature augmentation (FroFA)  (c) Step 3: Train on (augmented) frozen features. Figure 2. Pipeline for caching and training on (frozen) fea- tures. (2a): Given a (frozen) pretrained vision transformer, withL Transformer blocks (TBs), a multi-head attention pooling (MAP) layer, and a classification layer (CL), we select its L-th Trans- former block for caching. (2b): Next, we feed images x ∈ Dx to cache (frozen) features f ∈ Df . (2c): Finally, we use Df to train a lightweight model on top. We investigate frozen feature augmentation (FroFA) af ∈ Af in this scenario. images. A natural question arises: How to map such image augmentations to intermediate feature representations? Recall that the feature representation f = (fn,c) ∈ RN×C (layer index ℓ omitted) is two-dimensional. We first reshape it to a three-dimensional representation, i.e., f∗ = (f∗ n1,n2,c) ∈ R √ N× √ N×C. (1) We further define f∗ c = f∗ :,:,c ∈ R √ N× √ N×1 (2) as a two-dimensional representation of the c-th channel. Images and feature representations differ in two funda- mental aspects: channel dimensionality and value range. Before adapting image augmentations to the feature space, it is crucial to handle these differences. Channel dimensionality: RGB images have just three channels while intermediate representations possess an ar- bitrary number of channels. To address this, we ignore im- age augmentations that rely on three color channels, e.g., color jitter, and consider augmentations which can have an arbitrary number of channels instead, denoted asCa, cover- ing a majority of commonly applied image augmentations. Value range: RGB values lie within a specific range I, e.g., I = [0, 1] or I = {0, ...,255} ⊂N0, while in theory features have no such constraints. Assuming H = √ N and W = √ N, we define an image augmentation as ax : I √ N× √ N×Ca → I √ N× √ N×Ca , ax ∈ Ax, (3) where Ax is the set of image augmentations andCa = C is an arbitrary number of channels. To also address the value range mismatch, we introduce a deterministic feature-to- image mapping tf→x : R √ N× √ N×Ct → I √ N× √ N×Ct (4) that maps each element of f∗ (1) from R to I. In our exper- iments, we use xf = tf→x(f∗) = f∗ − fmin fmax − fmin , (5) where fmin and fmax are the minimum and maximum value of f∗, respectively, with elements of xf now in I = [0, 1]. We further define an image-to-feature mapping tf←x : I √ N× √ N×Ct → R √ N× √ N×Ct (6) that maps xf back to the original feature value range, with Ct = C by default. In this case, we simply invert (4) and use f∗ = tf←x(xf ) =xf · (fmax − fmin) +fmin. (7) Combining (3), (4), and (6), we obtain a generic (frozen) feature augmentation (FroFA) as a function composition af = tf←x ◦ ax ◦ tf→x. (8) We use three variations of af : 1. (Default) FroFA: We applyaf (8) once across the entire feature representation. We set Ca = Ct = C and com- pute fmin and fmax in (5), (7) across all elements of f∗. Further, as normally done in pixel space,ax (3) samples a random augmentation value and changes all elements of xf using the same value. For example, employing random contrast in a FroFA fashion scales each element of xf by the exact same randomly sampled factor. 2. Channel FroFA (cFroFA) : For each channel in the mapped features xf (5), ax (3) samples a random aug- mentation value per channel and applies that value to all elements in that channel. By using cFroFA for our ran- dom contrast example, we obtain C independently sam- pled scaling factors, one for each channel. 3. Channel2 FroFA (c2FroFA): In addition to applying augmentations per channel as done in cFroFA,tf→x (4) and tx←f (6) also operate per channel. In this case,fmin and fmax are the per-channel maximum and minimum, respectively. In contrast, FroFA and cFroFA use the maximum and minimum across the entire feature. We 3denote this variant as c 2FroFA since both the mappings (4), (6) and the augmentation (3) are applied on a per- channel basis. Although not adding additional stochas- ticity, we found that for random brightness this variant gives more stable results across a range of augmentation hyper parameters. While an element-wise FroFA might seem like a natural next step, our initial experiments lead to significantly worse results. We hypothesize that per-element augmentations might lead to substantial changes in the feature appearance. 4. Experimental Setup In this section, we introduce our experimental setup. 4.1. Network Architectures We employ the following pretrained vision transformers from prior work: Ti/16 [49], B/16 [16], and L/16 [16]. Fur- ther, we follow [56] and employ a lightweight multi-head attention pooling (MAP) layer [30] before the final classifi- cation layer on top of the frozen features (cf . Sec. 3.3). 4.2. Datasets Pretraining: We consider three datasets: JFT-3B, ImageNet-21k, and WebLI. First introduced by Hinton et al. [21], JFT is now a widely used proprietary, large-scale dataset [5, 7, 11, 16, 26, 27, 47, 56]. For our investigations we use the JFT-3B version following Zhai et al . [56]. It consists of nearly 3 billion multi-labeled images following a class-hierarchy of 29,593 labels. We further use ImageNet- 21k [14, 44] which consists of 14,197,122 (multi)-labeled images and 21,841 distinct labels. We equally split the first 51,200 images into a validation and test set and use the remaining 14,145,922 images for training. As a third dataset, we use WebLI [5] which is a recently introduced web-scale multilingual image-text dataset. Please refer to the Appendix, Sec. A3.1, for more details. Few-shot transfer : After pretraining we use eight datasets for few-shot transfer: ILSVRC-2012 [14, 44], CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. ILSVRC-2012, also known as ImageNet-1k, is a slimmed version of ImageNet-21k and contains 1,281,167 training images of 1,000 classes. We use it as our main few-shot benchmark throughout the paper. We randomly sample 1-shot, 5-shot, 10-shot, and 25-shot versions from the first 10% of the training set. We further create addi- tional disjoint sets by using the next four 10% fractions of the training set. In addition, we follow previous works [3] and create a ‘minival’ set using the last 1% (12,811 images) of the ILSVRC-2012 training set. The ‘minival’ set is used for hyper parameter tuning and design decisions while the official ILSVRC-2012 validation set is used as a test set. In summary, our setup consists of 1,000, 5,000, 10,000, or 25,000 training images, 12,811 validation images (‘mini- val’), and 50,000 test images (‘validation’). For the other seven datasets, we also select a training, validation, and test split and create few-shot versions. More details on how these splits are created can be found in the Appendix, Sec. A3.1. We follow a similar procedure as with ILSVRC-2012 and use 10% of the training images to cre- ate 1-shot, 5-shot, 10-shot, and 25-shot versions of each dataset. We further use each validation set for hyper pa- rameter tuning and report final results on the respective test set. 4.3. Data Augmentation We reuse the set of augmentations first defined in AutoAug- ment [9] and adopted in later works, such as RandAugment [10] and TrivialAugment [39]. In addition, we also consider a few other image augmentations [35, 48, 58]. We select five geometric augmentations, i.e., rotate, shear-x, shear-y, translate-x, and translate-y; four crop & drop augmenta- tions, i.e., crop, resized crop, inception crop [48], and patch dropout [35]; seven stylistic augmentations, i.e., brightness, contrast, equalize, invert, posterize, sharpness, and solarize; and two other augmentations, i.e., JPEG and mixup [58]. In total, we end up with eighteen distinct augmentations . Note that all data augmentations incorporate random oper- ations, e.g., a random shift in x- and y-direction (translate- x and translate-y, respectively), a randomly selected set of patches (patch dropout), a random additive value to each feature (brightness), or a random mix of two features and their respective classes (mixup). Please refer to the Ap- pendix, Sec. A3.2, for more details. We focus on the following set of experiments: 1. We investigate FroFA for all eighteen augmentations. 2. For our top-performing FroFAs, namely, brightness, contrast, and posterize, we incorporate additional stochasticity using cFroFA and c 2FroFA variants ( cf . Sec. 3.3). 3. We investigate a sequential protocol where two of the best three (c/c 2)FroFAs are arranged sequentially, namely, brightness c 2FroFA, contrast FroFA, and pos- terize cFroFA. We test all six possible combinations. 4. Finally, we also apply variations of RandAugment [10] and TrivialAugment [39] directly on top of cached frozen features. More details and results can be found in the Appendix, Secs. A3.2 and A4, respectively. 4.4. Training & Evaluation Details We describe some base settings for pretraining, few- shot learning, and evaluation. Please refer to Appendix, Sec. A3.3 for more training details. Pretraining: We use the Big Vision code base for https://github.com/google-research/big_vision 4pretraining. We take the Ti/16, B/16, and L/16 models pre- trained on JFT-3B from Zhai et al. [56]. In addition, we pretrain Ti/16, B/16 and L/16 on ImageNet-21k following the settings of Steiner et al. [46]. To further explore trans- fer capabilities we also use an L/16 model with sigmoid language-image pretraining (SigLIP) [57] on WebLI [5]. Few-shot learning: We use the Scenic code base [12] for few-shot learning. We train the lightweight MAP-based head by sweeping across five batch sizes (32, 64, 128, 256, and 512), four learning rates (0.01, 0.03, 0.06, and 0.1), and five training step sizes (1,000; 2000; 4,000; 8,000; and 16,000), yielding 100 configurations for each shot. We use the respective validation set for early stopping and to find the best sweep setting. Our cached-feature setup fits on a single-host TPUv2 platform where our experiments run in the order of minutes. Evaluation: We report the top-1 accuracy across all our few-shot datasets. On ILSVRC-2012, we tune few-shot models exclusively on our validation set (our ILSVRC-2012 ‘minival’, cf . Sec. 4.2) and report results on our test set (of- ficial ILSVRC-2012 ‘validation’ set, cf . Sec. 4.2). 4.5. Baseline Models We establish two baselines: MAP and linear probe. MAP: We first cache theN×C-shaped (frozen) features from the last transformer block. Afterwards, we train a lightweight MAP head from scratch using the cached fea- tures followed by the final classification layer ( cf . Fig. 2). For simplicity, the MAP head follows the same architectural design as the underlying pretrained model. In some exper- iments, we additionally apply weight decay (wd), denoted as MAPwd. We sweep across [ADD V ALUES] and use the respective validation set for early stopping and to find the best sweep setting. Linear probe: We use cached1×C-shaped outputs from the pretrained MAP head to solve an L2-regularized regres- sion problem with a closed-form solution [56]. We sweep the L2 decay factor using exponents of 2 ranging from -20 up to 10. This setting is our auxiliary baseline. 5. Finding the Optimal FroFA Setup We focus our first investigations on an L/16 model pre- trained on JFT-3B, i.e., our largest model and largest im- age classification pretraining dataset, followed by few-shot learning on subsets of ILSVRC-2012 training set, i.e., our largest few-shot dataset. We will refer to this setup as our L/16 JFT-3B base setup. 5.1. Baseline Performance We first report the baseline performance in Tab. 1. We ob- serve a large gap between MAP and linear probe in the 1- https://github.com/google-research/scenic Method 1-shot 5-shot 10-shot 25-shot MAP 57.9 78.8 80.9 83.2 Linear probe 66.5 79.6 81.5 82.4 Table 1. Average top-1 accuracy for baseline settings on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5) and follow the respective baseline setting ( cf . Sec. 4.5). The best setting for each baseline is found using our ILSVRC- 2012 validation set. Further, each shot is sampled five times. The best result per shot is boldfaced. shot setting (-8.6% absolute) which significantly decreases in the 5-, 10-, and 25-shot settings to -0.8%, -0.6%, and +0.8% absolute, respectively. In the following, our main point of comparison is the MAP baseline. This might be counter-intuitive since the performance is worse than linear probe in most cases. How- ever, the higher input dimensionality in the MAP-based set- ting (cf . Sec. 4.5) gives us the option to reshape the input to three dimensions ( cf . Sec. 3.3) which opens up more room and variety for frozen feature augmentations (Fro- FAs). Later in Sec. 6.4, we compare the performance of our best augmentations to the linear probe baseline. 5.2. Default FroFA As a next step, we investigate the effect of adding a single FroFA to the MAP baseline setting. We first focus on the default FroFA formulation which uses a single randomly sampled value per input ( cf . Sec. 3.3). Results are shown in Tab. 2 where we report gains with respect to the MAP baseline using eighteen distinct FroFAs categorized into ge- ometric, crop & drop, stylistic, and other. Geometric: Interestingly, all geometric augmentations consistently lead to worse performance across all settings. Crop & drop: A simple crop or a resized crop yield a significant performance boost in the 1-shot setting of +3.0% and +1.9% absolute, respectively. Further, patch dropout provides modest gains in the 1-shot regime. Dropping patches is related to training efficiency, so we investigate this further. Fig. 3a shows the top-1 accuracy on 1- and 25- shot as a function of number of patches. More results can be found in Appendix, Sec. A4.1. Similar to observations by Liu et al. [35] we can randomly drop a large fraction of patches (>50%) without loosing performance. A key dif- ference is that Liu et al. only investigated the effect in the image space, while we provide evidence that patch dropout also transfers to the feature space. Finally, inception crop does not improve performance. Stylistic: The largest gains can be observed when em- ploying a stylistic FroFA, in particular brightness, contrast, and posterize. We identified brightness as the best perform- ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on 5-shot, and up to 0.6% on 10-shot. 5Geometric Crop & drop Stylistic Other Shots MAP rotate shear-x shear-y translate-x translate-y crop res. crop incept. crop patch drop. brightness contrast equalize invert posterize sharpness* solarize* JPEG* mixup 1 57.9 −1.3 −0.6 −0.8 −1.2 −1.4 +3.0 +1.9 +0.0 +0.4 +4.8 +2.8 +1.0 +2.7 +3.7 −0.1 +1.0 −0.1 −1.4 5 78.8 −0.3 −0.2 −0.2 −0.3 −0.3 +0.0 −0.2 +0.0 +0.0 +1.1 +0.8 +0.5 −0.3 +0.8 +0.1 −0.1 −0.3 −0.3 10 80.9 −0.2 −0.1 −0.1 −0.2 −0.2 +0.0 −0.2 +0.0 +0.0 +0.6 +0.6 +0.4 +0.0 +0.6 +0.1 +0.0 −0.1 +0.2 25 83.2 −0.2 −0.1 −0.2 −0.1 −0.2 +0.0 −0.1 −0.1 +0.0 +0.1 +0.1 +0.0 −0.2 +0.0 +0.0 +0.0 +0.0 +0.1 Table 2. (Average) top-1 accuracy for default FroFA on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). In total, we investigate eighteen FroFAs, categorized intogeometric, crop & drop, stylistic, and other. We sweep across a base sweep ( cf . Sec. 4.4) and the respective augmentation sweep (cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 validation set. Each shot is sampled five times, except for JPEG, sharpness, and solarize (marked with ‘*’). We highlight deterioration by shades of red and improvement by shades of green . Best three FroFAs are boldfaced. 1 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA (a) Patch dropout FroFA 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot + brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA Figure 3. Average top-1 accuracy for FroFA variantson our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each FroFA operation point (cf . Appendix, Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. Brightness Contrast Posterize Shots MAP c c 2 c c 1 57.9 +4.8 +5.9 +6.1 +2.8 +2.5 +3.7 +5.9 5 78.8 +1.1 +1.5 +1.6 +0.8 +0.0 +0.8 +0.8 10 80.9 +0.6 +1.1 +0.9 +0.6 +0.0 +0.6 +0.5 25 83.2 +0.1 +0.4 +0.3 +0.1 −0.1 +0.0 +0.0 Table 3. Average top-1 accuracy for a selection of default ( ) and channel (c/c 2) FroFA on our ILSVRC-2012 test set. Ab- solute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep (cf . Sec. 4.4) and the respective augmentation sweep ( cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 val- idation set. Each shot is sampled five times. The best results per shot and FroFA are boldfaced (multiple ones if close, i.e., ±0.2). Other: Neither JPEG nor mixup yield performance gains but rather more or less worsen the performance. 5.3. Channel FroFA Next, we investigate channel FroFA (cFroFA) for bright- ness, contrast, and posterize. Results are shown in Tab. 3, where we report absolute gains with respect to the MAP baseline. First, contrast cFroFA worsens performance across all shots. Second, posterize cFroFA improves perfor- mance on 1-shot from +3.7% to +5.9% while maintaining performance on all other shots. Lastly, brightness cFroFA significantly improves performance across all shots, i.e., from +4.8% to +5.9% on 1-shot, from +1.1% to +1.5% on 5-shot, from +0.6% to +1.1% on 10-shot, and from +0.1% to +0.4% on 25-shot. Giving the strong improvements for brightness cFroFA, we further test brightness c 2FroFA (see Tab. 3). On a first look, both variants perform equally well. In Fig. 3b, we further report the top-1 accuracy on 1-shot and 25-shot as a function of the brightness augmentation level. Results across other shots are similar and can be found in Appendix, Sec. A4.1. We clearly observe that brightness cFroFA is much more sensitive to the brightness level than brightness c2FroFA. Aross all shots, brightness cFroFA only works well for small brightness levels (0.1 to 0.5), while the c2FroFA variant performs better than the MAP baseline across the board. We attribute the better sensitivity prop- 6erties of brightness c2FroFA to the channel-wise mappings (5), (7) since this is the only change between cFroFA and c2FroFA. We did not a observe similar effect when switch- ing from cFroFA posterize to c2FroFA posterize. 5.4. Sequential FroFA Finally, out of our best three augmentations, i.e., bright- ness c 2FroFA (B-c 2), contrast FroFA (C), and posterize cFroFA (P-c), we combine two of them sequentially. We end up with a total of six combinations. Tab. 4 compares the performance of these six combinations against our prior best (B-c 2). On 1-shot, (B-c 2→P-c) significantly outper- forms (B-c2), improving absolute gains from 6.1% to 7.7%, while maintaining performance on other shots. We con- clude that advanced FroFA protocols may further improve performance. As an initial investigation, we applied varia- tions of RandAugment and TrivialAugment using our best three FroFAs ( cf . Tab. 3), however, with limited success. We include results in the Appendix, Sec. A4.2, and leave a deeper investigation to future works. 6. FroFA on More Datasets and Architectures How well does our best non-sequential augmentation strat- egy (brightness c 2FroFA) transfer across multiple dataset and architectures settings? In Secs. 6.1 to 6.3, we report results on seven other downstream few-shot datasets, two additional architectures, and two additional pretraining se- tups, respectively. This time, however, we also incorpo- rate weight decay in all MAP-based models . Further, in Secs. 6.2 and 6.3, we solely focus on the improvements over the MAP baseline and include a discussion on the improve- ments over the linear probe baseline in Secs. 6.1 and 6.4. 6.1. Transfer to Other Downstream Datasets In Tab. 5, we report results on seven additional transfer datasets, i.e., CIFAR10, CIFAR100, DMLab, DTD, Re- sisc45, SUN397, and SVHN. We compare the weight- decayed MAP and L2-regularized linear probe baseline to our approach, i.e., weight-decayed MAP combined with brightness c2FroFA (MAPwd + FroFA). We observe that across almost all shots and transfer datasets, MAP wd + FroFA shows the best results. Moreover, MAP wd + FroFA outperforms L2-regularized linear probe with only one exception, i.e., SUN397 (1-shot). With respect to the mean across all seven datasets, MAP wd + FroFA is signifi- cantly better than MAPwd, with improvements ranging from +4.4% absolute on 1-shot to +1.0% absolute on 25-shot. Fig. 1, left, displays the absolute accuracy gains averaged across all eight transfer datasets, including ILSVRC-2012. As before, our approach, i.e., MAPwd + FroFA, yields the best results across all shots. We further observe that the gains decrease with higher shots which aligns with our pre- vious observations. Shots MAP B-c 2 B-c2→C C→ B-c2 B-c2→P-c P-c→ B-c2 C→P-c P-c→C 1 57.9 +6.1 +4.0 +2.7 +7.7 +5.2 +5.0 +3.1 5 78.8 +1.6 +1.5 +0.2 +1.5 +0.4 +1.3 +0.0 10 80.9 +0.9 +1.2 +0.1 +1.0 +0.1 +0.9 +0.3 25 83.2 +0.3 +0.4 −0.7 +0.2 −0.5 +0.2 −0.4 Table 4. Average top-1 accuracy for a sequential FroFA pro- tocol on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We combine the best settings of brightness c 2FroFA (B- c2), contrast FroFA (C), and posterize cFroFA (P-c) sequentially (two at a time, order indicated by ‘ ↑’). We sweep across a base sweep (cf . Sec. 4.4) to first find the best setting on our ILSVRC- 2012 validation set. Each shot is sampled five times. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Trans. dataset Method 1-shot 5-shot 10-shot 25-shot CIFAR10 MAPwd 85.1 96.7 97.1 97.5 Linear probe 80.9 94.1 96.7 97.3 MAPwd + FroFA 93.8 97.6 97.8 97.8 CIFAR100 MAPwd 63.1 82.7 85.5 86.8 Linear probe 58.4 80.9 83.8 85.1 MAPwd + FroFA 67.8 84.0 86.2 87.1 DMLab MAPwd 24.4 30.3 30.2 36.5 Linear probe 24.0 26.3 25.6 30.9 MAPwd + FroFA 27.1 29.4 30.3 36.8 DTD MAPwd 49.2 68.2 74.1 80.8 Linear probe 46.9 65.9 71.3 77.3 MAPwd + FroFA 53.5 70.7 76.1 82.2 Resisc45 MAPwd 63.2 86.9 89.8 90.7 Linear probe 67.1 85.6 88.2 91.0 MAPwd + FroFA 67.6 87.2 89.7 91.5 SUN397 MAPwd 51.3 73.5 77.7 80.3 Linear probe 56.7 70.9 75.6 78.6 MAPwd + FroFA 56.2 75.9 78.9 81.2 SVHN MAPwd 20.7 23.9 30.2 47.4 Linear probe 11.8 15.0 18.7 21.5 MAPwd + FroFA 21.8 31.0 43.5 50.3 Mean MAPwd 51.0 66.0 69.2 74.3 Linear probe 49.1 62.7 65.7 68.8 MAPwd + FroFA 55.4 68.0 71.8 75.3 Table 5. Top-1 accuracy of our best FroFA for additional transfer datasets using a JFT-3B L/16 model. Results are re- ported on the respective test set ( cf . Sec. A3.1). We compare results to a weight-decayed MAP baseline, i.e., MAP wd, and an L2-regularized linear probe. Depending on the setting, we sweep across a base,cf . Sec. 4.4, a weight decay or L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on the respective validation set. Per shot and dataset, the best result is boldfaced while the second-best result is underlined (multiple ones if close, i.e., ±0.2). 7Ti/16 B/16 L/16 model −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model −0.5 0.0 0.5 5-shot Ti/16 B/16 L/16 model 0.00 0.25 0.50 0.75 1.00 1.25 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 5 25-shot MAPwd linear probe (a) JFT-3B Ti/16 B/16 L/16 model −20 −15 −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 5-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 25-shot (b) ImageNet21k Figure 4. Average top-1 accuracy of brightness c2FroFA for JFT-3B (a) and ImageNet-21k (b) models on our ILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline are reported. Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. 6.2. Transfer to Other Architectures We employ brightness c2FroFA on two other JFT-pretrained models, namely Ti/16 and B/16. In Fig. 4a, we report im- provements in top-1 accuracy with respect to the weight- decayed MAP baseline. Across all shots and model archi- tectures, incorporating FroFA either maintains or improves performance, except for B/16, 25-shot. Given that larger models tend to be more prone to overfitting in the 1-shot setting, we observe increasing improvements from FroFA when scaling the architecture. With a higher number of shots, the observed improvements over the baseline model become smaller. We attribute this to the strong baseline per- formance leaving lesser headroom for improvements. We refer to the Appendix, Sec. A4.3, for the exact values. 6.3. Transfer to Other Pretraining Setups ImageNet-21k: In Fig. 4b, we report improvements in top- 1 accuracy with respect to the weight-decayed MAP base- line for ImageNet-21k-pretrained Ti/16, B/16, and L/16. Consistent with our JFT-3B observations, across all shots and model architectures, incorporating FroFA either main- tains or improves performance. The improvements dimin- ish as the number of shots increases. This trend is likely due to the higher baseline accuracies at higher shot counts. We again refer to the Appendix, Sec. A4.3, for the exact values. WebLI and SigLIP : We also tested an L/16 model with sigmoid language-image pretraining (SigLIP), follow- ing [57]. We report the absolute accuracy gains averaged across eight datasets. The results are shown in Fig. 1, right. From the results we can conclude that our FroFA setting also transfers to language-image pretrained models further emphasizing its generalizability. 6.4. Linear Probe Comparison on ILSVRC-2012 We will now look at Figs. 4a and 4b, but discuss gains with respect to the L2-regularized linear probe baseline. We start with models pretrained on JFT-3B (cf . Fig. 4a). On 1-shot, we observe that we lack behind linear probe but can close the gap by scaling up the model size. On 5- to 25-shot, with the exception of Ti/16 on 5-shot, brightness c 2FroFA significantly outperforms the linear probe baseline. On ImageNet-21k (cf . Fig. 4b), we observe even larger gaps to linear probe on 1-shot (up to -20% absolute). How- ever, similar to results on JFT-3B, performance on 5- to 25-shot improves significantly over linear probe or at worst stays the same. 7. Conclusions We investigated eighteen frozen feature augmentations (FroFAs) along three axes: model size, pretraining and transfer few-shot dataset. We show that a training with Fro- FAs, in particular stylistic ones, gives large improvements upon a representative baseline across all shots. In addition, per-channel variants further improve performance, e.g., by 1.6% absolute in the ILSVRC-2012 5-shot setting. Finally, we were able to show that our results transfer. Averaged results across seven downstream tasks show that using a variant of brightness FroFA improves by 4.4% absolute upon the same representative baseline in the 1-shot setting. 8References [1] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009. 2, 4, 12 [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit- twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass- abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv, 1612.03801:1–11, 2016. 4, 12 [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet- ter Plain ViT Baselines for ImageNet-1k.arXiv, 2205.01580: 1–3, 2022. 4 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recogni- tion. In Proc. of NeurIPS, pages 16664–16678, New Orleans, LA, USA, 2022. 2 [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad- bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A Jointly-Scaled Multilingual Language- Image Model. InProc. of ICLR, pages 1–33, Kigali, Rwanda, 2023. 1, 2, 4, 5, 12 [6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens- ing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE, 105(10):1865–1883, 2017. 4, 12 [7] Franc ¸ois Chollet. Xception: Deep Learning With Depthwise Separable Convolutions. In Proc. of CVPR , pages 1063– 6919, Honolulu, HI, USA, 2017. 4 [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. of CVPR, pages 3606–3613, Columbus, OH, USA, 2014. 4, 12 [9] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va- sudevan, and Quoc V . Le. AutoAugment: Learning Aug- mentation Strategies From Data. In Proc. of CVPR , pages 113–123, Long Beach, CA, USA, 2019. 2, 4 [10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandAugment: Practical Automated Data Augmenta- tion with a Reduced Search Space. In Proc. of NeurIPS , pages 18613–18624, virtual, 2020. 1, 4, 13 [11] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan. CoAtNet: Marrying Convolution and Attention for All Data Sizes. In Proc. of NeurIPS, pages 3965–3977, virtual, 2021. 4 [12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A JAX Library for Computer Vision Research and Beyond. In Proc. of CVPR, pages 21393–21398, New Orleans, LA, USA, 2022. 5 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan- nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku- mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar- avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh- nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Proc. of ICML , pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 12 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, 2009. 1, 2, 4, 12 [15] Terrance DeVries and Graham W. Taylor. Dataset Augmen- tation in Feature Space. In Proc. of ICLR - Workshops, pages 1–12, Toulon, France, 2017. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proc. of ICLR, pages 1–21, virtual, 2021. 1, 2, 4 [17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP- Adapter: Better Vision-Language Models with Feature Adapters. Int. J. Comput. Vis., pages 1–15, 2023. 2 [18] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im- proving Neural Language Models with a Continuous Cache. In Proc. of ICLR, pages 1–9, Toulon, France, 2017. 2 [19] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-Efficient Model Adaptation for Vision Transformers. In Proc. of AAAI, pages 817–825, Washington, DC, USA, 2023. 1, 2 [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. In Proc. of ICLR, pages 1–15, Virtual, 2020. 2 [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling Knowledge in a Neural Network. In proc. of NIPS - Work- shops, pages 1–9, Montr´eal, QC, Canada, 2014. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2, 4 [22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proc. of ICML , pages 2790–2799, Long Beach, CA, USA, 2019. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR, pages 1–13, virtual, 2022. 2 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- 9sual Prompt Tuning. In Proc. of ECCV, pages 709–727, Tel Aviv, Israel, 2022. 2 [25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, pages 1–15, San Diego, CA, USA, 2015. 13 [26] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General Visual Representation Learning. In Proc. of ECCV, pages 491–507, virtual, 2020. 2, 4 [27] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Mod- els. arXiv, 2112.13492:1–32, 2023. 4 [28] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl- liam Campbell. A Closer Look At Feature Space Data Aug- mentation For Few-Shot Intent Classification. In Proc. of EMNLP - Workshops, pages 1–10, Hong Kong, China, 2019. 2 [29] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The Omniglot Challenge: a 3-year Progress Re- port. Curr. Opin. Behav. Sci., 29:97–104, 2019. 2 [30] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In Proc. of ICML , pages 3744–3753, Long Beach, CA, USA, 2019. 1, 2, 4 [31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision Transformer for Small-size Datasets. arXiv, 2112.13492:1–11, 2021. 1 [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proc. of EMNLP, pages 3045–3059, virtual, 2021. 2 [33] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In Proc. of ICPR , pages 728–733, Beijing, China, 2018. 2 [34] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient Training of Visual Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, 2021. 1 [35] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az- izpour, and Kevin Smith. PatchDropout: Economizing Vi- sion Transformers Using Patch Dropout. In Proc. of WACV, pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 5 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV, pages 10012–10022, virtual, 2021. 1 [37] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson. Learning Multimodal Data Augmentation in Feature Space. In Proc. of ICLR, pages 1–15, Kigali, Rwanda, 2023. 2 [38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proc. of ICLR, pages 1–18, New Orleans, LA, USA, 2019. 13 [39] Samuel G. M ¨uller and Frank Hutter. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proc. of ICCV, pages 774–782, virtual, 2021. 2, 4, 13 [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat- ural Images with Unsupervised Feature Learning. In proc. of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 4, 12 [41] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah- moud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je- gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi- otr Bojanowski. Dinov2: Learning Robust Visual Features Without Supervision. arXiv, 2304.07193:1–31, 2023. 1 [42] Emin Orhan. A Simple Cache Model for Image Recognition. In Proc. of NeurIPS, pages 10128–10137, Montr´eal, Canada, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, pages 8748–8763, virtual, 2021. 1, 2 [44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. Int. J. Comput. Vis., 115(3):211–252, 2015. 1, 2, 4, 12 [45] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proc. of ICML, pages 4596–4604, Stockholm, Sweden, 2018. 13 [46] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. Trans. Mach. Learn. Res., pages 1–16, 2022. 2, 5, 13 [47] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proc. of ICCV , pages 843–852, Venice, Italy, 2017. 4 [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Ar- chitecture for Computer Vision. In Proc. of CVPR , pages 2818–2826, Las Vegas, NV , USA, 2016. 2, 4 [49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training Data-Efficient Image Transformers & Distillation Through Attention. In Proc. of ICML , pages 10347–10357, virtual, 2021. 4 [50] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na- jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben- gio. Manifold Mixup: Better Representations by Interpo- lating Hidden States. In Proc. of ICML, pages 6438–6447, Long Beach, CA, USA, 2019. 2 10[51] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Proc. of NIPS , pages 3637–3645, Barcelona, Spain, 2016. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2 [52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. In Proc. of ICCV , pages 548–558, virtual, 2021. 1 [53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In Proc. of CVPR, pages 3485–3492, San Francisco, CA, USA, 2010. 2, 4, 12 [54] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN Database: Exploring a Large Collection of Scene Categories. Int. J. Comput. Vis., 119(1): 3–22, 2016. 2, 4, 12 [55] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andr´e Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A Large-scale Study of Representation Learn- ing with the Visual Task Adaptation Benchmark. arXiv, 1910.04867:1–33, 2020. 4, 12 [56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling Vision Transformers. In Proc. of CVPR, pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4, 5, 12, 13 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre- Training. In Proc. of ICCV , pages 11975–11986, Paris, France, 2023. 1, 5, 8, 13 [58] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Mini- mization. In Proc. of ICLR , pages 1–13, Vancouver, BC, Canada, 2018. 1, 2, 4 [59] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun- chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip- Adapter: Training-Free Adaption of CLIP for Few-Shot Classification. In Proc. of ECCV, pages 493–510, Tel Aviv, Israel, 2022. 2 11Frozen Feature Augmentation for Few-Shot Image Classification Supplementary Material A1. Introduction We give additional details and results to complement the main paper. All included citations refer to the main paper’s references. A2. Brightness We provide the code snippet for brightness c2 FroFa def transform_aug_reverse( x, augment, aug_min_val=0, aug_max_val=1.0, x_min_val=None, x_max_val=None, clip=True): \"\"\"Transform to (low, high)-space, perform augmentation, transform back.\"\"\" l = x_min_val if x_min_val is None: l = tf.reduce_min(x) h = x_max_val if x_max_val is None: h = tf.reduce_max(x) # [l, h] --> [0, 1] x = (x - l) / (h - l + 1e-8) # [0, 1] --> [low, high] x = x * (aug_max_val - aug_min_val) x = x + aug_min_val x = tf.cast(augment(x), tf.float32) if clip: tf.clip_by_value(x, aug_min_val, aug_max_val) # [low, high] --> [0, 1] x = (x - aug_min_val) x = x / (aug_max_val - aug_min_val) x = x * (h - l + 1e-8) + l # [0, 1] --> [l, h] return x def get_random_brightness(max_delta=0.1, clip=False): # A random value in [-max_delta, +max_delta] # is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. def _random_brightness(image): return tf.image.random_brightness( image, max_delta) def tar(x): return transform_aug_reverse( x, augment=_random_brightness, aug_min_val=0, aug_max_val=1.0, clip=clip) return tar def get_random_brightness_per_channel_v2( max_delta=0.1, clip=True): \"\"\"Applies channel-wise random brightness transformations.\"\"\" # A random value in [-max_delta, +max_delta] is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. random_brightness = get_random_brightness( max_delta, clip) def _random_brightness_pc(x): x = tf.expand_dims(x, axis=2) # (H, W, 1, C) x = tf.unstack(x, axis=-1) # C x (H, W, 1) x = [random_brightness( {\"image\": x_i})[\"image\"] for x_i in x] return tf.concat(x, axis=-1) return _random_brightness_pc A3. Detailed Experimental Setup In the following, we provide additional details to our exper- imental setup. A3.1. Datasets In this section, we focus on details regarding our pretraining and few-shot datasets. Pretraining: As stated in the main paper, Sec. 4.2, we pretrain our models by either using JFT-3B [56], ImageNet- 21k [14, 44], or WebLI [5]. In JFT-3B, the images are annotated with noisy labels by using a semi-automated pipeline. We follow common practice [13, 56] and ignore the hierarchical aspect of the labels. ImageNet-21k is a superset of the well known ILSVRC-2012 dataset, also known as “ImageNet-1k” or just “ImageNet”. WebLI is a recently introduced image- and-language dataset. It contains 10 billion images and tens of billions image-text pairs with over 100 languages. Few-shot transfer: As stated in the main paper, Sec. 4.2, our experiments concentrate around few-shot transfer on ILSVRC-2012 [14, 44]. We also provide results on CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. When official test and validation splits are available, we use them for eval- uation across all datasets. In general, we use the versions in TensorFlow Datasets. CIFAR10 contains 60,000 images of 10 equally dis- tributed classes split into 50,000 training images and 10,000 test images. We further split the official training dataset into 45,000 training images and 5,000 validation images. CIFAR100 is a superset of CIFAR10 with 100 equally distributed classes and 60,000 images. Similar to CIFAR10, we use 45,000 images for training, 5,000 images for valida- tion and 10,000 images for test. DMLab consists of frames collected from the DeepMind Lab environment. Each frame is annotated with one out https://www.tensorflow.org/datasets 12of six classes. We use 65,550 images for training, 22,628 images for validation, and 22,735 for test. DTD is a collection of 5,640 textural images categorized into 47 distinct classes. Each of the three splits, i.e., train- ing, validation, and test, has exactly 1,880 images. Resisc45 is a benchmark with 31,500 images for image scene classification in remote sensing scenarios. In total, 47 different catogries for scenes are defined. We use the first 23,000 images for training, the subsequent 2,000 images for validation and the last 6,300 images for test. SUN397 is a 397-category database of 108,753 images for scene understanding. We use 76,128 images for training, 10,875 images for validation, and 21,750 images for test. SVHN is a Google Street View dataset with a large col- lection of house number images. In total, 10 distinct classes exist. We use the cropped version with 73,257 images for training and 26,032 images for test. Further, we create a val- idation subset by only using the first 70,000 out of 73,257 training images for actual training and the remaining 3,257 images for validation. A3.2. Data Augmentation In this section, we provide additional details on the used data augmentation techniques and protocols. (c/c2)FroFA: In Tab. 6, we give detailed descriptions of each FroFA, cFroFA, and c 2FroFA setting. We mostly build upon an AutoAugment implementation from Big Vision. To keep it simple, we use v or v1, v2 as sweep parameter(s) for all augmentations. By default, we first re- shape the two-dimensional features f to three-dimensional features f∗ (1) of shape √ N × √ N × C, with N = 196 and C ∈ {192, 768, 1024} in all our experiments. Note that the value of C depends on the architecture. We further want to point out, while some augmentations heavily rely on the three-dimensional representation, e.g., all geometric ones, some others are also transferable to a two-dimensional rep- resentation, e.g., brightness or contrast. As pointed out in the main paper, Tab. 3, brightness c2FroFA, contrast FroFA, and posterize cFroFA are our best FroFAs. For all three, we list the best sweep settings in Tab. 7. Advanced protocols: As mentioned in the main paper, Sec. 4.3, besides our fixed sequential protocol ( cf . Tab. 4) we also tested variations of RandAugment [10] and Triv- ialAugment [39]. In all protocols, we sample from the best settings of brightness c2FroFA, contrast FroFA, and poster- ize cFroFA. In particular, we use v = 1.0 for brightness c2FroFA, v = 6.0 for contrast FroFA, and v1 = 1, v2 = 8 for posterize cFroFA ( cf . Tab. 6). We re-use the abbrevi- ations from Tab. 4 in the following, i.e., B-c 2, C, and P- c, respectively. For the RandAugment and TrivialAugment https://github.com/google- research/big_vision/ blob/main/big_vision/pp/autoaugment.py variations, we uniformly sample from either the best three FroFAs, i.e., Atop3 = {B-c2, C, P-c}, or the best two Fro- FAs, i.e., Atop2 = A3 \\ {C}. Further, our RandAugment variation randomly constructs a sequence of augmentations by uniformly sampling the integer sequence length from 1 to |A|, with A ∈ {Atop2, Atop3} depending on whether Atop2 or Atop3 is used. A3.3. Training Details Pretraining: In the JFT-3B setup, we use pretrained mod- els from Zhai et al. [56]. The models are pretrained using a sigmoid cross-entropy loss. The weights are optimized by Adafactor [45] in half-precision mode, β1 = 0.9, and β2 = 0.999. Further, (decoupled) weight decay [38] is applied with 3.0 on the head and 0.03 for the rest of the network weights. The learning rate is adapted by a recip- rocal square-root schedule for 4,000,000 steps with a lin- ear warm-up phase of 10,000 steps and a linear cool-down phase of 50,000 steps. The starting learning rate is 0.01 for Ti/16 and L/16 and 0.03 for B/16. The images are prepro- cessed by an224×224 inception crop and a random horizon- tal flip. We set the batch size to 4,096. To stabilize training, a global norm clipping of 1.0 is used. In the ImageNet-21k setup, we follow settings from Steiner et al. [46] and use a sigmoid cross-entropy loss for multi-label pretraining. We use the Adam optimizer [25] in half-precision mode and set β1 = 0.9 and β2 = 0.999. Fur- ther, we apply (decoupled) weight decay with either 0.03 for Ti/16 or 0.1 for B/16 and L/16. We adapt the learning rate using a cosine schedule for roughly 930,000 steps (300 epochs) with a linear warm-up phase of 10,000 steps. We set the starting learning rate to 0.001 for all models. During preprocessing, we crop the images to 224×224 following an inception-style crop and a random horizontal flip. While we don’t use any additional augmentation for Ti/16, we fol- low suggestions by Steiner et al. [46] and use the ‘light1’ and ‘medium2’ augmentation settings for B/16 and L/16, respectively. Finally, we use a batch size of 4,096 and sta- bilize training by using a global norm clipping of 1.0. In the WebLI setup, we take an L/16 model from [57]. In particular, we use [ADD DETAILS]. Few-shot learning: We first cache each few-shot dataset by processing each of them through a pretrained model and store the extracted features (cf . Fig. 2). We resize each im- age to 224×224 before feeding it to the model. We follow up with a training where we mostly use trans- fer learning settings from Steiner et al. [46]. We use a sig- moid cross-entropy loss. This might be non-intuitive given that all of our few-shot datasets are not multi-labeled. How- ever, we didn’t really observe any performance drops com- pared to using the more common softmax cross-entropy loss, so we stick to the sigmoid cross-entropy loss. We use stochastic gradient descent with momentum of 0.9. Simi- 13Augmentation Description Geometric rotate We rotate each of the C feature channels fc (2) by z ∼ U(−v, v). We sweep across v ∈ {15, 30, 45, 60, 75, 90} representing the maximum positive and negative rotation angle in degrees. shear-{x,y} We (horizontally/vertically) shear each of the C feature channels fc (2) by z ∼ U(0, v). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7} representing the maximum level of horizontal or vertical shearing. translate-{x,y} We (horizontally/vertically) translate each of theC feature channels fc (2) by uniformly samplingz from {0, 1, ..., v}. We sweep across integer values 1 ≤ v ≤ 7 representing the maximum horizontal or vertical translation. Crop & drop crop We randomly crop each of the C feature channels fc (2) to v×v at the same spatial position. We sweep across integer values 1 ≤ v ≤ 13 representing the square crop size. resized crop We resize each of the C feature channels fc (2) to v × v and then randomly crop each to 14 × 14 at the same spatial position. We sweep across v ∈ {16, 18, 20, 22, 24, 26, 28, 35, 42} representing the resized squared spatial resolution. inception crop We apply an inception crop with probability v. We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. patch dropout We randomly keep v out of N patches of f having shape N × C. Note that the patch ordering is also randomized. We sweep across v ∈ {1, 2, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 116, 132, 148, 164, 180}. Stylistic brightness We randomly add a value z ∼ U(−v, v) to each of the C feature channels fc (2). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. We test this method using all FroFA variants. In the default FroFA and the cFroFA variants, the features are scaled by (5) taking the minimumfmin and maximum fmax across all chan- nels into account. In the c 2FroFA variant, each channel fc (2) is shifted individually and uses the channel minimum and maximum instead. Further, in the cFroFA and c2FroFA variants we sample C values of z, one for each channel. contrast We randomly scale each of the C feature channels fc (2) by z ∼ U( 1 v , v). We sweep across v ∈ {1.25, 1.5, 2, 3, 4, 5, 6, 7, 9, 10}. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sample C values of z, one for each channel. equalize We first map the features from value range R to the integer subset I = {0, 1, ...,195}, i.e., executing (5) followed up by a discretization step. We choose this value range as preliminary results mapping from R to the more commonly used I = {0, 1, ...,255} instead didn’t show any effects. We continue by equalizing 196 bins and then transforming the results back to the original space using (7). We apply equalize with probability v. In particular, we sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. invert We change the sign of features f∗ with probability v. We sweep acrossv ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. posterize We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. In other words, we use an 8-bit representation for features f∗. Posterize performs a quantization by a bit-wise left and right shift. We uniformly sample the shift value z between integer values v1 and v2. In our sweep, we test a subset of all possible combinations. In particular, we first set v2 = 8and reduce v1 from 7 to 1. We then fix v1 = 1and increase v2 from 2 to 7 again. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sampleC values of z, one for each channel. sharpness We first apply a two-dimensional convolution on f∗ (1) using a 3×3 smoothing filter. Next, we mix the original features with the resulting “smoothed” features using a randomly sampled blending factor z ∼ U(0, v). We sweep across v ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0}. solarize We do not map features from R to I = [0, 1], but stay in R. We compute the minimum fmin and maximum fmax across features f∗. We conditionally subtract all values smaller than0.5·fmin from fmin or larger than0.5·fmax from fmax. We apply this method with a probabilityv and sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Other JPEG We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. We then perform a JPEG compression of each channel by randomly sampling a JPEG quality z ∼ U(v1, v2). We sweep across combinations of v1 ∈ {10, 25, 50, 75} and v2 ∈ {25, 50, 75, 100}, with v2 > v1. mixup We do not map features from R to [0, 1], but stay in R. We mix two features f∗ i , f∗ j according to z ·f∗ i + (1−z) ·f∗ j by sampling a random value z ∼ B(α, α), with Beta distribution B(α, α) parameterized by α = v. The labels are mixed using the same procedure. We sweep across v ∈ {0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Table 6. Details on our used set of augmentations. For simplicity, instead of introducing a new hyper parameter for each data augmenta- tion, we re-use v as a sweep parameter that is set during a sweep and differs for each augmentation. If not stated otherwise, each method is only applied as default FroFA and we first map featuresf (two-dimensional representation) or f∗ (three-dimensional representation) from value range R to I = [0, 1] using (5). By default, we assume a three-dimensional representation f∗ although some augmentations would work also in the two-dimensional representation f, i.e., a reshaping is not necessary. lar to the pretraining setup, we also store the internal state in half-precision. We do not apply any weight decay. The learning rate is adapted following a cosine schedule with a linear warm-up phase of 500 steps. In addition, we stabilize 14FroFA Shots Base learning rate Batch size Training steps v or v1, v2 B-c2 1 0.01 512 4,000 1.0 10 0.01 64 16,000 1.0 15 0.01 256 8,000 0.9 25 0.01 512 8,000 0.8 C 1 0.01 32 16,000 6.0 10 0.01 128 8,000 6.0 15 0.01 512 2,000 6.0 25 0.01 256 4,000 7.0 P-c 1 0.01 512 8,000 1, 8 10 0.03 512 8,000 1, 8 15 0.03 512 16,000 1, 8 25 0.03 64 16,000 2, 8 Table 7. Our best sweep settings for our best three FroFAs , namely, brightness cFroFA (B-c 2), contrast (C), and posterize cFroFA (P-c). We list the shots, base learning rate, batch size, number of training steps, and the augmentation parameter, denoted as v or v1, v2 (see Tab. 6 for a detailed explanation ofv and v1, v2). The best sweep settings are found using our ILSVRC-2012 vali- dation set. RA∗ TA∗ Shots MAP B-c 2 Atop2 Atop3 Atop2 Atop3 1 58.4 +6.0 +3.9 +2.4 +4.8 +4.3 5 79.1 +1.5 +1.0 +0.4 +1.4 +1.2 10 80.7 +1.3 +1.0 +0.6 +1.4 +1.4 25 83.0 +0.6 +0.4 +0.0 +0.5 +0.4 Table 8. Top-1 accuracy for advanced FroFA protocols on our ILSVRC-2012 test set. Absolute gains to the MAP baseline (ref- erence run) are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We compare brightness c 2FroFA (B-c2) with our variations of RandAugment (RA∗) and TrivialAugment (TA∗), cf . Sec. A3.2. For the latter, we either use the top-2 ( Atop2) or top- 3 ( Atop3) augmentations. We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 val- idation set. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). training by using a global norm clipping of 1.0. Further, we sweep across batch size, learning rate and number of steps yielding 100 combinations (cf . Sec. 4.4) for each shot. A4. Additional Experimental Results In this section, we show additional experimental results. A4.1. Patch Dropout and Brightness In Fig. 3, we only report results for 1-shot and 25- shot settings using patch dropout FroFA and brightness (c/c2)FroFA. We extend this by also reporting results for 5-shot and 10-shot settings in Figs. 5 and 6. We observe the same effects in the other settings as well. A4.2. Advanced FroFA Protocols In Tab. 8, we report results for our RandAugment (RA ∗) and TrivialAugment (TA∗) variations. We did not average across five runs and thus only report absolute gains with re- spect to a reference run. Therefore, numbers which are also reported in the main paper, e.g., Tab. 4, are slightly differ- ent. All in all, we observe that both RA ∗ and TA∗ do not improve upon the best single augmentation, i.e., brightness c2FroFA (B-c2). We also observe that increasing the set of augmentations from Atop2 to Atop3 rather worsens the per- formance for both RA∗ and TA∗. A4.3. Detailed FroFA Transfer Results In Tab. 9, we report exact numbers for Fig. 4, i.e., Ti/16, B/16, and L/16 pretrained on either ImageNet-21k or JFT- 3B and subsequently finetuned on few-shotted ILSVRC- 2012 training sets. Numbers for the two baselines, i.e., MAP ( with weight decay) and linear probe, and our best method, i.e., MAP ( with weight decay) combined with brightness c2FroFA (MAP + FroFA), are reported. In addi- tion, we report numbers, where we use MAPwithout weight decay in Tab. 10. As before, we observe that our method performs worse on all 1-shot settings, but is on par or sig- nificantly better than MAP and/or linear probe on most 5- to 25-shot settings. 151 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 72 74 76 78 5-shot 1 50 100 150 number of patches 76 77 78 79 80 81 10-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA Figure 5. Average top-1 accuracy for patch dropout FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each number of patches (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 74 76 78 80 5-shot 0.1 0.3 0.5 0.7 0.9 brightness level 79 80 81 82 10-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot MAP + brightness cFroFA + brightness c2FroFA Figure 6. Top-1 accuracy for channel variants (c/c2) of brightness FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each brightness level (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAPwd 20.5 53.6 59.7 64.9 19.1 46.4 53.6 60.2 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAPwd + FroFA 20.6 54.5 60.1 65.2 19.6 47.2 53.6 60.3 B/16 MAPwd 30.5 71.7 75.3 78.0 51.3 74.8 77.5 79.8 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAPwd + FroFA 30.6 73.3 76.0 78.1 52.5 75.1 77.6 79.5 L/16 MAPwd 38.7 75.9 78.6 80.6 62.0 79.9 81.5 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAPwd + FroFA 39.3 78.0 80.0 81.0 63.7 80.4 82.0 83.6 Table 9. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline, as well as our best FroFA-based approach, i.e., weight-decayed MAP combined with brightness c 2FroFA (MAPwd + FroFA). Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAPwd + FroFA, is on par or significantly better than MAPwd and/or linear probe on most 5- to 25-shot settings. 16ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAP 20.4 53.2 59.5 64.7 17.9 45.5 53.5 60.1 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAP + FroFA 22.1 54.9 60.1 65.0 20.3 47.2 53.6 60.1 B/16 MAP 31.3 70.3 75.1 78.1 48.9 73.4 76.5 79.4 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAP + FroFA 30.6 73.4 76.3 78.3 52.4 75.2 77.8 79.9 L/16 MAP 38.8 74.9 78.5 80.7 57.9 78.8 80.9 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAP + FroFA 39.3 78.0 80.0 81.2 63.9 80.3 82.0 83.6 Table 10. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the MAP and L2-regularized linear probe baseline, as well as our best FroFA-based approach,i.e., MAP combined with brightness c2FroFA (MAP + FroFA). Depending on the setting, we sweep across a base,cf . Sec. 4.4, an L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAP + FroFA, is on par or significantly better than MAP and linear probe on most 5- to 25-shot settings. 17",
      "references": [
        "Learning Multiple Layers of Features from Tiny Images",
        "DeepMind Lab",
        "Better Plain ViT Baselines for ImageNet-1k",
        "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "Remote Sensing Image Scene Classification: Benchmark and State of the Art",
        "Xception: Deep Learning With Depthwise Separable Convolutions",
        "Describing Textures in the Wild",
        "AutoAugment: Learning Augmentation Strategies From Data",
        "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
        "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "Scenic: A JAX Library for Computer Vision Research and Beyond",
        "Scaling Vision Transformers to 22 Billion Parameters",
        "ImageNet: A Large-Scale Hierarchical Image Database",
        "Dataset Augmentation in Feature Space",
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
        "Improving Neural Language Models with a Continuous Cache",
        "Parameter-Efficient Model Adaptation for Vision Transformers",
        "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "Distilling Knowledge in a Neural Network",
        "Parameter-Efficient Transfer Learning for NLP",
        "LoRA: Low-Rank Adaptation of Large Language Models",
        "Visual Prompt Tuning",
        "Adam: A Method for Stochastic Optimization",
        "Big Transfer (BiT): General Visual Representation Learning",
        "Three Towers: Flexible Contrastive Learning with Pretrained Image Models",
        "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification",
        "The Omniglot Challenge: a 3-year Progress Report",
        "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
        "Vision Transformer for Small-size Datasets",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Data Augmentation via Latent Space Interpolation for Image Classification",
        "Efficient Training of Visual Transformers With Small Datasets",
        "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
        "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
        "Learning Multimodal Data Augmentation in Feature Space",
        "Decoupled Weight Decay Regularization",
        "TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation",
        "Reading Digits in Natural Images with Unsupervised Feature Learning",
        "Dinov2: Learning Robust Visual Features Without Supervision",
        "A Simple Cache Model for Image Recognition",
        "Learning Transferable Visual Models From Natural Language Supervision",
        "ImageNet Large Scale Visual Recognition Challenge",
        "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers",
        "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "Rethinking the Inception Architecture for Computer Vision",
        "Training Data-Efficient Image Transformers & Distillation Through Attention",
        "Manifold Mixup: Better Representations by Interpolating Hidden States",
        "Matching Networks for One Shot Learning",
        "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
        "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
        "SUN Database: Exploring a Large Collection of Scene Categories",
        "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
        "Scaling Vision Transformers",
        "Sigmoid Loss for Language Image Pre-Training",
        "Mixup: Beyond Empirical Risk Minimization",
        "Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification"
      ],
      "meta_data": {
        "arxiv_id": "2403.10519v2",
        "authors": [
          "Andreas Bär",
          "Neil Houlsby",
          "Mostafa Dehghani",
          "Manoj Kumar"
        ],
        "published_date": "2024-03-15T17:59:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Frozen Feature Augmentation (FroFA), a simple yet effective way to perform data augmentation directly in the frozen feature space of pretrained vision transformers for few-shot image classification. Demonstrates that point-wise stylistic augmentations—especially per-channel random brightness (c2FroFA)—yield consistent accuracy gains across 3 backbone sizes (Ti/16, B/16, L/16), 3 large-scale pretraining datasets (JFT-3B, ImageNet-21k, WebLI/SigLIP) and 8 downstream datasets, outperforming strong MAP and linear-probe baselines without modifying backbone parameters.",
        "methodology": "1) Cache last-block token features from frozen ViT backbones. 2) Reshape to H×W×C and linearly scale each channel (or whole tensor) to [0,1]. 3) Apply adapted image augmentations in feature space: default (single random scalar per tensor), cFroFA (independent per-channel scalar), and c2FroFA (per-channel scaling + per-channel min/max mapping). Eighteen augmentations explored; stylistic ones (brightness, contrast, posterize) work best. 4) Train a lightweight multi-head attention pooling (MAP) head plus classifier on the augmented frozen features. 5) Evaluate single and sequential augmentation protocols versus MAP and L2-regularised linear probe baselines.",
        "experimental_setup": "Pretraining: ViT Ti/16, B/16, L/16 pretrained on JFT-3B (≈3B images), ImageNet-21k (14M images), or WebLI with SigLIP. Few-shot transfer: 1,5,10,25-shot subsets of ILSVRC-2012 (ImageNet-1k) plus CIFAR-10/100, DMLab, DTD, Resisc45, SUN397, SVHN. Features cached once; MAP head trained with SGD (sweeps over 5 batch sizes×4 lrs×5 step counts =100 configs), early-stopped on validation splits. Augmentation hyper-parameters swept per op. Metrics: top-1 accuracy averaged over 5 random shot samples. Baselines: MAP (with/without weight decay) and closed-form L2-regularised linear probe.",
        "limitations": "• Gains on large dataset (ImageNet-1k) are modest vs smaller datasets.\n• Only point-wise and channel-wise augmentations succeed; geometric or element-wise transforms reduce accuracy, limiting augmentation diversity.\n• Study restricted to last-block ViT features and classification; generalisation to other layers, CNN backbones, or tasks (detection, segmentation) untested.\n• Requires hyper-parameter sweeps for each augmentation level and shot setting, adding tuning cost.\n• Relies partly on proprietary pretraining data (JFT-3B, WebLI).",
        "future_research_directions": "1) Design richer yet feature-compatible augmentations (local spatial warps, mixup variants) or learn augmentation policies automatically.\n2) Extend FroFA to other tasks (object detection, segmentation), modalities (video, multimodal) and to earlier layers or non-ViT backbones.\n3) Combine FroFA with parameter-efficient finetuning (adapters, LoRA) or prompt-based methods for further gains.\n4) Investigate theoretical understanding of why stochastic channel-wise shifts help few-shot generalisation.\n5) Explore joint training where backbone remains mostly frozen but lightly updated together with FroFA to overcome current performance ceiling on large datasets.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Channel Importance Matters in Few-Shot Image Classification",
      "full_text": "Channel Importance Matters in Few-Shot Image Classiﬁcation Xu Luo 1 Jing Xu 2 Zenglin Xu 2 3 Abstract Few-Shot Learning (FSL) requires vision mod- els to quickly adapt to brand-new classiﬁcation tasks with a shift in task distribution. Understand- ing the difﬁculties posed by this task distribution shift is central to FSL. In this paper, we show that a simple channel-wise feature transformation may be the key to unraveling this secret from a channel perspective. When facing novel few-shot tasks in the test-time datasets, this transformation can greatly improve the generalization ability of learned image representations, while being ag- nostic to the choice of datasets and training al- gorithms. Through an in-depth analysis of this transformation, we ﬁnd that the difﬁculty of rep- resentation transfer in FSL stems from the severe channel bias problem of image representations: channels may have different importance in differ- ent tasks, while convolutional neural networks are likely to be insensitive, or respond incorrectly to such a shift. This points out a core problem of the generalization ability of modern vision systems which needs further attention in the future. Our code is available at https://github.com/ Frankluox/Channel_Importance_FSL. 1. Introduction Deep convolutional neural networks (Krizhevsky et al., 2012; He et al., 2016) have revolutionized computer vi- sion in the last decade, making it possible to automatically learn representations from a large number of images. The learned representations can generalize well to brand-new images. As a result, image classiﬁcation performance is close to humans on most benchmarks. However, in addi- tion to recognizing previously-seen categories, humans can quickly change their focus of image patterns in changing 1University of Electronic Science and Technology of China 2Harbin Institute of Technology Shenzhen 3Pengcheng Laboratory. Correspondence to: Xu Luo <frank.luox@outlook.com>, Zenglin Xu <xuzenglin@hit.edu.cn>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Animals in   miniImageNet Plants in   iNaturalist Plant Disease Images Discriminative Features Figure 1.Examples of task distribution shift. Different classiﬁ- cation tasks may focus on distinct discriminative information. Top: animals in miniImageNet with different plants as background. Mid- dle: plants as the main categories in iNaturalist. Bottom: Different types of plant diseases in the ﬁne-grained Plant Disease dataset. environments and recognize new categories given only a few observations. This fast learning capability, known as Few-Shot Learning (FSL), challenges current vision models on the ability to quickly adapt to novel classiﬁcation tasks that are different from those in training. This task distri- bution shift means that categories, domains of images or granularity of categories in new tasks deviate from those in the training tasks. Recent studies of few-shot image classiﬁcation have high- lighted the importance of the quality of learned image repre- sentations (Raghu et al., 2020; Doersch et al., 2020; Dhillon et al., 2020; Tian et al., 2020; Rizve et al., 2021), and also showed that representations learned by neural networks do not generalize well to novel few-shot classiﬁcation tasks when there is task distribution shift (Chen et al., 2021; Do- ersch et al., 2020; Agarwal et al., 2021). Thus it is crucial to understand how task distribution shift affects the generaliza- tion ability of image representations in few-shot learning. As shown in Figure 1, task distribution shift may lead to changes in discriminative image features that are critical to the classiﬁcation task at hand. For example, in the task of recognizing animals, a convolutional neural network trained on miniImageNet can successfully identify the discrimina- tive information related to animals. Although the repre- sentations learned by the network may encode some plant information (from image background), plants do not appear as a main category in miniImageNet and it may be insuf- ﬁcient for the network to distinguish various plants in a novel few-shot task sampled from the iNaturalist dataset. arXiv:2206.08126v2  [cs.CV]  20 Jun 2022Channel Importance Matters in Few-Shot Image Classiﬁcation Even when the network is well trained to recognize plants on iNaturalist, it is difﬁcult to be adapted to the novel task of identifying plant diseases due to the granularity shift, since the discriminative information now becomes the more ﬁne-grained lesion part of leaves. In this paper, we show that this difﬁculty encountered in few-shot learning leads to achannel bias problem in learned image representations (i.e., features). Speciﬁcally, in the layer after global pooling, different channels in the learned feature seek for different patterns (as veriﬁed in (Zhou et al., 2015; Bau et al., 2017)) during training, and the channels are weighted (in a biased way) based on their importance to the training task. However, when applied to novel few-shot classiﬁcation tasks, the learned image features usually do not change much or have inappropriately changed without adapting to categories in novel tasks. This bias towards training tasks may result in imprecise attention to image features in novel tasks. What leads to our discovery of the channel bias problem is a simple transformation function that we found in a mathe- matical analysis textbook. Applied to top of image represen- tations channel-wisely only at test time on the ﬂy, this trans- formation function can consistently and largely improve predictions for out-of-distribution few-shot classiﬁcation tasks, being agnostic to the choice of datasets and training algorithms (e.g., 0.5-7.5% average improvement over 5-way 5-shot tasks on 19 different test-time datasets, as shown in Table 1). Through analysis, we reveal the existence of channel bias problem, and show that this transformation rec- tiﬁes channel emphasis by adjusting the Mean Magnitude of Channels (MMC) of image representations over the tar- get task. Concretely, it serves as a smoothing function that suppresses channels of large MMC and largely ampliﬁes channels of small MMC. To further understand the channel bias problem, we derive an oracle adjustment on the MMC of image representations in binary classiﬁcation tasks. Such studies demonstrate that the channel bias problem exists in many different target tasks with various types of task distributions shift, and it becomes severe with the distribution shift expanding (as shown in Figure 6). In addition, through test-time shot analysis, we verify that the channel bias problem requires more attention in few-shot setting, while simple ﬁne-tuning can help address this problem in many-shot setting. 2. A Channel-wise Feature Transformation 2.1. Problem Setup In few-shot image classiﬁcation, a training set Dtrain is used at ﬁrst to train a neural network parametrized by θ, which will be evaluated on a series of few-shot classiﬁcation tasks constructed from the test-time dataset Dtest. Impor- tantly, there should be task distribution shift betweenDtrain and Dtest, which may include category shift, domain shift or granularity shift. Each evaluated N-way K-shot few- shot classiﬁcation task τ is constructed by ﬁrst sampling N classes from Dtest, and then sampling K and M images from each class to constitute a support setSτ and a query set Qτ, respectively. The support setSτ = {(xτ k,n,yτ k,n)}K,N k,n=1 consisting of K ×N images xτ k,n and corresponding la- bels yτ k,n from the N classes is used to construct a classi- ﬁer pθ(·|x,Sτ), which is further evaluated on the query set Qτ = {x∗τ m,n}M,N m,n=1. The evaluation metric is the average prediction accuracy on query set over all sampled few-shot classiﬁcation tasks. In order to evaluate on different types and degrees of task distribution shift, in the following experiments, we select a broad range of datasets for Dtrain and Dtest. For Dtrain, we choose (1) the train split of miniImageNet (Vinyals et al., 2016) that contains 38400 images from 64 classes; (2) the train split of ImageNet 1K (Russakovsky et al., 2015) containing more than 1M images from 1000 classes; (3) train+val split of iNaturalist 2018 (Horn et al., 2018), a ﬁne- grained dataset of plants and animals with a total of more than 450000 training images from 8142 classes. For Dtest, we choose the test split of miniImageNet, and all evaluation datasets of Meta-dataset (Triantaﬁllou et al., 2020), BSCD- FSL benchmark (Guo et al., 2020) and DomainNet (Peng et al., 2019), for a total of 19 datasets, to ensure adequate coverage of different categories, domains and task granular- ities. 2.2. Universal Performance Gains from a Test-time Simple Feature Transformation Let x∈RD denote an image and fθ(·) a feature extractor learned from the training set Dtrain. The l-th channel of the feature z = fθ(x) ∈Rd is deﬁned as the l-th dimension of z, i.e., {zi}d i=1 is the set of all dchannels. The simple transformation function φk : [0,+∞) →[0,+∞) that we consider is deﬁned as φk(λ) = { 1 lnk( 1 λ+1) , λ> 0 0, λ = 0 (1) where k >0 is a hyperparameter. At test time, we sim- ply use this function to transform each channel of image features, i.e., φk(z) = (φk(z1),...,φ k(zd)). (2) When applying this transformation, we transform all image features in the target classiﬁcation task regardless of whether they are in the support set or query set; any subsequent operation keeps unchanged. Note that this function can only be applied to features taking non-negative values, common in most convolutional neural networks using ReLU as theChannel Importance Matters in Few-Shot Image Classiﬁcation Table 1.Performance gains of the simple feature transformation on various training and testing datasets with a broad range of choices of network architectures and algorithms . The black values indicate the original accuracy, and the red values indicate the increase. Each running of evaluation contains 10000 5-way 5-shot tasks sampled using a ﬁxed seed, and the average accuracy is reported. The three groups of test-time datasets come from MetaDataset, BSCD-FSL benchmark and DomainNet, respectively. TrainData mini-train ImageNet iNaturalist Algorithm PN PN CE MetaB MetaOpt CE S2M2 PN CE MoCo-v2 CE Architecture Conv-4 Res-12 Res-12 Res-12 Res-12 SE-Res50 WRN Res-50 Res-50 Res-50 Res-50 Average mini-test 66.6+1.2 73.5+2.2 75.9+1.6 74.7+2.6 74.8+0.5 76.2+0.2 82.5+1.2 82.2-1.6 89.1-0.5 93.7+2.2 69.9+2.2 78.1+1.1 CUB 52.0+2.8 57.0+3.0 59.6+2.3 60.1+2.6 60.3+1.7 59.9+2.2 68.5+2.8 65.3+2.5 78.2+0.4 70.0+6.8 94.7+0.0 66.0+2.5 Textures 50.9+2.3 57.1+4.2 63.1+2.4 61.2+3.7 60.2+1.8 63.5+0.6 69.3+2.9 61.9+2.4 71.6+0.8 82.8+0.9 63.2+2.3 64.1+2.0 Trafﬁc Signs 52.6+2.1 64.8+2.2 65.6+1.4 67.3+1.5 67.1+4.9 62.2+2.9 69.6+3.1 64.0+2.2 67.2+3.5 68.4+8.8 60.5+4.0 64.4+3.3 Aircraft 32.1+0.9 31.3+1.6 34.7+1.9 34.7+2.3 35.6+2.4 38.2+2.0 40.5+4.7 38.4+1.7 46.6+2.5 34.5+8.8 42.1+2.5 34.0+2.9 Omniglot 61.0+10.0 77.6+7.8 86.9+3.7 81.6+7.9 78.0+9.9 89.9+2.3 85.9+7.4 76.4+2.9 88.6+5.3 74.5+15.8 83.8+9.0 80.4+7.5 VGG Flower 71.0+3.1 71.1+5.5 79.2+3.8 78.3+4.5 78.4+3.1 83.0+1.7 87.8+2.5 81.4+2.6 89.3+1.7 86.2+6.3 91.9+1.1 81.6+3.3 MSCOCO 52.0+1.2 58.2+1.1 59.0+0.7 58.0+1.6 58.4+0.1 57.1+0.5 63.5+0.1 61.3-0.5 64.3-0.4 71.4+1.4 50.4+1.9 59.4+0.7 Quick Draw 49.7+6.5 60.2+5.4 67.5+6.5 61.9+9.0 61.0+6.2 69.8+2.8 66.4+8.2 59.8+6.9 70.2+3.0 63.7+8.3 60.8+6.2 62.8+6.3 Fungi 48.5+1.5 49.0+3.7 52.2+3.3 51.5+4.0 54.6+1.9 55.2+0.5 61.6+3.8 58.5+1.3 65.1+1.1 60.2+9.2 70.0+1.8 56.9+2.9 Plant Disease 66.6+7.8 73.3+7.9 80.0+5.1 75.6+7.6 78.6+4.5 83.1+3.2 86.4+3.5 72.5+8.0 84.1+3.3 87.1+4.7 85.6+4.1 79.4+5.4 ISIC 38.5+1.6 36.8+2.9 40.4+1.0 38.8+1.7 39.5+2.3 37.7+3.9 40.5+5.5 39.5+4.0 37.8+3.6 43.2+2.8 39.0+4.3 39.2+3.1 EuroSAT 63.0+4.5 67.3+5.5 75.7+2.9 71.9+4.5 72.8+5.8 75.7+1.6 81.2+2.9 72.5+6.1 78.4+2.2 83.5+2.7 73.5+3.7 74.1+3.9 ChestX 22.9+0.2 23.0+0.5 24.1+0.3 23.5+0.5 24.5+0.4 23.6+0.2 24.2+0.9 23.2+0.3 24.2+0.8 25.4+0.9 23.9+0.1 23.9+0.5 Real 67.0+1.8 72.2+3.1 76.3+1.6 75.0+2.6 75.8+1.1 76.7+0.5 81.7+1.9 80.5+0.4 87.1-0.1 88.8+2.1 72.9+1.7 77.6+1.5 Sketch 42.6+2.9 45.3+5.0 51.1+2.6 50.2+3.4 50.6+2.0 50.9+2.4 56.8+4.1 53.1+1.5 63.2+2.5 63.9+5.8 51.9+1.4 52.7+3.1 Infograph 33.1+2.8 34.7+3.7 35.3+2.8 35.0+4.0 38.3+1.1 38.2+2.5 39.2+3.7 39.7+2.7 42.3+4.2 41.6+7.1 38.5+2.9 37.8+3.4 Painting 49.0+1.7 52.5+3.3 56.1+1.4 55.1+2.5 56.2+0.7 59.3+0.8 64.2+1.8 61.8-0.2 69.6+0.5 76.5+3.0 56.4+1.9 59.7+1.6 Clipart 47.5+3.6 49.7+4.8 55.5+3.1 54.9+4.3 56.4+2.6 60.4+2.3 63.0+4.3 60.9+1.8 72.7+1.5 67.4+7.0 58.4+2.2 58.8+3.4 activation function. We discuss one variant of the function dealing with features having negative values (e.g., networks with Leaky ReLU) in Appendix D. A plot of this function with various choices of kis shown in Figure 2. Table 1 shows the performance gains brought by this trans- formation on 5-way 5-shot FSL tasks. We test the transfor- mation on representations trained with different algorithms, including (1) the conventional training methods including cross-entropy (CE) and the S2M2 algorithm (Mangla et al., 2020), (2) meta-learning methods including ProtoNet (Snell et al., 2017) (PN), Meta-baseline (Chen et al., 2021) and MetaOpt (Lee et al., 2019), and (3) MoCo-v2 (He et al., 2020), a unsupervised contrastive learning method. We test these methods with various backbone networks: Conv- 4 (Vinyals et al., 2016) and four variants of ResNet (He et al., 2016) including ResNet-12 (Oreshkin et al., 2018), WRN- 28-10 (Zagoruyko & Komodakis, 2016), ResNet-50 and SE-ResNet50 (Hu et al., 2018). We replace Leaky ReLU with ReLU in ResNet-12 to obtain positive features (cause of performance degradation in Table 1). At test-time, we use the Nearest-Centroid Classiﬁer (Snell et al., 2017) for CE, linear probing for S2M2 and MoCo-v2, and for meta- learning algorithms we use their own test-time classiﬁer. Training and evaluation details can be found in Appendix B. The result shows how this simple feature transformation substantially improves few-shot learning across various al- gorithms, datasets and architectural choices, with a ﬁxed hyperparameter k= 1.3 (We show how performance varies with different choices of k in Appendix C). The only ex- ception happens when the test-time task distribution is very 0.00 0.05 0.10 0.15 0.20 0.25 x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8y k=0.5 k=1.3 k=2.0 k=5.0 Figure 2.The simple transformation function φk with various choices of k. similar to a subset of training distribution: training the su- pervised models on ImageNet and testing on miniImageNet, MSCOCO, Real or Painting1, or training on iNaturalist and testing on CUB. Is this transformation useful only if there exists task distribution shift between training and testing? To verify this, we train a CE model on each of ten datasets and test on 5-way 5-shot tasks sampled from each dataset. When testing on the training dataset, we evaluate on images not included in training. The results shown in Figure 3 clearly give evidence that the transformation is beneﬁcial only to few-shot classiﬁcation with task distribution shift—the per- formance is improved only when test-time task distribution deviates from training, and this distribution shift includes domain shift (e.g., from Sketch to QuickDraw), category 1There are a lot of painting-style images in ImageNet. Con- trastive learning (MoCo) can be seen as an inﬁnitely ﬁne-grained classiﬁcation task, thus having a relatively large different task distribution shift from training to testing, even on the same dataset.Channel Importance Matters in Few-Shot Image Classiﬁcation -0.4 +2.3 +2.4 +1.4 +5.1 +1.0 +2.9 +2.6 +6.5 +3.3  +0.5 -0.2 +1.0 +2.4 +2.4 +2.3 +0.8 +1.0 +4.3 +1.1  +0.2 +0.1 -0.2 +0.1 +0.3 +0.2 +0.3 +0.1 +0.1 +0.0  +2.2 +2.3 +1.6 +0.0 +3.5 +2.6 +2.5 +2.0 +4.5 +1.7  +2.0 +2.1 +1.0 +0.6 +0.0 +1.0 +3.7 +0.6 +7.6 +1.6  +0.1 +0.6 +0.3 -0.2 +2.0 -0.8 +1.0 +0.1 +0.8 +0.5  +0.5 +0.2 +0.7 +0.5 +4.0 +0.0 -0.1 +1.5 +4.4 +0.6  +2.4 +2.1 +2.6 +5.0 +4.2 +1.8 +6.9 -0.7 +2.0 +2.5  +2.2 +0.9 +0.2 +1.2 +5.3 +1.3 +1.4 +1.8 -0.5 +1.4  +3.1 +5.5 +3.6 +5.4 +3.6 +4.3 +6.4 +2.8 +5.7 -0.9  mini  CUB  T exture  T S  PlantD  ISIC  ES A T  Sk etch  QDr a w  Fungi  mini CUB T exture T S PlantD ISIC ES A T Sk etch QDr a w Fungi  T r ain  T est  Figure 3.In-distribution (diagonal) and out-of-distribution (off- diagonal) performance gains of the simple channel-wise transfor- mation on representations trained with CE. When the test-time dataset equals the training dataset (diagonal), the categories of images remain the same but test-time images are unseen during training (as in conventional classiﬁcation). shift (e.g., from Plant Disease to Fungi) and granularity shift (e.g., from iNaturalist to Plant Disease in Table 1). 3. The Channel Bias Problem In this section, we analyze the simple transformation, which leads us to discover the channel bias problem of visual rep- resentations. Given the transformation function described in Eq.(1), it can be ﬁrst noticed that φ′ k(λ) >0, lim λ→0+ φ′ k(λ) = +∞, ∃t> 0, s.t. ∀λ∈(0,t),φ′′ k(λ) <0, (3) where tis a large value for most k, relative to the magni- tudes of almost all channels (e.g., when k= 1.3, t≈0.344, while most channel values are less than 0.3). The positive- ness of the derivative ensures that the relative relationship between channels will not change, while the negative sec- ond derivative narrows their gaps; the inﬁnite derivative near zero pulls up small channels by a large margin, i.e., limλ→0+ φk(λ) λ = +∞. See Appendix E for the necessity of all these properties. A clear impact of these properties on features is to make channel distribution smooth: suppress channels with high magnitude, and largely amplify channels with low magnitude. This phenomenon is clearly shown in Figure 4, where we plot mean magnitudes of all 640 feature channels on miniImageNet and PlantDisease, with red ones being the original distribution, blue ones being the trans- formed distribution. The transformed distribution becomes more uniform. Intuitively, different channels have high responses to dif- ferent features, and a larger Mean Magnitude of a Channel 0 100 200 300 400 500 600 miniImageNet 0.00 0.02 0.04 0.06 0.08 0.10 0.12magnitude before after 0 100 200 300 400 500 600 Plant Disease 0.00 0.05 0.10 0.15 0.20 0.25magnitude before after Figure 4.Mean magnitudes of feature channels before and af- ter applying the simple transformation. The feature extractor is trained using PN on the training set ofminiImageNet. Left: test set of miniImageNet. Right: The Plant Disease dataset. The change of relative magnitude is due to different variances of channels. (MMC) implies that the model puts more emphasis on this channel, hoping that this channel is more important for the task at hand. Combining the analysis above with previous experiment results, we conjecture that the MMC of repre- sentations should change when testing on novel tasks with a shift in distribution. This meets our intuition that different tasks are likely to be characterized by distinct discriminative features, as shown in the examples of Figure 1. 3.1. Deriving the Oracle MMC of Any Binary Task We now wonder how much the MMC estimated by neu- ral networks in a task deviates from the best MMC or channel importance of that task. To achieve this goal, we ﬁrst derive the optimal MMC for any classiﬁcation task by multiplying a positive constant to each channel of fea- tures, given that we know the ﬁrst-order and second-order statistics of features. For convenience, we consider the binary classiﬁcation problem. Speciﬁcally, let D1, D2 de- note probability distributions of two classes over feature space Z ⊂[0,+∞)d, and z1 ∼ D1, z2 ∼ D2 denote samples of each class. Let µ1,µ2 and Σ1,Σ2 denote their means and covariance matrices, respectively. We assume that the channels of features are uncorrelated with each other, i.e., there exist σ1,σ2 ∈[0,+∞)d, s.t. Σ1 = diag(σ1), Σ2 = diag(σ2). The original MMC of the binary task is deﬁned as ωo = (µ1 + µ2)/2. We assume that the MMC after adjustment is ω∈[0,+∞)d. Let ˜z1, ˜z2 denote stan- dadized version of z1, z2 that have unit MMC, i.e., ˜z1,l = z1,l/ωo l,˜z2,l = z2,l/ωo l ⇒(˜µ1,l + ˜µ2,l)/2 = 1,∀l ∈[d] ([d] is equivalent to {1,2,...,d }). A simple approach to adjust MMC to ωis to transform features to ω⊙˜z1 and ω⊙˜z2 respectively, where⊙denotes the hadamard product. Here, we consider a metric-based classiﬁer. Speciﬁcally, a standardized feature ˜z is classiﬁed as the ﬁrst class if ||ω⊙(˜z−˜µ1)||2 <||ω⊙(˜z−˜µ2)||2 and otherwise the second class. This classiﬁer is actually the Nearest-Centroid Classiﬁer (NCC) (Snell et al., 2017) with accurate centroids. Assume that two classes of images are sampled equal times,Channel Importance Matters in Few-Shot Image Classiﬁcation 0.0 0.1 0.2 0.3 0.4 0.5 0.6 None 0.1 0.0 0.1 0.2 0.3 0.4 0.5 queries of class 1. ACC: 52.38% queries of class 2. ACC: 48.69% support 0.0 0.2 0.4 0.6 0.8 1.0 Simple 0.2 0.0 0.2 0.4 0.6 0.8 1.0 queries of class 1. ACC: 69.84% queries of class 2. ACC: 66.80% support 0.2  0.1  0.0 0.1 0.2 Oracle 0.0 0.1 0.2 0.3 0.4 0.5 queries of class 1. ACC: 88.88% queries of class 2. ACC: 80.08% support Figure 5.Visualization of two channels of image features in two classes of Plant Disease. The feature extractor is trained using PN on miniImageNet. We visualize a one-shot task with only two channels available for classiﬁcation. The plot with “None” shows the original channels. The plots with “Simple” and “Oracle” show channels adjusted by the simple and oracle transformation. The per-class accuracy is calculated as the proportion of samples correctly classiﬁed by the classiﬁcation boundary in each class. then the expected misclassiﬁcation rate of this classiﬁer is R= 1 2[Pz1∼D1 (||ω⊙(˜z1 −˜µ1)||2 >||ω⊙(˜z1 −˜µ2)||2) +Pz2∼D2 (||ω⊙(˜z2 −˜µ2)||2 >||ω⊙(˜z2 −˜µ1)||2)]. (4) The following theorem gives an upper bound of the mis- classiﬁcation rate and further gives the oracle MMC of any given task. Proposition 3.1. Assume that µ1,l ̸= µ2,l and σ1,l+σ2,l > 0 hold for any l∈[d], then we have R≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . (5) To minimize this upper bound, the adjusted oracle MMC of each channel ωl should satisfy: ωl ∝|µ1,l −µ2,l| σ1,l + σ2,l . (6) Proofs are given in Appendix A. We here use the word “oracle” because it is derived using the class statistics of the target dataset, which is not available in few-shot tasks. This derived MMC has an intuitive explanation: if the difference between the means of features from two classes is large but the variances of features from two classes are both small, the single channel can better distinguish the two classes and thus should be emphasized in the classiﬁcation task. In fact, if we further assume x1,l and x2,l are Gaussian-distributed and consider only using the l-th channel for classiﬁcation, then the misclassiﬁcation error for the i-th class (i= 1,2) is a strictly monotonically decreasing function of |µ1,l − µ2,l|/σi,l. Table 2 shows the performance improvement over the simple feature transformation when adjusting the MMC to derived oracle one in each of the real few-shot binary classiﬁcation tasks. For every sampled binary task in a dataset, we calcu- late the oracle adjustment based on Eq. (6); see Appendix F.1 for details. The oracle MMC improves performance on all datasets, and always by a large margin. Note that although the oracle MMC is derived using a metric-based classiﬁer, it can also help a linear classiﬁer to boost perfor- mance, which will be further discussed in Section 4. The large performance gains using the derived channel impor- tance indicate that the MMC of features on new test-time few-shot task indeed has a large mismatch with ground-truth channel importance. To obtain a better understanding, in Figure 5, we visualize image representations of two classes when transferred from miniImageNet to Plant Disease. The two exhibited classes are apples with Apple Scab and Black Rot diseases, respec- tively. We visualize 2 out of 640 channels in the features, shown as the x-axis and y-axis in the ﬁgure. We select these channels by ﬁrst selecting a channel that requires a large sup- pression of MMC (x-axis), and then a channel that requires a large increase (y-axis). As seen, the x-axis channel has a large intra-class variance ( the variances are 0.13 and 0.11 in two classes on the x-axis channel, compared to 0.03 and 0.08 on the y-axis channel) and a small class mean differ- ence (about 0.03, compared to 0.13 on the y-axis channel), so it is hard to distinguish two classes through this chan- nel. By adjusting the mean magnitude of this channel, the simple transformation and oracle adjustment decrease the intra-class variance of the x-axis channel, and so decrease its inﬂuence on classiﬁcation. Similarly, the y-axis channel can better distinguish two classes due to its relatively larger class mean difference and smaller intra-class variance, so the inﬂuence of the y-axis channel should be strengthened. 3.2. Analysis of Channel Importance Next, we take the derived oracle MMC as an approxima- tion of the ground-truth channel importance, and use it to observe how the simple transformation works, as well as how much the channel emphasis of neural networks de- viates from the ground-truth channel importance of tasks in each test-time dataset. We deﬁne MMC of a dataset D as the average l1-normalized MMCs over all possibleChannel Importance Matters in Few-Shot Image Classiﬁcation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Original Simple Oracle 0.00000.00250.00500.00750.01000.01250.01500.01750.0200 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle 0.000 0.002 0.004 0.006 0.008 0.010 0.012 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007MMC after transformation Original Simple Oracle 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 0.030 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010MMC after transformation Original Simple Oracle 0.00 0.01 0.02 0.03 0.04 0.05 MMC before transformation 0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014MMC after transformation Original Simple Oracle 0.000 0.005 0.010 0.015 0.020 0.025 MMC before transformation 0.000 0.002 0.004 0.006 0.008MMC after transformation Original Simple Oracle miniImageNet CUB T exture Traffic Sign Plant Disease ISIC EuroSAT Sketch QuickDraw Fungi Figure 6.Visualization of MMC of ten datasets ωD before and after the use of simple and oracle transformation. In each plot, a point represents a channel, and the x-axis and y-axis represent the MMC before and after transformation respectively, averaged over all possible binary tasks in the corresponding dataset. For comparison, we also plot the line y= xrepresenting the “None” scenario where none of the transformations are applied to features. The feature extractor is trained using PN on miniImageNet. Table 2.The performance gains of the oracle MMC on 5-shot binary classiﬁcation tasks on various datasets. The derived MMC improves the few-shot performance of both metric and non-metric test-time methods: Nearest-Centroid Classiﬁer (NCC) and Linear Classiﬁer (LC). Algorithm Classiﬁer Transformation mini CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Avg PN NCC None 90.5 80.6 80.6 85.1 89.2 65.7 86.5 71.9 82.4 74.6 80.7 Simple 91.3 82.4 83.1 85.8 93.0 68.6 89.2 75.2 85.1 77.2 83.1 Oracle 93.1 88.7 87.2 92.4 95.6 69.1 91.5 81.2 89.4 88.4 87.7 S2M2 LC None 94.0 87.1 85.7 88.7 95.0 68.7 93.5 78.7 85.5 82.8 86.0 Simple 94.4 88.3 87.3 91.2 96.4 72.2 93.8 81.0 89.2 84.5 87.8 Oracle 96.3 94.0 90.7 96.1 98.3 72.6 95.2 87.0 93.0 93.3 91.7 binary tasks in that dataset. Speciﬁcally, suppose in one dataset Dthere are Cclasses, and let ωij denote the MMC in the binary task discriminating the i-th and j-th class. ωij = ωij/||ωij||1 normalizes the MMC, such that the l-th component of the vector ωij represents the percent- age of channel emphasis on the l-th channel. Then the MMC of D is deﬁned as ωD = ∑ 1≤i<j≤C ωij, which gives average percentages of channel emphasis over all bi- nary tasks. We visualize the oracle MMC, compared with MMC adjusted by the simple transformation and the orig- inal MMC of each dataset in Figure 6. A point in each ﬁgure represents a channel of the image features, with x and y axis being its MMC of that dataset before and af- ter transformation, respectively. To obtain a more precise understanding, we also want to quantitatively measure dif- ference between different MMCs or image features. To achieve this, given a distance measure d(·,·) (not necessar- ily a metric), we deﬁne three levels of distances: (1) dataset- level distance d(ωDa,ωDb) that measures the distance be- tween MMCs of two datasets (or the same dataset with different transformations); (2) in-dataset task-level distance C(C+1) 2 ∑ 1≤i<j≤C d(ωa ij,ωb ij) that measures average dis- tance between MMCs of all tasks from a dataset obtained by different feature transformations, and (3) image-level distance 1 |D| ∑|D| i=1 d(zia,zi b), a more ﬁne-grained one that measures average distance between all l1-normalized image features zia,zi b of dataset Dunder different feature transfor- mations. For dataset-level distance, we adopt the normalized mean square difference d(x,y) = 1 d ∑d l=1(xl −yl)2/x2 l, since it treats each channel equally w.r.t. to the scale and is sensitive to high deviation. However, for task-level and image-level distance, we choose the mean square differ- ence d(x,y) = 1 d ∑d l=1(xl −yl)2 instead to avoid high variations caused by a single task or image feature that has channels with very small magnitude; see Appendix F.2 for details. We calculate the distance (1) between the original MMC of the training set (mini-train) and each test set, to see how much neural networks change channel emphasis when faced with novel tasks, (2) between the original and oracle MMC to see how much the changed emphasis is biased on each dataset, and (3) between the simple and oracle MMC of each dataset to see how much the simple transformation alleviates the problem. The results are shown in Table 3. Neural networks are overconﬁdent in previously learned channel importance. Comparing the ﬁrst and sec- ond rows in Table 3, we can see that the adjustment of MMC that the network made on new tasks is far from enough: the distance of original MMCs between train and test set (the ﬁrst row) is much smaller than that between original and oracle MMCs on the test set. This suggests channels that areChannel Importance Matters in Few-Shot Image Classiﬁcation Table 3.Three levels of distance between different MMCs or l1-normalized image features. The ﬁrst row shows the dataset-level distance between the original MMC of the training set (mini-train) and each test set; the second row shows the dataset-level distance between the original and oracle MMCs on each dataset; rows 3-6 show the task-level and image-level distances (both ampliﬁed by 106 times) between MMCs obtained by simple and oracle transformation or between original MMCs (None) and the MMCs obtained by oracle transformation. The feature extractor is trained using PN on the training set of miniImageNet (mini-train). Test dataset Level Compared dataset Trans. mini-train mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Dataset Train v.s. Test None - 0.18 1.56 0.88 1.13 1.54 2.28 1.30 1.01 1.58 0.79 Test None v.s. Oracle 0.42 0.72 3.60 1.78 4.04 3.92 3.47 5.62 4.26 3.37 3.87 Task Test None v.s. Oracle 3.60 4.04 3.53 4.13 3.68 4.09 3.15 4.24 5.31 4.22 3.38 Simple v.s. Oracle 3.54 3.80 2.93 3.62 3.22 3.65 2.59 3.71 4.35 3.18 2.78 Image Test None v.s. Oracle 10.52 10.65 11.53 25.20 9.88 9.75 13.04 16.33 27.36 13.46 11.39 Simple v.s. Oracle 7.98 8.14 8.69 16.74 7.06 7.34 8.43 11.22 19.50 9.32 8.71 mixing bowlAfrican hunting dogmalamute electric guitarblack-footed ferret Query OriginalOracle Figure 7.Examples of Grad-Cam (Selvaraju et al., 2017) class activation maps of query samples using PN before and after the oracle adjustment of MMC on binary 5-shot tasks sampled from the test set of miniImageNet. important to previously learned tasks are still considered by the neural network to be important for distinguishing new tasks, but in fact, the discriminative channels are very likely to change on new tasks. This can be also observed from each plot in Figure 6, where the oracle MMC pushes up channels having small magnitudes and suppresses channels having large magnitudes. The magnitudes of a large number of small-valued channels are ampliﬁed 10×times or more by the oracle MMC, while large-valued channels are sup- pressed 5×times or more, and in most datasets originally large-valued channels eventually have similar channel im- portance to those of originally small-valued channels. The simple transformation, although not being perfect, also reg- ularizes channels due to its smoothing property discussed in Section 3. We call this problem the channel bias problem. The channel bias problem diminishes as task distribu- tion shift lessens. The channel patterns in Figure 6 on all datasets look similar, except for miniImageNet, whose over- all pattern is close to the liney= xrepresenting the original MMCs. There does not exist dominant channels when test- ing on miniImageNet (The maximum scale of channels is within 0.006), while on other datasets there are channels where the neural network assigns much higher but wrong MMCs which deviate far away from the y= xline. In the second row of Table 3, we can also see that the distance between the original and oracle MMCs on miniImageNet, especially on mini-train that the model trained on, is much smaller than that on other datasets2. Since mini-test has a similar task distribution with mini-train, we can infer that the channel bias is less serious on datasets that have similar task distribution. This explains why in Table 1 and Figure 3 the simple transformation gets a relatively low improve- ment when trained on mini-train and tested on mini-test, and even degrades performance when trained and tested on tasks sampled from the same task distribution. The channel bias problem distracts the neural network from new objects. In Figure 7, we compare some class activation maps before and after the oracle adjustment of MMC. We observe that adjusting channel importance helps the model adjust the attention to the objects responsible for classiﬁcation using a classiﬁer constructed by only a few support images. This matches observation in previous work (Zhou et al., 2015; Bau et al., 2017) that different chan- nels of image representations are responsible for detecting different objects. The task distribution shift makes mod- els confused about which object to focus on, and a proper adjustment of channel emphasis highlights the objects of interest. The simple transformation pushes MMCs towards the oracle ones. Observing Figure 6, it is evident that the sim- ple transformation pushes MMCs towards the oracle ones (compared with the line y = x), albeit not perfectly. This observation is further conﬁrmed by the None v.s. Oracle and Simple v.s. Oracle comparison of ﬁne-grained task- level and image-level distance shown from the third row to the last row of Table 3. On each of the test-time dataset, the distance between MMCs obtained by simple and oracle transformation is smaller than that bewteen original MMCs 2Unnormalized mean square difference ignores critical changes of small-valued channels. This is why we do not observe simi- lar phenomenon from the task and image-level difference; see Appendix F.2 for detailed explanations.Channel Importance Matters in Few-Shot Image Classiﬁcation 100 101 102 Number of Shots 60 70 80 90ACC(%) miniImageNet NCC LC Fine-tune 100 101 102 Number of Shots 6 4 2 0 2 (ACC)% miniImageNet NCC LC Fine-tune Figure 8.Shot analysis of miniImageNet. Left: performance of dif- ferent test-time methods. Right: performance gains of the simple transformation using different test-time methods. and the MMCs obtained by oracle transformation. 4. Analysis of the Number of Shots We have seen that the channel bias problem is one of the main reasons why image representations cannot generalize well to new few-shot classiﬁcation tasks. However, two questions remain to be answered: (1) we are still unclear whether this problem is only tied with few-shot image clas- siﬁcation. In all previous experiments, we tested on tasks where only 5 labeled images per class are given. What will happen if we have more training examples in the new task? (2) How much will different test-time methods be inﬂuenced by the channel bias problem? If we have the opportunity to ﬁne-tune the learned representations, will the proposed simple transformation still work? In order to give answers to these questions, we conduct shot analysis experiments on three representative test-time meth- ods that are adopted or are the basis of most mainstream few-shot classiﬁcation algorithms: (1) The metric-based method Nearest-Centroid Classiﬁer (NCC) presented in Pro- toNet, which ﬁrst average image features of each class in the support set to form class centroids and then assign query features to the class of the nearest centroid; (2) Linear Clas- siﬁer (LC), which trains a linear layer upon learned image features in the support set, and (3) Fine-tuning, which ﬁne- tunes the feature extractor together with the linear layer using images in the support set. The feature extractor is trained using the state-of-the-art S2M2 algorithm on the training set of miniImageNet, and we test it on the test set of miniImageNet using the above three test-time methods with different numbers of labeled images in each class of the support set. The results are shown in Figure 8. We show the original accuracy of all methods, as well as the impact of simple transformation on the performance. We ﬁrst take a look at the right plot, which shows the im- pact of the simple transformation on all the methods. The performance gains on NCC and LC stay at a relatively high value for all tested shots, which is up to 400 labeled images per class. This indicates that the channel bias problem is not only linked to few-shot settings, but also exists in many-shot settings. However, when we have abundant support images, we have an alternative choice of ﬁne-tuning the feature ex- tractor directly. Fine-tuning methods have the potential to fully resolve the channel bias problem by directly modifying the image representation and rectifying the channel distribu- tion. The right ﬁgure shows that the simple transformation does not improve ﬁne-tuning methods, so indeed the channel bias problem has been largely alleviated. In the left ﬁgure, the ﬁne-tuning method exhibits its advantages in many-shot setting, but falls short in few-shot settings. Therefore, we can infer that the channel bias problem exists only in the few-shot setting where freezing the feature extractor and building the classiﬁer on learned features becomes a better choice. We also have another notable observation. While the perfor- mance gain of simple transformation on NCC stays around a ﬁxed value, the performance gain on LC decreases with the increase of shots. Thus the channel bias problem is alle- viated to some extent in many-shot settings. This is because more labeled data tells the linear classiﬁer sufﬁcient infor- mation about intra-class variance of data, making it possible to adjust MMC by modifying the scale of each row of the linear transformation matrix. So Linear Classiﬁer can stably increase its performance when more labeled data comes in, until no more linear separation can be achieved, and also the time ﬁne-tuning should get into play to adjust the feature space directly. 5. Discussion and Related Work Task distribution shift. Task distribution shift may hap- pen when a model faces category shift, domain shift or granularity shift. Conventional benchmarks of FSL only consider category shift, i.e. the categories are disjoint for training and testing, such as miniImageNet (Vinyals et al., 2016) and CIFAR-FS (Bertinetto et al., 2019). In cross- domain few-shot learning (Chen et al., 2019), domain shift exists between train and test-time tasks, and several later benchmarks such as BSCD-FSL (Guo et al., 2020) and Meta-dataset (Triantaﬁllou et al., 2020) both target at such setting. Recently, the shift of granularities of categories has been considered as another type of task distribution shift, and is also called Coarse-to-Fine Few-Shot (C2FS) Learn- ing (Luo et al., 2021a; Bukchin et al., 2021; Yang et al., 2021a), which trains a model on coarse-labeled images and tests on few-shot tasks that aim at distinguishing between ﬁne-grained subclasses of training categories. Our work reveals that all three types of task distribution shift have a similar phenomenon of channel bias problem. The inﬂuence of task distribution shift on FSL has been ﬁrstly studied in (Doersch et al., 2020). They ﬁnd that the representation constructed by meta-learning algorithms can- not capture useful discriminative information outside of the training categories. They solve this problem by highlightingChannel Importance Matters in Few-Shot Image Classiﬁcation the crucial spatial information for classiﬁcation, using a cross-attention module between support and query features in new tasks. The algorithm COSOC (Luo et al., 2021b) also considers ﬁltering task-irrelevant spatial information, but it achieves it more directly. They identify image back- ground as harmful information in both training and testing and design a method to remove the background, in order to reduce the difﬁculty of category transfer. The perspective of our work is different, not focusing on discriminative spatial positions of features, but orthogonally taking inspections on discriminative channel information of features. Test-time methods for FSL. The presented three methods in Section 4 represent three types of mainstream algorithms in FSL. (1) Finetuning—Optimization-based algorithms, originated mainly from MAML (Finn et al., 2017), opti- mizes both the learned feature extractor and classiﬁer to- gether at test-time. Most work that fall into this type use meta-learning to train the network (Rusu et al., 2019; Ra- jeswaran et al., 2019; Zintgraf et al., 2019; Park & Oliva, 2019). When training adopts conventional supervised ap- proaches, the method turns to resemble transfer learning approaches, and is adopted in (Dhillon et al., 2020) and BiT (Kolesnikov et al., 2020). In the experiments of Section 4, we notice that in few-shot settings, although alleviating channel bias problem, ﬁne-tuning method performs gener- ally worse and may require very different hyperparameters for different test-time datasets to avoid overﬁtting, which is impossible to achieve in a realistic few-shot scenario, thus we believe ﬁnetuning would not be the best test-time choice. (2) NCC—metric-based algorithms (Vinyals et al., 2016; Snell et al., 2017; Zhang et al., 2020; Hou et al., 2019; Do- ersch et al., 2020) that aim at learning a well-shaped feature space equipped with a distance metric for comparing the sim- ilarity of images, on which the test-time prediction depends. Metric-based methods, as we have shown, beneﬁt from in- ductive bias given by the metric and thus are widely adopted in state-of-the-art algorithms. (3) LC—most conventionally trained methods adopt LC as the test-time methods (Chen et al., 2019; Mangla et al., 2020; Tian et al., 2020; Liu et al., 2020; Rizve et al., 2021), and two meta-learning algorithms MetaOpt (Lee et al., 2019) and ANIL (Raghu et al., 2020) use LC in both training and testing. The importance of a good quality of image representation is mainly ﬁgured out from this line of work. Other feature transformations in FSL. LFT (Tseng et al., 2020) introduces learnable channel-wise feature transforma- tions into training for cross-domain few-shot learning. The transformations are put inside backbone, instead of on top of representations, and are only used at train time, learned in a learning-to-learn fashion using multiple domains of datasets. Z-score transformation upon image representations is introduced in (Fei et al., 2021) for solving the hubness problem of image representations in FSL. CCF (Xu et al., 2021) proposes a variant of variational autoencoder to trans- form features, which utilizes category relationship between training and test-time classes to rectify the feature distribu- tions. Feature-wise linear modulation (FiLM) (Perez et al., 2018) that turns scaling and shifting coefﬁcients in batch normalization layer (seen as parameters of a linear feature transformation) into dataset- or task-dependent learnable parameters has been adopted in several FSL algorithms (Ore- shkin et al., 2018; Requeima et al., 2019; Triantaﬁllou et al., 2021; Li et al., 2022). The core idea of these methods is to only tune the FiLM modules at test time in order to reduce overﬁtting. Thus these methods in some sense belong to ﬁnetuning-based methods, and have the potential to perform better than vanilla ﬁnetuning in low-shot settings. Contrary to our work, all methods discussed above do not discover or target at the channel bias problem. The most relevant method to our paper may be ConFeSS (Das et al., 2022), a framework that masks task-irrelevant channels in image representations at test time for cross-domain few-shot learn- ing. Our work shows that the success of ConFeSS may be attributed to alleviating the channel bias problem by aban- doning overconﬁdent channels when transferred to novel tasks. 6. Conclusion In this paper, we reveal the channel bias problem in few-shot image classiﬁcation. The problem can be alleviated by a simple channel-wise feature transformation presented in this work. This transformation, used at test-time without adding any computation overhead, can be applied to most pre- trained convolutional neural networks and few-shot learning algorithms. We show it serves as prior knowledge that regu- larizes the channel distribution of features. Further analysis, including a derivation of the oracle MMC adjustment, ana- lyzes comprehensively the channel bias problem. We hope that the channel bias problem revealed in this work, along with analysis of different test-time methods, can provide the community with a better understanding of task distribution shift and representation transfer in few-shot classiﬁcation, which may in turn help produce better algorithms. Acknowledgments Special thanks to Qi Yong for providing indispensable spir- itual support for the work. We also would like to thank all reviewers for very constructive comments that help us improve the paper. This work was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204), and a key program of fun- damental research from Shenzhen Science and Technology Innovation Commission (No. JCYJ20200109113403826).Channel Importance Matters in Few-Shot Image Classiﬁcation References Agarwal, M., Yurochkin, M., and Sun, Y . On sensitivity of meta-learning to support data. In Advances in Neural Information Processing Systems, 2021. Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541–6549, 2017. Bertinetto, L., Henriques, J. F., Torr, P. H. S., and Vedaldi, A. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations, 2019. Bukchin, G., Schwartz, E., Saenko, K., Shahar, O., Feris, R., Giryes, R., and Karlinsky, L. Fine-grained angular contrastive learning with coarse labels. In IEEE Confer- ence on Computer Vision and Pattern Recognition , pp. 8730–8740, 2021. Cantelli, F. P. Sui conﬁni della probabilita. In Atti del Congresso Internazionale dei Matematici: Bologna del 3 al 10 de settembre di 1928, pp. 47–60, 1929. Chen, W., Liu, Y ., Kira, Z., Wang, Y . F., and Huang, J. A closer look at few-shot classiﬁcation. In International Conference on Learning Representations, 2019. Chen, Y ., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta- baseline: exploring simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9062–9071, 2021. Das, D., Yun, S., and Porikli, F. Confess: A framework for single source cross-domain few-shot learning. In International Conference on Learning Representations, 2022. Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto, S. A baseline for few-shot image classiﬁcation. In Inter- national Conference on Learning Representations, 2020. Doersch, C., Gupta, A., and Zisserman, A. Crosstransform- ers: spatially-aware few-shot transfer. In Advances in Neural Information Processing Systems, 2020. Fei, N., Gao, Y ., Lu, Z., and Xiang, T. Z-score normalization, hubness, and few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 142–151, 2021. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning, pp. 1126–1135, 2017. Guo, Y ., Codella, N., Karlinsky, L., Codella, J. V ., Smith, J. R., Saenko, K., Rosing, T., and Feris, R. A broader study of cross-domain few-shot learning. In European Conference on Computer Vision, pp. 124–141, 2020. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016. He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729– 9738, 2020. Horn, G. V ., Aodha, O. M., Song, Y ., Cui, Y ., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. J. The inaturalist species classiﬁcation and detection dataset. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8769–8778, 2018. Hou, R., Chang, H., Ma, B., Shan, S., and Chen, X. Cross attention network for few-shot classiﬁcation. In Advances in Neural Information Processing Systems , pp. 4005– 4016, 2019. Hu, J., Shen, L., and Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132–7141, 2018. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In European Conference on Computer Vision, pp. 491–507, 2020. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1106–1114, 2012. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta- learning with differentiable convex optimization. InIEEE Conference on Computer Vision and Pattern Recognition, pp. 10657–10665, 2019. Li, W.-H., Liu, X., and Bilen, H. Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7161–7170, 2022. Liu, B., Cao, Y ., Lin, Y ., Li, Q., Zhang, Z., Long, M., and Hu, H. Negative margin matters: Understanding margin in few-shot classiﬁcation. In European Conference on Computer Vision, pp. 438–455, 2020.Channel Importance Matters in Few-Shot Image Classiﬁcation Luo, X., Chen, Y ., Wen, L., Pan, L., and Xu, Z. Boosting few-shot classiﬁcation with view-learnable contrastive learning. In IEEE International Conference on Multime- dia and Expo (ICME), pp. 1–6, 2021a. Luo, X., Wei, L., Wen, L., Yang, J., Xie, L., Xu, Z., and Tian, Q. Rectifying the shortcut learning of background for few-shot learning. In Advances in Neural Information Processing Systems, 2021b. Mangla, P., Singh, M., Sinha, A., Kumari, N., Balasubra- manian, V . N., and Krishnamurthy, B. Charting the right manifold: Manifold mixup for few-shot learning. InIEEE Winter Conference on Applications of Computer Vision, pp. 2207–2216, 2020. Oreshkin, B. N., L ´opez, P. R., and Lacoste, A. TADAM: task dependent adaptive metric for improved few-shot learning. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 719–729, 2018. Park, E. and Oliva, J. B. Meta-curvature. In Advances in Neural Information Processing Systems, pp. 3309–3319, 2019. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V ., et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, pp. 2825–2830, 2011. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1406–1415, 2019. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. Film: Visual reasoning with a general con- ditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. In International Conference on Learning Representations, 2020. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems , pp. 113–124, 2019. Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. Fast and ﬂexible multi-task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019. Rizve, M. N., Khan, S. H., Khan, F. S., and Shah, M. Explor- ing complementary strengths of invariant and equivariant representations for few-shot learning. In IEEE Confer- ence on Computer Vision and Pattern Recognition , pp. 10836–10846, 2021. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M. S., Berg, A. C., and Li, F. Imagenet large scale visual recognition challenge. In IJCV, volume 115, pp. 211–252, 2015. Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. Meta-learning with latent embedding optimization. In International Confer- ence on Learning Representations, 2019. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explana- tions from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017. Snell, J., Swersky, K., and Zemel, R. S. Prototypical net- works for few-shot learning. In Advances in Neural In- formation Processing Systems, pp. 4077–4087, 2017. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. Rethinking few-shot image classiﬁcation: A good embedding is all you need? InEuropean Conference on Computer Vision, pp. 266–282, 2020. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man- zagol, P., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020. Triantaﬁllou, E., Larochelle, H., Zemel, R. S., and Du- moulin, V . Learning a universal template for few-shot dataset generalization. In Proceedings of the 38th Inter- national Conference on Machine Learning, pp. 10424– 10433, 2021. Tseng, H., Lee, H., Huang, J., and Yang, M. Cross-domain few-shot classiﬁcation via learned feature-wise transfor- mation. In International Conference on Learning Repre- sentations, 2020. Tukey, J. W. et al. Exploratory data analysis , volume 2. Reading, MA, 1977. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630–3638, 2016.Channel Importance Matters in Few-Shot Image Classiﬁcation Xu, J., Pan, X., Luo, X., Pei, W., and Xu, Z. Exploring category-correlated feature for few-shot image classiﬁca- tion. arXiv preprint arXiv:2112.07224, 2021. Yang, J., Yang, H., and Chen, L. Towards cross-granularity few-shot learning: Coarse-to-ﬁne pseudo-labeling with visual-semantic meta-embedding. In ACM Multimedia Conference, pp. 3005–3014, 2021a. Yang, S., Liu, L., and Xu, M. Free lunch for few-shot learn- ing: Distribution calibration. In International Conference on Learning Representations, 2021b. Ye, H. and Chao, W. How to train your maml to excel in few-shot classiﬁcation. In International Conference on Learning Representations, 2022. Zagoruyko, S. and Komodakis, N. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016. Zhang, C., Cai, Y ., Lin, G., and Shen, C. Deepemd: Few- shot image classiﬁcation with differentiable earth mover’s distance and structured classiﬁers. In IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pp. 12200–12210, 2020. Zhou, B., Khosla, A., Lapedriza, `A., Oliva, A., and Tor- ralba, A. Object detectors emerge in deep scene cnns. In International Conference on Learning Representations, 2015. Zintgraf, L. M., Shiarlis, K., Kurin, V ., Hofmann, K., and Whiteson, S. Fast context adaptation via meta-learning. In Proceedings of the 36th International Conference on Machine Learning, pp. 7693–7702, 2019.Channel Importance Matters in Few-Shot Image Classiﬁcation A. Proof of Proposition 3.1 Lemma A.1. (Cantelli’s inequality(Cantelli, 1929)) Let X be a random variable with ﬁnite expected value µand ﬁnite non-zero variance σ2. Then for any k> 0, P(X−µ≥kσ) ≤ 1 1 +k2 . (7) Lemma A.2. Let ai >0,bi >0,i = 1,...,D . Deﬁne f : [0,+∞)D/{0}→ R by f(x) = ∑D i=1 bix2 i (∑D i=1 aixi)2 , (8) then min x f(x) = 1 ∑D i=1 a2 i bi . (9) The mimimum value is reached when there exists a constant c> 0, such that ∀i∈[D],xi = aic bi . Proof. We show it by induction on dimension D. Denote the domain of f by U, i.e., U = [0,+∞)D/{0}. When D= 1, f(x) ≡b1 a2 1 is a constant, so the result holds. Assume that when D ≤ k, the result holds. We now prove that when D = k + 1, the result holds. It is obvious that ∀c >0,f(cx) = f(x), thus it sufﬁces to ﬁnd a minimum point in S = {x|a ≤||x||2 ≤b and x ∈U}for any chosen 0 < a < b. Since Sis a closed set and f is continuous, the minimum point exists. The minimum point either lies on the hyperspheres: ∂S = {x|||x||2 = a and x ∈ U}∪{ x|||x||2 = b and x ∈ U}or in between: S= {x|a< ||x||2 <b and x∈U}. If there exists a minimum point on one of the hyperspheres, say, the outer hypersphere {x|||x||2 = band x∈U}, then there exists another minimum point (a+b)x 2b ∈S (or (a+b)x 2a ∈S for the inner hypersphere). Thus it sufﬁces to ﬁnd the minimum point in S. Let S/i = {x|x∈S and xi = 0}and S>0 = {x|x∈S and xi >0 for alli∈[k+ 1]}. We have S= (∪k+1 i=1 S/i) ∪S>0. If x∈S/i, then f(x) = ∑i−1 j=1 bjx2 j + ∑k+1 j=i+1 bjx2 j (∑i−1 j=1 ajxj + ∑k+1 j=i+1 ajxj)2 , (10) which can be seen as a function with input dimension k. Thus from the induction, we have min x∈S/i f(x) = 1 ∑i−1 j=1 a2 j bj + ∑k+1 j=i+1 a2 j bj . (11) Next, we handle the setting when x∈S>0, i.e., ﬁnd all possible extreme points of f inside S>0. Note that ∂f ∂xi = 2(bixi ∑k+1 j=1 ajxj −ai ∑k+1 j=1 bjx2 j) (∑k+1 j=1 ajxj)3 , (12) then an extreme point xmust satisfy bixi k+1∑ j=1 ajxj −ai k+1∑ j=1 bjx2 j = 0,∀i∈[k+ 1], (13) which is equivalent to xi = ai ∑k+1 j=1 bjx2 j bi ∑k+1 j=1 ajxj = ( ∑k+1 j=1 bjx2 j ∑k+1 j=1 ajxj )ai bi ,∀i∈[k+ 1]. (14)Channel Importance Matters in Few-Shot Image Classiﬁcation Thus xi = cai bi , where c= ∑k+1 j=1 bjx2 j∑k+1 j=1 ajxj . Furthermore, it is easy to show that xi = cai bi satisﬁes Eq. (14) for any c> 0. Denote any of the points satisfying this property as x∗, then f(x∗) = 1 ∑k+1 i=1 a2 i bi . (15) Comparing Eq. (11) and Eq. (15), it can be seen that f(x∗) < min x∈S/i f(x),∀i∈[k+ 1]. (16) Finally, note that ∂S and {S/i}k+1 i=1 constitute the boundary of S>0, thus Eq. (16) and earlier discussion about ∂S indicate that x∗is the minimum point of f, as desired. Proposition 3.1. Assume that µ1,l ̸= µ2,l and σ1,l + σ2,l >0 hold for any l∈[d], then we have R≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . (17) To minimize this upper bound, the adjusted oracle MMC of each channelωl should satisfy: ωl ∝|µ1,l −µ2,l| σ1,l + σ2,l . (18) Proof. R= 1 2[Pz1∼D1 (||ω⊙(˜z1 −˜µ1)||2 >||ω⊙(˜z1 −˜µ2)||2) +Pz2∼D2 (||ω⊙(˜z2 −˜µ2)||2 >||ω⊙(˜z2 −˜µ1)||2)] = 1 2[Pz1∼D1 ( d∑ l=1 ω2 l[(˜z1,l −˜µ1,l)2 −(˜z1,l −˜µ2,l)2] >0) +Pz2∼D2 ( d∑ l=1 ω2 l[(˜z2,l −˜µ2,l)2 −(˜z2,l −˜µ1,l)2] >0)] = 1 2[Pz1∼D1 ( d∑ l=1 ω2 l(1 −˜z1,l)(˜µ1,l −˜µ2,l) >0) +Pz2∼D2 ( d∑ l=1 ω2 l(1 −˜z2,l)(˜µ2,l −˜µ1,l) >0)] ≤ 2 ∑d l=1 ω4 l(˜µ1,l −˜µ2,l)2(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [Applying Lemma A.1] ≤ 2 ∑d l=1 ω4 l(˜µ1,l + ˜µ2,l)2(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [˜µ1,l˜µ2,l ≥0] = 8 ∑d l=1 ω4 l(˜σ2 1,l + ˜σ2 2,l) (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 [Standadization: (˜µ1,l + ˜µ2,l)/2 = 1] ≤ 8 ∑d l=1 ω4 l(˜σ1,l + ˜σ2,l)2 (∑d l=1 ω2 l(˜µ1,l −˜µ2,l)2)2 . [˜σ1,l˜σ2,l ≥0] (19) Let xl = ω2 l,al = (˜µ1,l−˜µ2,l)2,bl = (˜σ1,l+ ˜σ2,l)2, then according to Lemma A.2, the minimum value of the upper bound (19) is reached when ω2 l ∝(˜µ1,l−˜µ2,l)2 (˜σ1,l+˜σ2,l)2 = (µ1,l−µ2,l)2 (σ1,l+σ2,l)2 , i.e., ωl ∝|µ1,l−µ2,l| σ1,l+σ2,l . B. Training and Evaluation Details For S2M2 and MoCo-v2 in Table 1, we directly use the ofﬁcial publicly-available pre-trained checkpoints. All other algorithms in Table 1 are trained using a learning rate 0.1 with cosine decay schedule without restart. SGD with momentum 0.9 is adopted as the optimizer. For all meta-learning algorithms, a total of 60000 5-way 5-shot tasks are sampled for training,Channel Importance Matters in Few-Shot Image Classiﬁcation 0.5 1.0 1.5 2.0 Value of k 0.5 1.0 1.5 2.0 2.5 (ACC)% miniImageNet 0.5 1.0 1.5 2.0 Value of k 0.0 0.5 1.0 1.5 2.0 2.5 3.0 (ACC)% CUB 0.5 1.0 1.5 2.0 Value of k 3.4 3.6 3.8 4.0 4.2 (ACC)% T extures 0.5 1.0 1.5 2.0 Value of k 0.75 1.00 1.25 1.50 1.75 2.00 2.25 (ACC)% Traffic Signs 0.5 1.0 1.5 2.0 Value of k 5.5 6.0 6.5 7.0 7.5 8.0 (ACC)% Plant Disease 0.5 1.0 1.5 2.0 Value of k 2.0 2.2 2.4 2.6 2.8 3.0 (ACC)% ISIC 0.5 1.0 1.5 2.0 Value of k 4.4 4.6 4.8 5.0 5.2 5.4 (ACC)% EuroSAT 0.5 1.0 1.5 2.0 Value of k 3.8 4.0 4.2 4.4 4.6 4.8 5.0 (ACC)% Sketch 0.5 1.0 1.5 2.0 Value of k 3.50 3.75 4.00 4.25 4.50 4.75 5.00 5.25 (ACC)% QuickDraw 0.5 1.0 1.5 2.0 Value of k 2.50 2.75 3.00 3.25 3.50 3.75 (ACC)% Fungi 0.5 1.0 1.5 2.0 Value of k 1.1 1.2 1.3 1.4 1.5 1.6 (ACC)% Aircraft 0.5 1.0 1.5 2.0 Value of k 6.0 6.5 7.0 7.5 8.0 (ACC)% Omniglot 0.5 1.0 1.5 2.0 Value of k 4.4 4.6 4.8 5.0 5.2 5.4 (ACC)% VGG Flower 0.5 1.0 1.5 2.0 Value of k 0.5 0.0 0.5 1.0 (ACC)% MSCOCO 0.5 1.0 1.5 2.0 Value of k 0.40 0.45 0.50 0.55 0.60 (ACC)% ChestX 0.5 1.0 1.5 2.0 Value of k 3.75 4.00 4.25 4.50 4.75 5.00 5.25 (ACC)% Clipart 0.5 1.0 1.5 2.0 Value of k 2.50 2.75 3.00 3.25 3.50 3.75 4.00 (ACC)% Infograph 0.5 1.0 1.5 2.0 Value of k 1.75 2.00 2.25 2.50 2.75 3.00 (ACC)% Real 0.5 1.0 1.5 2.0 Value of k 2.0 2.2 2.4 2.6 2.8 3.0 3.2 (ACC)% Painting Figure 9.The effect of the hyperparameter kin the simple transformation on each test-time dataset. The feature extractor is trained on miniImageNet using PN. All experiments are conducted on 10000 5-way 5-shot tasks sampled with a ﬁxed seed. each of which contains 15 query images per class. The batch size (number of sampled tasks of each iteration) is 4. All other hyperparameters of MetaOpt match the default settings in the original paper. All conventionally-trained algorithms are trained for 60 epochs, and the batch size is set to 128. For the training of the CE (Cross-Entropy) algorithm, we normalize the representation before the fully-connected layer. We ﬁnd that if we do not normalize the representation during the training of CE, the simple transformation does not work. We leave it for future work to investigate this phenomenon. For the test-time linear classiﬁcation method we implement for MoCo and S2M2 in Table 1 and Figure 8, we adopt the Logistic Regression implementation of scikit-learn (Pedregosa et al., 2011). C. The Effect of Hyperparameter k In Figure 9, we show how the hyperparameter k in Eq. (1) inﬂuences the few-shot classiﬁcation performance. On all datasets, As the k becomes larger, the accuracy ﬁrst increases and then decreases. The optimal value of k varies for different datasets, ranging from 0.6 to 1.8. That being said, the simple transformation gives a relatively stable performance improvement on all datasets when k∈[1,2]. Notably, datasets with larger task distribution shift often give a smaller optimal k. This phenomenon is reasonable because as seen from Figure 2, a smaller kleads to a larger smoothing effect, and the transformation can better rectify the channel distribution when the task distribution shift is also larger. D. Attempts at Handling Negative Output Values of Neural Networks As shown in Section 3, the MMC of image representations can represent the emphasis of neural network on different channels. However, things get complicated if the output of the neural network can take negative values. If the value of a channel represents the activation of a feature, it is difﬁcult to say whether a large negative value means a large or small activation. At this time, we consider negative value as a signal of feature activation as well, which leads to the followingChannel Importance Matters in Few-Shot Image Classiﬁcation Table 4.Performance gains when applying the extended version of the simple transformation to ResNet-12 with Leaky ReLU trained on mini-train. Algorithm mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Avg PN 76.0+0.5 59.3+0.6 62.0+0.7 66.3-0.2 78.2+2.8 38.1+1.3 75.1+0.8 52.7+0.3 66.5+2.9 55.4+0.0 63.0+1.0 CE 79.4+0.2 64.5+0.8 66.1-0.2 69.9+0.1 84.9+2.1 40.1+0.1 77.6+0.4 53.6+0.4 72.1+3.7 57.4+1.1 66.6+0.9 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Aircraft Original Simple Oracle 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 MMC before transformation 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008MMC after transformation Omniglot Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation VGG Flower Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation MSCOCO Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation ChestX Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 MMC before transformation 0.000 0.001 0.002 0.003 0.004MMC after transformation Clipart Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation Infograph Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035 0.0040MMC after transformation Real Original Simple Oracle 0.000 0.001 0.002 0.003 0.004 0.005 MMC before transformation 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0035MMC after transformation Painting Original Simple Oracle Figure 10.The Visualization of MMC of the other nine datasets before and after the use of simple and oracle transformation.All notations are the same as in Figure 6. simple extension of the transformation: φk(λ) = { sign(λ) lnk( 1 |λ| +1) , |λ|>0 0, λ = 0 (20) now this extended simple transformation can be applied directly to the standard ResNet-12 using leaky ReLU, and the results are shown in Table 4. While leaky ReLU improves the basic performance compared to vanilla ReLU, the improvement of the simple transformation becomes signiﬁcantly smaller. We conjecture that a large negative magnitude of a channel does not strictly mean that this channel is important. It is future work to investigate how to exactly measure channel importance in such circumstances. E. Necessary Ingredients for a Good Transformation One may ask that whether all of the three properties presented in Eq. (3) are necessary for a transformation to successfully improve few-shot learning performance, or whether there exist good transformations other than φk(λ) considered in the main article. To verify the necessity of all properties, we design several functions, each of which does not satisfy one of the properties. First, we consider the function p(λ) =ln(aλ+ 1), where a> 0. This function has positive derivative and negative second derivative, but does not have large enough derivative near zero (p′(0) =a). In the left plot of Figure 11, we see that the improvement brought by this function is smaller than φ1.3(λ), and that the gap becomes smaller when a increases. This validates the necessity of having a large enough derivative near zero. We then consider the piece-wise function q(λ) = { φk(λ), 0 ≤λ<λ 0 a2λ2 + a1λ+ a0, λ ≥λ0 (21) where the values of a2,a1,a0 ensure the smoothness of q(λ) at λ= λ0 up to ﬁrst derivative, and also control the position of the extreme point x0 = −a1 2a2 . This function does not have positive derivative when λ ≥x0. We set x0 = 0.05, and change the value of λ0. The results are shown in the second plot in Figure 11. As seen, introducing negative derivative into the transformation substantially degrades performance. Finally, the property of having negative second derivative can be naturally broken by increasing the value of kin φk(λ), and Figure 9 shows that doing this would degrade performance.Channel Importance Matters in Few-Shot Image Classiﬁcation 0 2000 4000 6000 8000 10000 Value of a 56 57 58 59ACC(%) p( ) 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 Value of 0 56 57 58 59ACC(%) q( ) 0.00 0.01 0.02 0.03 0.04 Value of r 56 57 58 59ACC(%) 1.3( )+r 0.2 0.4 0.6 0.8 1.0 Value of k 56 57 58 59ACC(%) g( ) Figure 11.Exploration of necessary ingredients for a good channel-wise transformation. The accuracies show the average 5-way 5-shot performance over all 19 datasets using PN trained on miniImageNet. The red dashed line shows the original performance; the green dashed line shows the performance when using the simple transformation φ1.3(λ). The leftmost plot shows the performance when using the function p(λ) =ln(aλ+ 1). The second plot shows the performance when using the piece-wise function q(λ). The third plot shows the performance when adding a constant rto the simple transformation φ1.3(λ). The rightmost plot shows the performance when using the power function g(λ) =xk. Since inactivate channels may represent absence of a feature in a task instead of having low emphasis, thus they are likely to have no importance. Therefore another property φk(0) = 0, not shown in Eq. (3), could also be important for a good transformation. To investigate this, we add a constant rto φk(λ) and see how the performance would change. The third plot in Figure 11 shows the opposite result: a small constant added to the transformation helps further improve the performance. As adding this constant has more inﬂuence on small-valued channels, we conjecture that this helps further alleviate the channel bias problem, and that some inactivate channels indeed should gain some focus. Apparently, φk(λ) is not the only function that satisﬁes all the three properties. We consider the power function g(λ) =λk, where k >0. Although being very simple, this function matches all desired properties. The rightmost plot in Figure 11 shows that this function can indeed improve the performance as well. Note that this function has been used in (Yang et al., 2021b), where it is called the Tukey’s Ladder of Powers transformation (Tukey et al., 1977), and is used to transform the feature distribution to be more like a Gaussian distribution. Here we show that mitigating the channel bias problem may be another reason for why it works. F. Details of MMC Calculation and Comparison F.1. Oracle Transformation To apply the oracle transformation, for every test-time dataset D, we ﬁrst calculate feature mean µc and variance σc of each class cin D. Then for every sampled binary classiﬁcation task τ = {Sτ,Qτ}that aims at discriminating two classes c1 and c2, we calculate the oracle MMC ωdirectly from Eq. (6). Next, we standardize each image feature zin Sτ and Qτ, and multiply it by ωto obtain the transformed feature z←ω⊙˜z. The transformed features can be already used for classiﬁcation, but we ﬁnd that for some channels with very small means µl, the corresponding value of oracle MMC ωl becomes too big, deviating from what we expect. To avoid generating such outliers, we additionally restrict that for every channel l, the ratio of transformed MMC to the original MMC should not surpass a threshold, i.e., ωl/ωo l ≤αfor some α∈R+. If a channel ldoes not meet this requirement, we simply set ωl = ωo l. In all of our experiments, we set α= 50. The optimal MMC visualizied in Figure 6 and Figure 10 is also computed using this strategy. We leave it for future work to investigate the reason behind such phenomenon. F.2. Choice of Distance Measure The normalized mean square difference d(x,y) =1 d ∑d l=1(xl −yl)2/x2 l has the advantage of having equal treatment for both channels with small and large values. However, it can be largely inﬂuenced by “outlier channel” with very small xl. Since dataset-level MMC ωD is averaged over MMCs of all possible tasks in D, it is more stable and can use such distance measure. The task-level MMC and image features have much higher variances acorss tasks/images, thus for these two ﬁne-grained settings we just use the mean square difference d(x,y) =1 d ∑d l=1(xl −yl)2. This, however, introduces another problem that critical changes of channels with small values are always ignored by such unnormalized distance. Let’s see a simple example. Let ω1 = (0.05,0.08,0.87) and ω2 = (0.4,0.3,0.3) be two 3-dimensional l1-normalized MMCs. Assume that after transformation, their l1-normalized values become ω′ 1 = (0.15,0.1,0.75) and ω′ 2 = (0.55,0.22,0.23). The value of the ﬁrst dimension of ω1 triples and surpasses that of the second dimension after transformation, thus theChannel Importance Matters in Few-Shot Image Classiﬁcation Table 5.Found best hyperparameters of the test-time ﬁnetuning method on miniImageNet. The feature extrator is ResNet-12, trained by S2M2 algorithm. Shot Batch size Number of epochs Learning rate 1 5 10 0.1 5 25 30 0.05 10 50 50 0.05 20 50 100 0.02 50 64 100 0.01 100 64 500 0.005 400 64 500 0.005 Table 6.The inﬂuence of using different seeds during training or testing. The feature extractor is trained by PN on mini-train. Average 5-way 5-shot performance gains brought by the simple transformation on 10 datasets with 95% conﬁdence interval (over 5 trials) are shown. Seed mini-test CUB Texture TS PlantD ISIC ESAT Sketch QDraw Fungi Test +2.36±0.06 +2.98±0.03 +4.26±0.10 +2.37±0.06 +8.04±0.14 +2.80±0.10 +5.50±0.09 +5.02±0.09 +5.46±0.14 +3.90±0.11 Train +2.08±0.33 +3.28±0.43 +3.78±0.77 +2.00±0.89 +8.21±0.80 +3.64±0.44 +4.03±1.37 +4.21±0.93 +7.01±2.62 +3.59±0.50 channel emphasis changes substantially, while the channel emphasis of ω2 does not change much. We expect that the distance measuring the change of ω1 should be much larger than that measuring the change of ω2. The normalized mean square differences between the MMC before and after transformation are 1.36 and 0.09 for ω1 and ω2 respectively, which is in line with our intuition. However, the mean square differences are0.008 and 0.011 for ω1 and ω2 respectively. Thus under such circumstances, the normalized mean square difference is a much better choice. Although being simple, ω2 and ω1 are a good analogy to the MMC pattern on miniImageNet and some other datasets in Figure 6, respectively. In Figure 6, we can see that most MMC values on miniImageNet are around mid-level, which resembles ω2; most MMC values on other datasets are either very small or large, which resembles ω1. This explains why in Table 3 the task-level and image-level differences on miniImageNet are not smaller than those on other datasets. We leave it for future work to ﬁnd a distance measure that could avoid unstable results, while being sensitive to small-valued channels. G. More Details on Fine-tuning Based Method For ﬁne-tuning methods in Figure 8, we grid search the best hyperparameters in each shot setting on the test set. All best conﬁgurations are shown in Table 5. As seen, the hyperparameters of ﬁne-tuning methods are very sensitive to the number of shots. In low-shot settings, care should be taken for controlling the total steps of ﬁnetuning and learning rate, in order to avoid overﬁtting. This phenomenon is also shown in (Ye & Chao, 2022), where the authors show that MAML (Finn et al., 2017), one of the most widely adopted ﬁnetuning-based methods, has a much higher optimal test-time ﬁne-tuning steps than expected. H. Error Bars All results regarding performance in the main paper are shown without error bars. In Table 6, we show how different seeds affect the improvement brought by the simple transformation φk(λ). There are two seeds that could inﬂuence the result, one for training, and one for testing. When considering test seed, we ﬁx the feature extractor and use different seeds to sample tasks; when considering train seed, we ﬁx the test seed (same tasks) and evaluate different feature extractors trained with different seeds. As seen, while varying the test seed hardly affect the performance, varying the train seed produces some ﬂuctuations. After considering the ﬂuctuations, the improvement given by the transformation can still be statistically guaranteed.",
      "references": [
        "On sensitivity of meta-learning to support data.",
        "Network dissection: Quantifying interpretability of deep visual representations.",
        "Meta-learning with differentiable closed-form solvers.",
        "Fine-grained angular contrastive learning with coarse labels.",
        "Sui conﬁni della probabilita.",
        "A closer look at few-shot classiﬁcation.",
        "Meta-baseline: exploring simple meta-learning for few-shot learning.",
        "Confess: A framework for single source cross-domain few-shot learning.",
        "A baseline for few-shot image classiﬁcation.",
        "Crosstransformers: spatially-aware few-shot transfer.",
        "Z-score normalization, hubness, and few-shot learning.",
        "Model-agnostic meta-learning for fast adaptation of deep networks.",
        "A broader study of cross-domain few-shot learning.",
        "Momentum contrast for unsupervised visual representation learning.",
        "Deep residual learning for image recognition.",
        "The inaturalist species classiﬁcation and detection dataset.",
        "Cross attention network for few-shot classiﬁcation.",
        "Squeeze-and-excitation networks.",
        "Big transfer (bit): General visual representation learning.",
        "Imagenet classiﬁcation with deep convolutional neural networks.",
        "Meta-learning with differentiable convex optimization.",
        "Cross-domain few-shot learning with task-speciﬁc adapters.",
        "Negative margin matters: Understanding margin in few-shot classiﬁcation.",
        "Boosting few-shot classiﬁcation with view-learnable contrastive learning.",
        "Rectifying the shortcut learning of background for few-shot learning.",
        "Charting the right manifold: Manifold mixup for few-shot learning.",
        "TADAM: task dependent adaptive metric for improved few-shot learning.",
        "Meta-curvature.",
        "Scikit-learn: Machine learning in python.",
        "Moment matching for multi-source domain adaptation.",
        "Film: Visual reasoning with a general conditioning layer.",
        "Rapid learning or feature reuse? towards understanding the effectiveness of MAML.",
        "Meta-learning with implicit gradients.",
        "Fast and ﬂexible multi-task classiﬁcation using conditional neural adaptive processes.",
        "Exploring complementary strengths of invariant and equivariant representations for few-shot learning.",
        "Meta-learning with latent embedding optimization.",
        "Imagenet large scale visual recognition challenge.",
        "Grad-cam: Visual explanations from deep networks via gradient-based localization.",
        "Prototypical networks for few-shot learning.",
        "Rethinking few-shot image classiﬁcation: A good embedding is all you need?",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples.",
        "Learning a universal template for few-shot dataset generalization.",
        "Cross-domain few-shot classiﬁcation via learned feature-wise transformation.",
        "Exploratory data analysis.",
        "Matching networks for one shot learning.",
        "Exploring category-correlated feature for few-shot image classiﬁcation.",
        "Towards cross-granularity few-shot learning: Coarse-to-ﬁne pseudo-labeling with visual-semantic meta-embedding.",
        "Free lunch for few-shot learning: Distribution calibration.",
        "How to train your maml to excel in few-shot classiﬁcation.",
        "Wide residual networks.",
        "Deepemd: Few-shot image classiﬁcation with differentiable earth mover’s distance and structured classiﬁers.",
        "Object detectors emerge in deep scene cnns.",
        "Fast context adaptation via meta-learning."
      ],
      "meta_data": {
        "arxiv_id": "2206.08126v2",
        "authors": [
          "Xu Luo",
          "Jing Xu",
          "Zenglin Xu"
        ],
        "published_date": "2022-06-16T12:38:45Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces and empirically validates the concept of \"channel bias\" in few-shot image classification, showing that convolutional networks over-emphasise certain feature channels learned on training tasks and fail to re-weight them for novel tasks. Proposes an extremely simple, test-time, channel-wise logarithmic feature transformation that alleviates this bias, is model-agnostic, incurs no extra training cost, and delivers 0.5–7.5% absolute accuracy gains across 19 test datasets, 6 backbone architectures and 10 learning algorithms. Provides theoretical analysis deriving an oracle Mean Magnitude of Channels (MMC) weighting and establishes an upper-bound on misclassification error, confirming why the proposed transform works.",
        "methodology": "1) Define MMC as the average absolute activation per channel after global pooling. 2) Apply a per-channel transformation φ_k(λ)=sign(λ)*ln_k(1+1/|λ|) (k≈1.3) to all support and query features only at test time; it suppresses high-magnitude channels and amplifies low ones. 3) Theoretically derive an oracle channel weighting ω_l ∝ |μ1,l−μ2,l| / (σ1,l+σ2,l) by minimising an error bound for a Nearest-Centroid classifier, showing the ideal re-weighting depends on class-wise means and variances. 4) Quantify channel bias distances before/after transformation; analyse Grad-CAM and shot-based behaviour; compare with fine-tuning, linear probing and metric methods.",
        "experimental_setup": "Training data: miniImageNet train split (64 classes), ImageNet-1K, and iNaturalist-2018. Test data: 19 datasets covering Meta-Dataset, BSCD-FSL and DomainNet (e.g., miniImageNet-test, CUB, Textures, TrafficSigns, PlantDisease, ISIC, EuroSAT, Sketch, QuickDraw, Fungi, etc.). Tasks: 5-way 5-shot (main), binary 5-shot for oracle study, and up to 5-way 400-shot for shot analysis; each evaluation uses 10,000 randomly sampled tasks. Backbones: Conv-4, ResNet-12, Wide-ResNet-28-10, ResNet-50, SE-ResNet-50. Training algorithms evaluated: Cross-Entropy, S2M2, MoCo-v2, ProtoNet, Meta-Baseline, MetaOptNet. Validation metric: average query accuracy per task; improvements reported as absolute %. Hyperparameter k fixed to 1.3 unless otherwise noted.",
        "limitations": "1) Requires non-negative feature outputs (ReLU); effectiveness diminishes with negative activations (Leaky-ReLU). 2) Single global hyperparameter k that must be tuned; optimal value varies across datasets. 3) Benefits shrink or can slightly hurt when test tasks are very similar to training distribution (e.g., ImageNet→miniImageNet). 4) Oracle weighting assumes channel independence and needs class statistics unavailable in real few-shot scenarios. 5) Analysis restricted to image classification; impact on other modalities or tasks untested. 6) Transformation is heuristic; no end-to-end learning of channel importance at training time.",
        "future_research_directions": "1) Develop learnable or adaptive channel re-weighting modules that operate without task statistics and handle negative features. 2) Integrate channel-bias regularisation during training to reduce reliance on post-hoc fixes. 3) Explore channel bias effects in other vision tasks (detection, segmentation) and in non-visual modalities. 4) Automate selection of parameter k (e.g., via meta-learning). 5) Extend theoretical analysis to correlated channels and more complex classifiers. 6) Combine with fine-tuning or feature modulation methods for further gains in medium-shot regimes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Current few-shot learners remain vulnerable to *targeted* corruption of their most informative channels.  Uniform masking (CRUM) improves average-case robustness but still collapses when the very channels with highest saliency or energy are removed – exactly the pattern produced by aggressive pruning, quantization errors or sensor drift on edge devices.  No published method trains models to withstand this *worst-case* channel loss while simultaneously mitigating channel bias in the clean setting.",
    "method": "Adversarial Channel Dropout & Mix (ACDM).\n1. For every mini-batch we compute penultimate features h∈ℝ^{B×C}.\n2. Adversarial channel selector  S_adv: for each sample i, rank channels by |h_i[c]| and set M_adv_i[c]=0 for the top⌈p·C⌉ channels (p∼U(0.3,0.7)), 1 otherwise.\n3. Random selector S_rand: independent Bernoulli mask with same sparsity.\n4. Produce three logits per sample: f (clean), f_adv (h⊙M_adv), f_rand (h⊙M_rand).\n5. Loss = L_CE(f,y)                               # task loss\n           + λ_var·Var_c( mean_i |h_i[c]| )        # MMC variance (UCR)\n           + λ_adv·KL( softmax(f/T), softmax(f_adv/T) )  # invariance to *worst-case* mask\n           + λ_rand·KL( softmax(f/T), softmax(f_rand/T) ) # invariance to random mask\n6. Channel Mix augmentation: with prob 0.5 create a mixed sample h_mix = α·h_i + (1−α)·h_j (α∼Beta(2,2)) and use same label mixup; include h_mix in step 4 to spread information across channels.\nNo extra parameters; <3 % training overhead.\nNovelty: first method to combine (a) adversarial feature-space dropout focusing on dominant channels, (b) random dropout, (c) variance equalisation and (d) channel-space MixUp, creating *balanced and worst-case robust* representations without test-time tricks.",
    "experimental_setup": "Backbones: ResNet-18 & ResNet-50.\nDatasets: CIFAR-FS (64/36 split), miniImageNet (64/20/16), plus DomainNet-mini (real/ppt/sketch).\nTraining: SGD, cosine LR, basic image augmentations, 200 epochs.\nModels:\n1. Baseline CE\n2. Baseline+UCR (λ_var)\n3. Baseline+CRUM (λ_var, λ_rand)\n4. Baseline+ACDM (λ_var, λ_rand, λ_adv, channel-mix) – proposed\nHyper-parameters: λ_var=0.05, λ_rand=0.1, λ_adv=0.3, T=2.\nFew-shot eval: 5-way {1,5,20}-shot, 600 episodes, linear classifier.\nRobustness eval:\n  a) Random 30 % channel drop\n  b) Targeted drop of top-30 % magnitude channels\nRecord clean accuracy and Δacc.\nCompute 95 % CIs by 10 k bootstrap.",
    "primary_metric": "accuracy",
    "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nfrom torchvision.models import resnet18\nclass ACDMModel(nn.Module):\n    def __init__(self, num_classes, λ_var=0.05, λ_rand=0.1, λ_adv=0.3, T=2.):\n        super().__init__()\n        self.backbone = resnet18(num_classes=num_classes)\n        self.λ_var, self.λ_rand, self.λ_adv, self.T = λ_var, λ_rand, λ_adv, T\n    def _features(self, x):\n        m = self.backbone\n        x = m.relu(m.bn1(m.conv1(x)))\n        x = m.maxpool(x)\n        x = m.layer1(x); x = m.layer2(x); x = m.layer3(x); x = m.layer4(x)\n        x = m.avgpool(x)\n        return x.view(x.size(0), -1)\n    def forward(self, x):\n        return self.backbone(x)\n    def loss(self, x, y):\n        h = self._features(x)                # (B,C)\n        B, C = h.size()\n        # variance term\n        mmc = h.abs().mean(0)\n        loss_var = mmc.var(unbiased=False)\n        # random mask\n        p = torch.empty(1, device=x.device).uniform_(0.3, 0.7).item()\n        M_rand = (torch.rand(B, C, device=x.device) > p).float()\n        # adversarial mask (mask top-p channels per sample)\n        k = int(p * C)\n        topk = h.abs().topk(k, dim=1, largest=True).indices\n        M_adv = torch.ones_like(h)\n        M_adv.scatter_(1, topk, 0.)\n        # mixed features (optional)\n        if torch.rand(1).item() < 0.5:\n            perm = torch.randperm(B)\n            α = torch.distributions.Beta(2,2).sample((B,1)).to(x.device)\n            h_mix = α*h + (1-α)*h[perm]\n            y_mix = None  # handled in episode construction\n        # logits\n        logits_clean = self.backbone.fc(h)\n        logits_rand  = self.backbone.fc(h * M_rand)\n        logits_adv   = self.backbone.fc(h * M_adv)\n        loss_ce = F.cross_entropy(logits_clean, y)\n        def kl(p,q):\n            return F.kl_div(F.log_softmax(p/self.T,dim=1),\n                            F.softmax(q/self.T,dim=1),\n                            reduction='batchmean') * (self.T**2)\n        loss = loss_ce + self.λ_var*loss_var + \\\n               self.λ_rand*kl(logits_clean, logits_rand) + \\\n               self.λ_adv *kl(logits_clean, logits_adv)\n        return loss",
    "expected_result": "CIFAR-FS 5-way 5-shot (ResNet-18):\n• Baseline 72.0 ±0.7 %  \n• UCR 74.0 ±0.7 %  \n• CRUM 76.0 ±0.6 %  \n• ACDM 77.5 ±0.6 %  (+1.5 pp vs CRUM)\n1-shot: 50.0→52.5 % (+2.5 pp vs CRUM)\nRobustness (clean→rand drop→targeted drop, Δacc):\n• Baseline 72→54 (−18)→40 (−32)\n• CRUM    76→71 (−5) →60 (−16)\n• ACDM    77.5→74.5 (−3)→71 (−6)  (5× smaller worst-case drop)\nSimilar gaps on miniImageNet and DomainNet-mini.\nTraining adds ≈3 % time, no inference overhead.",
    "expected_conclusion": "ACDM demonstrates that explicitly training for *worst-case* channel failure, not just average-case randomness, yields state-of-the-art robustness and the highest clean few-shot accuracy reported for parameter-free regularisers.  By unifying adversarial channel dropout, random dropout, variance equalisation and channel-mix augmentation in a single, lightweight loss, the method closes a critical gap left by CIM, UCR and CRUM.  Academically, it highlights adversarial hidden-space perturbations as a powerful form of regularisation; practically, it equips on-device models with strong resilience to structured noise and pruning without sacrificing performance or efficiency."
  },
  "experimental_design": {
    "experiment_summary": "Objective: Demonstrate that Adversarial Channel Dropout & Mix (ACDM) yields higher few-shot classification accuracy and dramatically better robustness to worst-case (targeted) channel loss than the strongest published baseline (CRUM).\nTask: 5-way few-shot image classification (1, 5, 20 shots) on CIFAR-FS while evaluating under three test conditions – clean features, random 30 % channel drop, and targeted 30 % drop of the channels with the highest feature magnitudes.\nWorkflow:\n1. Train two ResNet-18 backbones from scratch on the CIFAR-FS meta-training split (64 classes):\n   • Baseline-CRUM – cross-entropy + MMC variance loss + random uniform channel masking (λ_rand).\n   • Proposed ACDM – CRUM losses plus adversarial channel masking, additional KL term and feature MixUp.\n   Both use identical optimiser settings, so any performance gap is attributable to the regulariser.\n2. Meta-test: freeze backbones, train a linear classifier per episode on the support set and record query accuracy.\n3. Robustness test: repeat evaluation after applying channel-drop corruptions to the frozen backbone features.\n4. Compute accuracy, robustness drop and 95 % bootstrap CIs.\nScale considerations: ResNet-18 (≈12 M parameters) fits comfortably in <2 GB GPU memory; with batch size 256 and three forward passes per mini-batch (clean, random, adversarial) peak memory <12 GB. Training 200 epochs on CIFAR-FS (60 k images) takes ≈5 GPU-hours on a single A100, well within the provided gpu-runner resources.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA A100 or H200, VRAM: 80 GB or more, RAM: 2048 GB or more"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Correctness criteria: a prediction is correct when the predicted class label exactly matches the ground-truth label for the query image.\nCalculation: accuracy = (number of correct predictions) / (total number of predictions) × 100 % computed over 600 meta-test episodes and averaged across the 1, 5 and 20-shot settings.\nTask appropriateness: few-shot classification is a discrete multi-class task where each query has a single correct label, so accuracy directly measures performance.\nVisualisations: bar chart comparing methods across shot numbers; 95 % CI error bars."
      },
      {
        "name": "worst_case_drop",
        "description": "Correctness criteria: uses the same per-image correctness definition as accuracy.\nCalculation: worst_case_drop = accuracy_clean − accuracy_targeted_drop (both in percent points).\nTask appropriateness: quantifies robustness loss under the adversarial corruption the hypothesis targets.\nVisualisations: grouped bar chart of clean, random-drop and targeted-drop accuracies; line plot of Δacc."
      }
    ],
    "models_to_use": [
      "ResNet-18 (11.7 M)"
    ],
    "datasets_to_use": [
      "CIFAR-FS"
    ],
    "proposed_method": {
      "method_name": "Adversarial Channel Dropout & Mix (ACDM)",
      "description": "Adds adversarial feature-space channel masking, random masking, MMC variance equalisation and channel-mix mixup to standard cross-entropy training, forcing information to spread across channels and conferring robustness to worst-case channel loss.",
      "training_config": {
        "learning_rate": 0.1,
        "batch_size": 256,
        "epochs": 200,
        "optimizer": "sgd",
        "warmup_steps": 0,
        "weight_decay": 0.0005,
        "scheduler": "cosine",
        "seed": 42,
        "additional_params": "{\"lambda_var\":0.05,\"lambda_rand\":0.1,\"lambda_adv\":0.3,\"temperature\":2,\"p_range\":[0.3,0.7]}"
      },
      "optuna_config": {
        "enabled": false,
        "n_trials": 0
      }
    },
    "comparative_methods": [
      {
        "method_name": "CRUM (Uniform Channel Masking)",
        "description": "Baseline regulariser that combines MMC variance equalisation with random uniform channel dropout; no adversarial mask or mixup.",
        "training_config": {
          "learning_rate": 0.1,
          "batch_size": 256,
          "epochs": 200,
          "optimizer": "sgd",
          "warmup_steps": 0,
          "weight_decay": 0.0005,
          "scheduler": "cosine",
          "seed": 42,
          "additional_params": "{\"lambda_var\":0.05,\"lambda_rand\":0.1}"
        },
        "optuna_config": {
          "enabled": false,
          "n_trials": 0
        }
      }
    ]
  }
}